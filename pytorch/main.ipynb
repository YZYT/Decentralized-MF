{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b1d710d4a2dd0e836743a9708dcf2dd87750cb6db75a03dbc0a1931aaec4e6cb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from utils.mylib import *\n",
        "from d2l import torch as d2l"
      ],
      "outputs": [],
      "metadata": {
        "id": "k-onQd4JNA5H",
        "outputId": "0317f255-2dd4-4eeb-822f-ed20d8ea782b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "init_Seed()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class MLDataset(Dataset):\n",
        "    \"\"\" Dataset for loading and preprocessing the MoviesLen dataset. \"\"\"\n",
        "    def __init__(self, path, mode='train', target_only=False):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        data = np.loadtxt(path, dtype='long')\n",
        "\n",
        "        # Convert data into PyTorch tensors\n",
        "        self.data = torch.LongTensor(data[:, :2])\n",
        "        self.target = torch.FloatTensor(data[:, 2])\n",
        " \n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of MoviesLen Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users=1000, m_items=2000, n_factors=20):\n",
        "        super(MF, self).__init__()\n",
        "\n",
        "        self.U = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.V = torch.nn.Embedding(m_items, n_factors)\n",
        "\n",
        "        self.U.weight.data.uniform_(-0.005, 0.005)\n",
        "        self.V.weight.data.uniform_(-0.005, 0.005)\n",
        "        \n",
        "        self.criterion = nn.MSELoss(reduction='sum')\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        # return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
        "        return torch.einsum('ij, ij -> i', [self.U(user), self.V(item)])\n",
        "    \n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ],
      "metadata": {
        "id": "AlhTlkE7MDo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = MLDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "outputs": [],
      "metadata": {
        "id": "hlhLk5t6MBX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train/Dev/Test**"
      ],
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# U = torch.rand(size=(1005, 20), requires_grad=True)\n",
        "# V = torch.rand(size=(2005, 20), requires_grad=True)\n",
        "# with torch.no_grad():\n",
        "#     U /= 100\n",
        "#     V /= 100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "MAM8QecJOyqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# def model(X):\n",
        "#     return torch.einsum('ij, ij -> i', [U[X[:, 0]], V[X[:, 1]]])\n",
        "\n",
        "# def squared_loss(y_hat, y):  #@save\n",
        "#     \"\"\"Squared loss.\"\"\"\n",
        "#     return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "\n",
        "# def objective(X, y):\n",
        "#     return squared_loss(model(X), y).sum() + (U[X[:, 0]].norm() ** 2 + V[X[:, 1]].norm() ** 2) * 0.01\n",
        "\n",
        "# def sgd(params, lr, batch_size):  #@save\n",
        "#     \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "#     with torch.no_grad():\n",
        "#         for param in params:\n",
        "#             param -= lr * param.grad\n",
        "#             param.grad.zero_()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def train(tr_set, dv_set, model, config):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []} \n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_epochs:\n",
        "        model.train()\n",
        "        for X, y in tr_set:\n",
        "            optimizer.zero_grad()    \n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = model(X[:, 0], X[:, 1])\n",
        "            mse_loss = model.cal_loss(y_hat, y)\n",
        "            mse_loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item() / batch_size)\n",
        "        \n",
        "        epoch += 1\n",
        "        \n",
        "        print(\"train_loss: {:.4f}\".format(np.mean(loss_record['train'][-100:])))\n",
        "\n",
        "        dev_mse = dev(dv_set, model, device) / batch_size\n",
        "        if dev_mse < min_mse:\n",
        "            min_mse = dev_mse\n",
        "            early_stop_cnt = 0\n",
        "            print(\"Saving model (epoch = {:4d}  loss = {:.4f} )\".format(epoch, dev_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "        \n",
        "        \n",
        "        loss_record['dev'].append(dev_mse)\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            break\n",
        "\n",
        "    print(\"Finish training after {} epochs\".format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# def train2():\n",
        "#     n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "#     loss = squared_loss\n",
        "        \n",
        "#     # Setup optimizer\n",
        "#     optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "#         [U, V], **config['optim_hparas'])\n",
        "\n",
        "#     min_mse = 1000.\n",
        "#     loss_record = {'train': [], 'dev': []} \n",
        "#     early_stop_cnt = 0\n",
        "#     epoch = 0\n",
        "\n",
        "#     while epoch < n_epochs:\n",
        "\n",
        "#         for X, y in tr_set:\n",
        "#             optimizer.zero_grad()\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             y_hat = model(X)\n",
        "#             mse_loss = loss(y_hat, y)\n",
        "#             mse_loss.sum().backward()\n",
        "#             optimizer.step()\n",
        "            \n",
        "#         epoch += 1\n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             train_l = loss(model(X), y)\n",
        "#             print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation**"
      ],
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for X, y in dv_set:                         # iterate through the dataloader\n",
        "        X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(X[:, 0], X[:, 1])                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(X)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "yrxrD3YsN3U2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "g0pdrhQAO41L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "aSBMRFlYN5tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ],
      "metadata": {
        "id": "SvckkF5dvf0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = False                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 1000,              # maximum number of epochs\n",
        "    'batch_size': 10000,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.005,                # learning rate\n",
        "        # 'weight_decay': 0.0001\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 60,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "NPXpdumwPjE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and model**"
      ],
      "metadata": {
        "id": "6j1eOV3TOH-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "tr_set = prep_dataloader('../data/ML100K/ML100K_copy1_train.txt', 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader('../data/ML100K/ML100K_copy1_test.txt', 'dev', config['batch_size'], target_only=target_only)\n",
        "# tt_set = prep_dataloader(\"data/ML100K/ML100K_copy1_test.txt\", 'test', config['batch_size'], target_only=target_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of MoviesLen Dataset (60000 samples found, each dim = 2)\n",
            "Finished reading the dev set of MoviesLen Dataset (20000 samples found, each dim = 2)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "model = MF().to(device) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Training!**"
      ],
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "2190df3d-28a1-4849-a9a8-5179cff3130d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "%%time\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 13.7341\n",
            "Saving model (epoch =    1  loss = 13.6949 )\n",
            "train_loss: 13.7331\n",
            "Saving model (epoch =    2  loss = 13.6905 )\n",
            "train_loss: 13.7302\n",
            "Saving model (epoch =    3  loss = 13.6777 )\n",
            "train_loss: 13.7242\n",
            "Saving model (epoch =    4  loss = 13.6530 )\n",
            "train_loss: 13.7145\n",
            "Saving model (epoch =    5  loss = 13.6133 )\n",
            "train_loss: 13.7001\n",
            "Saving model (epoch =    6  loss = 13.5562 )\n",
            "train_loss: 13.6804\n",
            "Saving model (epoch =    7  loss = 13.4795 )\n",
            "train_loss: 13.6549\n",
            "Saving model (epoch =    8  loss = 13.3814 )\n",
            "train_loss: 13.6231\n",
            "Saving model (epoch =    9  loss = 13.2610 )\n",
            "train_loss: 13.5845\n",
            "Saving model (epoch =   10  loss = 13.1174 )\n",
            "train_loss: 13.5389\n",
            "Saving model (epoch =   11  loss = 12.9504 )\n",
            "train_loss: 13.4861\n",
            "Saving model (epoch =   12  loss = 12.7601 )\n",
            "train_loss: 13.4260\n",
            "Saving model (epoch =   13  loss = 12.5469 )\n",
            "train_loss: 13.3584\n",
            "Saving model (epoch =   14  loss = 12.3115 )\n",
            "train_loss: 13.2835\n",
            "Saving model (epoch =   15  loss = 12.0548 )\n",
            "train_loss: 13.2013\n",
            "Saving model (epoch =   16  loss = 11.7781 )\n",
            "train_loss: 13.0995\n",
            "Saving model (epoch =   17  loss = 11.4824 )\n",
            "train_loss: 12.9569\n",
            "Saving model (epoch =   18  loss = 11.1692 )\n",
            "train_loss: 12.7998\n",
            "Saving model (epoch =   19  loss = 10.8402 )\n",
            "train_loss: 12.6185\n",
            "Saving model (epoch =   20  loss = 10.4969 )\n",
            "train_loss: 12.4169\n",
            "Saving model (epoch =   21  loss = 10.1410 )\n",
            "train_loss: 12.1985\n",
            "Saving model (epoch =   22  loss = 9.7746 )\n",
            "train_loss: 11.9604\n",
            "Saving model (epoch =   23  loss = 9.3994 )\n",
            "train_loss: 11.7018\n",
            "Saving model (epoch =   24  loss = 9.0174 )\n",
            "train_loss: 11.4267\n",
            "Saving model (epoch =   25  loss = 8.6303 )\n",
            "train_loss: 11.1363\n",
            "Saving model (epoch =   26  loss = 8.2403 )\n",
            "train_loss: 10.8319\n",
            "Saving model (epoch =   27  loss = 7.8494 )\n",
            "train_loss: 10.5116\n",
            "Saving model (epoch =   28  loss = 7.4594 )\n",
            "train_loss: 10.1779\n",
            "Saving model (epoch =   29  loss = 7.0724 )\n",
            "train_loss: 9.8341\n",
            "Saving model (epoch =   30  loss = 6.6896 )\n",
            "train_loss: 9.4812\n",
            "Saving model (epoch =   31  loss = 6.3131 )\n",
            "train_loss: 9.1229\n",
            "Saving model (epoch =   32  loss = 5.9449 )\n",
            "train_loss: 8.7567\n",
            "Saving model (epoch =   33  loss = 5.5860 )\n",
            "train_loss: 8.3879\n",
            "Saving model (epoch =   34  loss = 5.2381 )\n",
            "train_loss: 8.0171\n",
            "Saving model (epoch =   35  loss = 4.9024 )\n",
            "train_loss: 7.6435\n",
            "Saving model (epoch =   36  loss = 4.5802 )\n",
            "train_loss: 7.2724\n",
            "Saving model (epoch =   37  loss = 4.2722 )\n",
            "train_loss: 6.9058\n",
            "Saving model (epoch =   38  loss = 3.9793 )\n",
            "train_loss: 6.5430\n",
            "Saving model (epoch =   39  loss = 3.7020 )\n",
            "train_loss: 6.1863\n",
            "Saving model (epoch =   40  loss = 3.4412 )\n",
            "train_loss: 5.8375\n",
            "Saving model (epoch =   41  loss = 3.1969 )\n",
            "train_loss: 5.4969\n",
            "Saving model (epoch =   42  loss = 2.9691 )\n",
            "train_loss: 5.1679\n",
            "Saving model (epoch =   43  loss = 2.7578 )\n",
            "train_loss: 4.8485\n",
            "Saving model (epoch =   44  loss = 2.5628 )\n",
            "train_loss: 4.5438\n",
            "Saving model (epoch =   45  loss = 2.3837 )\n",
            "train_loss: 4.2483\n",
            "Saving model (epoch =   46  loss = 2.2206 )\n",
            "train_loss: 3.9684\n",
            "Saving model (epoch =   47  loss = 2.0723 )\n",
            "train_loss: 3.7016\n",
            "Saving model (epoch =   48  loss = 1.9382 )\n",
            "train_loss: 3.4509\n",
            "Saving model (epoch =   49  loss = 1.8176 )\n",
            "train_loss: 3.2132\n",
            "Saving model (epoch =   50  loss = 1.7095 )\n",
            "train_loss: 2.9919\n",
            "Saving model (epoch =   51  loss = 1.6132 )\n",
            "train_loss: 2.7849\n",
            "Saving model (epoch =   52  loss = 1.5277 )\n",
            "train_loss: 2.5931\n",
            "Saving model (epoch =   53  loss = 1.4521 )\n",
            "train_loss: 2.4152\n",
            "Saving model (epoch =   54  loss = 1.3857 )\n",
            "train_loss: 2.2509\n",
            "Saving model (epoch =   55  loss = 1.3273 )\n",
            "train_loss: 2.1009\n",
            "Saving model (epoch =   56  loss = 1.2764 )\n",
            "train_loss: 1.9636\n",
            "Saving model (epoch =   57  loss = 1.2320 )\n",
            "train_loss: 1.8390\n",
            "Saving model (epoch =   58  loss = 1.1934 )\n",
            "train_loss: 1.7255\n",
            "Saving model (epoch =   59  loss = 1.1599 )\n",
            "train_loss: 1.6237\n",
            "Saving model (epoch =   60  loss = 1.1310 )\n",
            "train_loss: 1.5315\n",
            "Saving model (epoch =   61  loss = 1.1059 )\n",
            "train_loss: 1.4501\n",
            "Saving model (epoch =   62  loss = 1.0844 )\n",
            "train_loss: 1.3770\n",
            "Saving model (epoch =   63  loss = 1.0658 )\n",
            "train_loss: 1.3120\n",
            "Saving model (epoch =   64  loss = 1.0498 )\n",
            "train_loss: 1.2544\n",
            "Saving model (epoch =   65  loss = 1.0359 )\n",
            "train_loss: 1.2035\n",
            "Saving model (epoch =   66  loss = 1.0240 )\n",
            "train_loss: 1.1584\n",
            "Saving model (epoch =   67  loss = 1.0137 )\n",
            "train_loss: 1.1193\n",
            "Saving model (epoch =   68  loss = 1.0048 )\n",
            "train_loss: 1.0846\n",
            "Saving model (epoch =   69  loss = 0.9972 )\n",
            "train_loss: 1.0543\n",
            "Saving model (epoch =   70  loss = 0.9905 )\n",
            "train_loss: 1.0277\n",
            "Saving model (epoch =   71  loss = 0.9847 )\n",
            "train_loss: 1.0046\n",
            "Saving model (epoch =   72  loss = 0.9797 )\n",
            "train_loss: 0.9847\n",
            "Saving model (epoch =   73  loss = 0.9753 )\n",
            "train_loss: 0.9669\n",
            "Saving model (epoch =   74  loss = 0.9714 )\n",
            "train_loss: 0.9513\n",
            "Saving model (epoch =   75  loss = 0.9681 )\n",
            "train_loss: 0.9377\n",
            "Saving model (epoch =   76  loss = 0.9651 )\n",
            "train_loss: 0.9259\n",
            "Saving model (epoch =   77  loss = 0.9625 )\n",
            "train_loss: 0.9158\n",
            "Saving model (epoch =   78  loss = 0.9602 )\n",
            "train_loss: 0.9063\n",
            "Saving model (epoch =   79  loss = 0.9582 )\n",
            "train_loss: 0.8987\n",
            "Saving model (epoch =   80  loss = 0.9564 )\n",
            "train_loss: 0.8917\n",
            "Saving model (epoch =   81  loss = 0.9547 )\n",
            "train_loss: 0.8858\n",
            "Saving model (epoch =   82  loss = 0.9533 )\n",
            "train_loss: 0.8799\n",
            "Saving model (epoch =   83  loss = 0.9520 )\n",
            "train_loss: 0.8753\n",
            "Saving model (epoch =   84  loss = 0.9509 )\n",
            "train_loss: 0.8709\n",
            "Saving model (epoch =   85  loss = 0.9498 )\n",
            "train_loss: 0.8670\n",
            "Saving model (epoch =   86  loss = 0.9489 )\n",
            "train_loss: 0.8635\n",
            "Saving model (epoch =   87  loss = 0.9481 )\n",
            "train_loss: 0.8603\n",
            "Saving model (epoch =   88  loss = 0.9474 )\n",
            "train_loss: 0.8573\n",
            "Saving model (epoch =   89  loss = 0.9467 )\n",
            "train_loss: 0.8547\n",
            "Saving model (epoch =   90  loss = 0.9461 )\n",
            "train_loss: 0.8526\n",
            "Saving model (epoch =   91  loss = 0.9455 )\n",
            "train_loss: 0.8507\n",
            "Saving model (epoch =   92  loss = 0.9450 )\n",
            "train_loss: 0.8486\n",
            "Saving model (epoch =   93  loss = 0.9445 )\n",
            "train_loss: 0.8468\n",
            "Saving model (epoch =   94  loss = 0.9441 )\n",
            "train_loss: 0.8452\n",
            "Saving model (epoch =   95  loss = 0.9437 )\n",
            "train_loss: 0.8438\n",
            "Saving model (epoch =   96  loss = 0.9434 )\n",
            "train_loss: 0.8422\n",
            "Saving model (epoch =   97  loss = 0.9431 )\n",
            "train_loss: 0.8413\n",
            "Saving model (epoch =   98  loss = 0.9428 )\n",
            "train_loss: 0.8402\n",
            "Saving model (epoch =   99  loss = 0.9426 )\n",
            "train_loss: 0.8389\n",
            "Saving model (epoch =  100  loss = 0.9423 )\n",
            "train_loss: 0.8382\n",
            "Saving model (epoch =  101  loss = 0.9421 )\n",
            "train_loss: 0.8370\n",
            "Saving model (epoch =  102  loss = 0.9419 )\n",
            "train_loss: 0.8362\n",
            "Saving model (epoch =  103  loss = 0.9418 )\n",
            "train_loss: 0.8352\n",
            "Saving model (epoch =  104  loss = 0.9416 )\n",
            "train_loss: 0.8347\n",
            "Saving model (epoch =  105  loss = 0.9414 )\n",
            "train_loss: 0.8339\n",
            "Saving model (epoch =  106  loss = 0.9413 )\n",
            "train_loss: 0.8332\n",
            "Saving model (epoch =  107  loss = 0.9411 )\n",
            "train_loss: 0.8327\n",
            "Saving model (epoch =  108  loss = 0.9410 )\n",
            "train_loss: 0.8320\n",
            "Saving model (epoch =  109  loss = 0.9409 )\n",
            "train_loss: 0.8316\n",
            "Saving model (epoch =  110  loss = 0.9408 )\n",
            "train_loss: 0.8311\n",
            "Saving model (epoch =  111  loss = 0.9407 )\n",
            "train_loss: 0.8306\n",
            "Saving model (epoch =  112  loss = 0.9407 )\n",
            "train_loss: 0.8301\n",
            "Saving model (epoch =  113  loss = 0.9406 )\n",
            "train_loss: 0.8297\n",
            "Saving model (epoch =  114  loss = 0.9406 )\n",
            "train_loss: 0.8294\n",
            "Saving model (epoch =  115  loss = 0.9406 )\n",
            "train_loss: 0.8288\n",
            "Saving model (epoch =  116  loss = 0.9405 )\n",
            "train_loss: 0.8284\n",
            "Saving model (epoch =  117  loss = 0.9404 )\n",
            "train_loss: 0.8283\n",
            "Saving model (epoch =  118  loss = 0.9404 )\n",
            "train_loss: 0.8281\n",
            "Saving model (epoch =  119  loss = 0.9404 )\n",
            "train_loss: 0.8276\n",
            "Saving model (epoch =  120  loss = 0.9403 )\n",
            "train_loss: 0.8274\n",
            "Saving model (epoch =  121  loss = 0.9403 )\n",
            "train_loss: 0.8273\n",
            "Saving model (epoch =  122  loss = 0.9402 )\n",
            "train_loss: 0.8266\n",
            "Saving model (epoch =  123  loss = 0.9402 )\n",
            "train_loss: 0.8268\n",
            "Saving model (epoch =  124  loss = 0.9402 )\n",
            "train_loss: 0.8263\n",
            "Saving model (epoch =  125  loss = 0.9401 )\n",
            "train_loss: 0.8264\n",
            "train_loss: 0.8263\n",
            "train_loss: 0.8261\n",
            "train_loss: 0.8256\n",
            "train_loss: 0.8251\n",
            "train_loss: 0.8252\n",
            "train_loss: 0.8252\n",
            "train_loss: 0.8248\n",
            "train_loss: 0.8248\n",
            "train_loss: 0.8248\n",
            "train_loss: 0.8245\n",
            "train_loss: 0.8243\n",
            "train_loss: 0.8242\n",
            "train_loss: 0.8242\n",
            "train_loss: 0.8239\n",
            "train_loss: 0.8240\n",
            "train_loss: 0.8239\n",
            "train_loss: 0.8237\n",
            "train_loss: 0.8236\n",
            "train_loss: 0.8235\n",
            "train_loss: 0.8238\n",
            "train_loss: 0.8235\n",
            "train_loss: 0.8234\n",
            "train_loss: 0.8232\n",
            "train_loss: 0.8235\n",
            "train_loss: 0.8231\n",
            "train_loss: 0.8230\n",
            "train_loss: 0.8229\n",
            "train_loss: 0.8230\n",
            "train_loss: 0.8229\n",
            "train_loss: 0.8228\n",
            "train_loss: 0.8228\n",
            "train_loss: 0.8230\n",
            "train_loss: 0.8225\n",
            "train_loss: 0.8230\n",
            "train_loss: 0.8226\n",
            "train_loss: 0.8222\n",
            "train_loss: 0.8224\n",
            "train_loss: 0.8224\n",
            "train_loss: 0.8227\n",
            "train_loss: 0.8222\n",
            "train_loss: 0.8222\n",
            "train_loss: 0.8224\n",
            "train_loss: 0.8221\n",
            "train_loss: 0.8223\n",
            "train_loss: 0.8223\n",
            "train_loss: 0.8222\n",
            "train_loss: 0.8222\n",
            "train_loss: 0.8220\n",
            "train_loss: 0.8221\n",
            "train_loss: 0.8223\n",
            "train_loss: 0.8218\n",
            "train_loss: 0.8217\n",
            "train_loss: 0.8219\n",
            "train_loss: 0.8219\n",
            "train_loss: 0.8219\n",
            "train_loss: 0.8218\n",
            "train_loss: 0.8218\n",
            "train_loss: 0.8220\n",
            "train_loss: 0.8218\n",
            "train_loss: 0.8220\n",
            "Finish training after 186 epochs\n",
            "CPU times: user 2min 38s, sys: 351 ms, total: 2min 38s\n",
            "Wall time: 1min 14s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# train2()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "plot_learning_curve(model_loss_record, title='MF model')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "01787b1a-86eb-4560-81ad-62766a56b2b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "np.sqrt(model_loss)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.969582354112455"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "np.sqrt(model_loss_record['dev'][-1])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9699897812767153"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "# del model\n",
        "# model = MF().to(device)\n",
        "# ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "# model.load_state_dict(ckpt)\n",
        "# # plot_pred(dv_set, model, device)  # Show prediction on the validation set\n",
        "# dev(dv_set, model, device)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "c2e65627-13cd-46f9-e51b-5973eb26d865"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ],
      "metadata": {
        "id": "aQikz3IPiyPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# def save_pred(preds, file):\n",
        "#     ''' Save predictions to specified file '''\n",
        "#     print('Saving results to {}'.format(file))\n",
        "#     with open(file, 'w') as fp:\n",
        "#         writer = csv.writer(fp)\n",
        "#         writer.writerow(['id', 'tested_positive'])\n",
        "#         for i, p in enumerate(preds):\n",
        "#             writer.writerow([i, p])\n",
        "\n",
        "# preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "# save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "30dfbdf5-3b47-4bec-a993-e306f49f47a5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hints**\n",
        "\n",
        "## **Simple Baseline**\n",
        "* Run sample code\n",
        "\n",
        "## **Medium Baseline**\n",
        "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
        "\n",
        "## **Strong Baseline**\n",
        "* Feature selection (what other features are useful?)\n",
        "* DNN architecture (layers? dimension? activation function?)\n",
        "* Training (mini-batch? optimizer? learning rate?)\n",
        "* L2 regularization\n",
        "* There are some mistakes in the sample code, can you find them?"
      ],
      "metadata": {
        "id": "nfrVxqJanGpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "This code is completely written by Heng-Jui Chang @ NTUEE.  \n",
        "Copying or reusing this code is required to specify the original author. \n",
        "\n",
        "E.g.  \n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ],
      "metadata": {
        "id": "9tmCwXgpot3t"
      }
    }
  ]
}