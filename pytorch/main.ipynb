{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b1d710d4a2dd0e836743a9708dcf2dd87750cb6db75a03dbc0a1931aaec4e6cb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from utils.mylib import *\n",
        "from d2l import torch as d2l"
      ],
      "outputs": [],
      "metadata": {
        "id": "k-onQd4JNA5H",
        "outputId": "0317f255-2dd4-4eeb-822f-ed20d8ea782b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "init_Seed()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU!\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class MLDataset(Dataset):\n",
        "    \"\"\" Dataset for loading and preprocessing the MoviesLen dataset. \"\"\"\n",
        "    def __init__(self, path, mode='train', target_only=False):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        data = np.loadtxt(path, dtype='long')\n",
        "\n",
        "        # Convert data into PyTorch tensors\n",
        "        self.data = torch.LongTensor(data[:, :2])\n",
        "        self.target = torch.FloatTensor(data[:, 2])\n",
        " \n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of MoviesLen Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users=1000, m_items=2000, n_factors=20):\n",
        "        super(MF, self).__init__()\n",
        "\n",
        "        self.U = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.V = torch.nn.Embedding(m_items, n_factors)\n",
        "\n",
        "        self.U.weight.data.uniform_(-0.005, 0.005)\n",
        "        self.V.weight.data.uniform_(-0.005, 0.005)\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        # return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
        "        return torch.einsum('ij, ij -> i', [self.U(user), self.V(item)])\n",
        "    \n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ],
      "metadata": {
        "id": "AlhTlkE7MDo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = MLDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "outputs": [],
      "metadata": {
        "id": "hlhLk5t6MBX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train/Dev/Test**"
      ],
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# U = torch.rand(size=(1005, 20), requires_grad=True)\n",
        "# V = torch.rand(size=(2005, 20), requires_grad=True)\n",
        "# with torch.no_grad():\n",
        "#     U /= 100\n",
        "#     V /= 100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "MAM8QecJOyqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# def model(X):\n",
        "#     return torch.einsum('ij, ij -> i', [U[X[:, 0]], V[X[:, 1]]])\n",
        "\n",
        "# def squared_loss(y_hat, y):  #@save\n",
        "#     \"\"\"Squared loss.\"\"\"\n",
        "#     return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "\n",
        "# def objective(X, y):\n",
        "#     return squared_loss(model(X), y).sum() + (U[X[:, 0]].norm() ** 2 + V[X[:, 1]].norm() ** 2) * 0.01\n",
        "\n",
        "# def sgd(params, lr, batch_size):  #@save\n",
        "#     \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "#     with torch.no_grad():\n",
        "#         for param in params:\n",
        "#             param -= lr * param.grad\n",
        "#             param.grad.zero_()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def train(tr_set, dv_set, model, config):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    \n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []} \n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_epochs:\n",
        "        model.train()\n",
        "        for X, y in tr_set:\n",
        "            optimizer.zero_grad()    \n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = model(X[:, 0], X[:, 1])\n",
        "            mse_loss = model.cal_loss(y_hat, y)\n",
        "            mse_loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "        \n",
        "        epoch += 1\n",
        "        \n",
        "        print(\"train_loss: {:.4f}\".format(np.mean(loss_record['train'][-100:])))\n",
        "\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            min_mse = dev_mse\n",
        "            early_stop_cnt = 0\n",
        "            print(\"Saving model (epoch = {:4d}  loss = {:.4f} )\".format(epoch, dev_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "        \n",
        "        \n",
        "        loss_record['dev'].append(dev_mse)\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            break\n",
        "\n",
        "    print(\"Finish training after {} epochs\".format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# def train2():\n",
        "#     n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "#     loss = squared_loss\n",
        "        \n",
        "#     # Setup optimizer\n",
        "#     optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "#         [U, V], **config['optim_hparas'])\n",
        "\n",
        "#     min_mse = 1000.\n",
        "#     loss_record = {'train': [], 'dev': []} \n",
        "#     early_stop_cnt = 0\n",
        "#     epoch = 0\n",
        "\n",
        "#     while epoch < n_epochs:\n",
        "\n",
        "#         for X, y in tr_set:\n",
        "#             optimizer.zero_grad()\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             y_hat = model(X)\n",
        "#             mse_loss = loss(y_hat, y)\n",
        "#             mse_loss.sum().backward()\n",
        "#             optimizer.step()\n",
        "            \n",
        "#         epoch += 1\n",
        "        \n",
        "#         with torch.no_grad():\n",
        "#             train_l = loss(model(X), y)\n",
        "#             print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation**"
      ],
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for X, y in dv_set:                         # iterate through the dataloader\n",
        "        X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(X[:, 0], X[:, 1])                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(X)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "yrxrD3YsN3U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "g0pdrhQAO41L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "aSBMRFlYN5tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ],
      "metadata": {
        "id": "SvckkF5dvf0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = False                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 2000,              # maximum number of epochs\n",
        "    'batch_size': 1000,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        # 'lr': 1,                 # learning rate of SGD\n",
        "        'weight_decay': 0.0001\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 30,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "NPXpdumwPjE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and model**"
      ],
      "metadata": {
        "id": "6j1eOV3TOH-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "tr_set = prep_dataloader('../data/ML100K/ML100K_copy1_train.txt', 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader('../data/ML100K/ML100K_copy1_test.txt', 'dev', config['batch_size'], target_only=target_only)\n",
        "# tt_set = prep_dataloader(\"data/ML100K/ML100K_copy1_test.txt\", 'test', config['batch_size'], target_only=target_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of MoviesLen Dataset (60000 samples found, each dim = 2)\n",
            "Finished reading the dev set of MoviesLen Dataset (20000 samples found, each dim = 2)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "model = MF().to(device) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Training!**"
      ],
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "2190df3d-28a1-4849-a9a8-5179cff3130d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "%%time\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 13.7021\n",
            "Saving model (epoch =    1  loss = 13.5217 )\n",
            "train_loss: 13.2614\n",
            "Saving model (epoch =    2  loss = 12.0933 )\n",
            "train_loss: 11.6354\n",
            "Saving model (epoch =    3  loss = 9.5159 )\n",
            "train_loss: 9.0623\n",
            "Saving model (epoch =    4  loss = 6.6277 )\n",
            "train_loss: 6.2636\n",
            "Saving model (epoch =    5  loss = 4.2326 )\n",
            "train_loss: 3.9970\n",
            "Saving model (epoch =    6  loss = 2.6963 )\n",
            "train_loss: 2.5579\n",
            "Saving model (epoch =    7  loss = 1.9019 )\n",
            "train_loss: 1.8066\n",
            "Saving model (epoch =    8  loss = 1.5314 )\n",
            "train_loss: 1.4391\n",
            "Saving model (epoch =    9  loss = 1.3464 )\n",
            "train_loss: 1.2573\n",
            "Saving model (epoch =   10  loss = 1.2383 )\n",
            "train_loss: 1.1523\n",
            "Saving model (epoch =   11  loss = 1.1696 )\n",
            "train_loss: 1.0756\n",
            "Saving model (epoch =   12  loss = 1.1220 )\n",
            "train_loss: 1.0323\n",
            "Saving model (epoch =   13  loss = 1.0880 )\n",
            "train_loss: 0.9948\n",
            "Saving model (epoch =   14  loss = 1.0619 )\n",
            "train_loss: 0.9704\n",
            "Saving model (epoch =   15  loss = 1.0429 )\n",
            "train_loss: 0.9480\n",
            "Saving model (epoch =   16  loss = 1.0273 )\n",
            "train_loss: 0.9319\n",
            "Saving model (epoch =   17  loss = 1.0160 )\n",
            "train_loss: 0.9203\n",
            "Saving model (epoch =   18  loss = 1.0068 )\n",
            "train_loss: 0.9089\n",
            "Saving model (epoch =   19  loss = 0.9991 )\n",
            "train_loss: 0.8997\n",
            "Saving model (epoch =   20  loss = 0.9932 )\n",
            "train_loss: 0.8954\n",
            "Saving model (epoch =   21  loss = 0.9879 )\n",
            "train_loss: 0.8912\n",
            "Saving model (epoch =   22  loss = 0.9838 )\n",
            "train_loss: 0.8854\n",
            "Saving model (epoch =   23  loss = 0.9808 )\n",
            "train_loss: 0.8804\n",
            "Saving model (epoch =   24  loss = 0.9784 )\n",
            "train_loss: 0.8767\n",
            "Saving model (epoch =   25  loss = 0.9756 )\n",
            "train_loss: 0.8735\n",
            "Saving model (epoch =   26  loss = 0.9733 )\n",
            "train_loss: 0.8725\n",
            "Saving model (epoch =   27  loss = 0.9716 )\n",
            "train_loss: 0.8662\n",
            "Saving model (epoch =   28  loss = 0.9702 )\n",
            "train_loss: 0.8679\n",
            "Saving model (epoch =   29  loss = 0.9691 )\n",
            "train_loss: 0.8682\n",
            "Saving model (epoch =   30  loss = 0.9678 )\n",
            "train_loss: 0.8651\n",
            "Saving model (epoch =   31  loss = 0.9667 )\n",
            "train_loss: 0.8691\n",
            "Saving model (epoch =   32  loss = 0.9664 )\n",
            "train_loss: 0.8635\n",
            "Saving model (epoch =   33  loss = 0.9650 )\n",
            "train_loss: 0.8596\n",
            "Saving model (epoch =   34  loss = 0.9645 )\n",
            "train_loss: 0.8621\n",
            "Saving model (epoch =   35  loss = 0.9638 )\n",
            "train_loss: 0.8615\n",
            "Saving model (epoch =   36  loss = 0.9636 )\n",
            "train_loss: 0.8536\n",
            "Saving model (epoch =   37  loss = 0.9630 )\n",
            "train_loss: 0.8589\n",
            "Saving model (epoch =   38  loss = 0.9627 )\n",
            "train_loss: 0.8573\n",
            "Saving model (epoch =   39  loss = 0.9622 )\n",
            "train_loss: 0.8593\n",
            "train_loss: 0.8574\n",
            "Saving model (epoch =   41  loss = 0.9615 )\n",
            "train_loss: 0.8543\n",
            "Saving model (epoch =   42  loss = 0.9611 )\n",
            "train_loss: 0.8550\n",
            "train_loss: 0.8565\n",
            "Saving model (epoch =   44  loss = 0.9605 )\n",
            "train_loss: 0.8593\n",
            "train_loss: 0.8550\n",
            "Saving model (epoch =   46  loss = 0.9603 )\n",
            "train_loss: 0.8558\n",
            "Saving model (epoch =   47  loss = 0.9598 )\n",
            "train_loss: 0.8546\n",
            "train_loss: 0.8540\n",
            "train_loss: 0.8542\n",
            "Saving model (epoch =   50  loss = 0.9595 )\n",
            "train_loss: 0.8560\n",
            "Saving model (epoch =   51  loss = 0.9594 )\n",
            "train_loss: 0.8531\n",
            "Saving model (epoch =   52  loss = 0.9591 )\n",
            "train_loss: 0.8555\n",
            "Saving model (epoch =   53  loss = 0.9590 )\n",
            "train_loss: 0.8547\n",
            "train_loss: 0.8527\n",
            "train_loss: 0.8548\n",
            "Saving model (epoch =   56  loss = 0.9586 )\n",
            "train_loss: 0.8513\n",
            "train_loss: 0.8552\n",
            "train_loss: 0.8544\n",
            "train_loss: 0.8532\n",
            "Saving model (epoch =   60  loss = 0.9585 )\n",
            "train_loss: 0.8506\n",
            "train_loss: 0.8521\n",
            "Saving model (epoch =   62  loss = 0.9584 )\n",
            "train_loss: 0.8530\n",
            "Saving model (epoch =   63  loss = 0.9582 )\n",
            "train_loss: 0.8495\n",
            "train_loss: 0.8506\n",
            "train_loss: 0.8507\n",
            "Saving model (epoch =   66  loss = 0.9581 )\n",
            "train_loss: 0.8549\n",
            "Saving model (epoch =   67  loss = 0.9580 )\n",
            "train_loss: 0.8490\n",
            "Saving model (epoch =   68  loss = 0.9575 )\n",
            "train_loss: 0.8526\n",
            "train_loss: 0.8520\n",
            "train_loss: 0.8519\n",
            "train_loss: 0.8503\n",
            "train_loss: 0.8503\n",
            "train_loss: 0.8491\n",
            "Saving model (epoch =   74  loss = 0.9574 )\n",
            "train_loss: 0.8485\n",
            "Saving model (epoch =   75  loss = 0.9572 )\n",
            "train_loss: 0.8490\n",
            "train_loss: 0.8508\n",
            "train_loss: 0.8481\n",
            "Saving model (epoch =   78  loss = 0.9570 )\n",
            "train_loss: 0.8481\n",
            "train_loss: 0.8480\n",
            "Saving model (epoch =   80  loss = 0.9564 )\n",
            "train_loss: 0.8481\n",
            "train_loss: 0.8453\n",
            "train_loss: 0.8488\n",
            "train_loss: 0.8492\n",
            "Saving model (epoch =   84  loss = 0.9561 )\n",
            "train_loss: 0.8454\n",
            "Saving model (epoch =   85  loss = 0.9561 )\n",
            "train_loss: 0.8480\n",
            "Saving model (epoch =   86  loss = 0.9559 )\n",
            "train_loss: 0.8455\n",
            "train_loss: 0.8446\n",
            "Saving model (epoch =   88  loss = 0.9553 )\n",
            "train_loss: 0.8426\n",
            "Saving model (epoch =   89  loss = 0.9552 )\n",
            "train_loss: 0.8445\n",
            "train_loss: 0.8422\n",
            "Saving model (epoch =   91  loss = 0.9541 )\n",
            "train_loss: 0.8416\n",
            "train_loss: 0.8416\n",
            "Saving model (epoch =   93  loss = 0.9539 )\n",
            "train_loss: 0.8403\n",
            "train_loss: 0.8410\n",
            "Saving model (epoch =   95  loss = 0.9534 )\n",
            "train_loss: 0.8393\n",
            "train_loss: 0.8390\n",
            "Saving model (epoch =   97  loss = 0.9525 )\n",
            "train_loss: 0.8362\n",
            "Saving model (epoch =   98  loss = 0.9524 )\n",
            "train_loss: 0.8353\n",
            "Saving model (epoch =   99  loss = 0.9521 )\n",
            "train_loss: 0.8362\n",
            "Saving model (epoch =  100  loss = 0.9512 )\n",
            "train_loss: 0.8327\n",
            "train_loss: 0.8311\n",
            "Saving model (epoch =  102  loss = 0.9508 )\n",
            "train_loss: 0.8317\n",
            "Saving model (epoch =  103  loss = 0.9502 )\n",
            "train_loss: 0.8318\n",
            "Saving model (epoch =  104  loss = 0.9495 )\n",
            "train_loss: 0.8284\n",
            "Saving model (epoch =  105  loss = 0.9492 )\n",
            "train_loss: 0.8277\n",
            "Saving model (epoch =  106  loss = 0.9483 )\n",
            "train_loss: 0.8278\n",
            "Saving model (epoch =  107  loss = 0.9482 )\n",
            "train_loss: 0.8219\n",
            "Saving model (epoch =  108  loss = 0.9471 )\n",
            "train_loss: 0.8251\n",
            "Saving model (epoch =  109  loss = 0.9465 )\n",
            "train_loss: 0.8204\n",
            "Saving model (epoch =  110  loss = 0.9462 )\n",
            "train_loss: 0.8215\n",
            "Saving model (epoch =  111  loss = 0.9450 )\n",
            "train_loss: 0.8210\n",
            "Saving model (epoch =  112  loss = 0.9440 )\n",
            "train_loss: 0.8187\n",
            "Saving model (epoch =  113  loss = 0.9434 )\n",
            "train_loss: 0.8132\n",
            "Saving model (epoch =  114  loss = 0.9427 )\n",
            "train_loss: 0.8082\n",
            "Saving model (epoch =  115  loss = 0.9423 )\n",
            "train_loss: 0.8081\n",
            "Saving model (epoch =  116  loss = 0.9408 )\n",
            "train_loss: 0.8080\n",
            "Saving model (epoch =  117  loss = 0.9399 )\n",
            "train_loss: 0.8031\n",
            "Saving model (epoch =  118  loss = 0.9391 )\n",
            "train_loss: 0.8019\n",
            "Saving model (epoch =  119  loss = 0.9383 )\n",
            "train_loss: 0.8009\n",
            "Saving model (epoch =  120  loss = 0.9370 )\n",
            "train_loss: 0.7968\n",
            "Saving model (epoch =  121  loss = 0.9363 )\n",
            "train_loss: 0.7933\n",
            "Saving model (epoch =  122  loss = 0.9355 )\n",
            "train_loss: 0.7905\n",
            "Saving model (epoch =  123  loss = 0.9346 )\n",
            "train_loss: 0.7888\n",
            "Saving model (epoch =  124  loss = 0.9342 )\n",
            "train_loss: 0.7851\n",
            "Saving model (epoch =  125  loss = 0.9327 )\n",
            "train_loss: 0.7839\n",
            "Saving model (epoch =  126  loss = 0.9320 )\n",
            "train_loss: 0.7803\n",
            "Saving model (epoch =  127  loss = 0.9307 )\n",
            "train_loss: 0.7764\n",
            "Saving model (epoch =  128  loss = 0.9299 )\n",
            "train_loss: 0.7733\n",
            "Saving model (epoch =  129  loss = 0.9287 )\n",
            "train_loss: 0.7699\n",
            "Saving model (epoch =  130  loss = 0.9284 )\n",
            "train_loss: 0.7707\n",
            "Saving model (epoch =  131  loss = 0.9270 )\n",
            "train_loss: 0.7665\n",
            "Saving model (epoch =  132  loss = 0.9259 )\n",
            "train_loss: 0.7630\n",
            "Saving model (epoch =  133  loss = 0.9254 )\n",
            "train_loss: 0.7588\n",
            "Saving model (epoch =  134  loss = 0.9244 )\n",
            "train_loss: 0.7588\n",
            "Saving model (epoch =  135  loss = 0.9239 )\n",
            "train_loss: 0.7529\n",
            "Saving model (epoch =  136  loss = 0.9231 )\n",
            "train_loss: 0.7479\n",
            "Saving model (epoch =  137  loss = 0.9224 )\n",
            "train_loss: 0.7450\n",
            "Saving model (epoch =  138  loss = 0.9208 )\n",
            "train_loss: 0.7427\n",
            "Saving model (epoch =  139  loss = 0.9204 )\n",
            "train_loss: 0.7399\n",
            "Saving model (epoch =  140  loss = 0.9202 )\n",
            "train_loss: 0.7368\n",
            "Saving model (epoch =  141  loss = 0.9195 )\n",
            "train_loss: 0.7331\n",
            "Saving model (epoch =  142  loss = 0.9184 )\n",
            "train_loss: 0.7317\n",
            "Saving model (epoch =  143  loss = 0.9181 )\n",
            "train_loss: 0.7245\n",
            "train_loss: 0.7261\n",
            "Saving model (epoch =  145  loss = 0.9174 )\n",
            "train_loss: 0.7198\n",
            "Saving model (epoch =  146  loss = 0.9165 )\n",
            "train_loss: 0.7141\n",
            "Saving model (epoch =  147  loss = 0.9161 )\n",
            "train_loss: 0.7132\n",
            "Saving model (epoch =  148  loss = 0.9157 )\n",
            "train_loss: 0.7090\n",
            "Saving model (epoch =  149  loss = 0.9147 )\n",
            "train_loss: 0.7084\n",
            "train_loss: 0.7006\n",
            "Saving model (epoch =  151  loss = 0.9139 )\n",
            "train_loss: 0.6981\n",
            "train_loss: 0.6965\n",
            "train_loss: 0.6902\n",
            "Saving model (epoch =  154  loss = 0.9139 )\n",
            "train_loss: 0.6886\n",
            "Saving model (epoch =  155  loss = 0.9136 )\n",
            "train_loss: 0.6856\n",
            "Saving model (epoch =  156  loss = 0.9131 )\n",
            "train_loss: 0.6809\n",
            "Saving model (epoch =  157  loss = 0.9130 )\n",
            "train_loss: 0.6776\n",
            "Saving model (epoch =  158  loss = 0.9121 )\n",
            "train_loss: 0.6716\n",
            "train_loss: 0.6685\n",
            "train_loss: 0.6673\n",
            "train_loss: 0.6605\n",
            "train_loss: 0.6560\n",
            "train_loss: 0.6527\n",
            "train_loss: 0.6485\n",
            "Saving model (epoch =  165  loss = 0.9119 )\n",
            "train_loss: 0.6452\n",
            "train_loss: 0.6403\n",
            "train_loss: 0.6375\n",
            "train_loss: 0.6350\n",
            "train_loss: 0.6289\n",
            "train_loss: 0.6272\n",
            "train_loss: 0.6208\n",
            "train_loss: 0.6185\n",
            "train_loss: 0.6126\n",
            "train_loss: 0.6077\n",
            "train_loss: 0.6064\n",
            "train_loss: 0.6018\n",
            "train_loss: 0.5970\n",
            "train_loss: 0.5925\n",
            "train_loss: 0.5877\n",
            "train_loss: 0.5836\n",
            "train_loss: 0.5785\n",
            "train_loss: 0.5737\n",
            "train_loss: 0.5729\n",
            "train_loss: 0.5692\n",
            "train_loss: 0.5630\n",
            "train_loss: 0.5606\n",
            "train_loss: 0.5566\n",
            "train_loss: 0.5513\n",
            "train_loss: 0.5482\n",
            "train_loss: 0.5417\n",
            "train_loss: 0.5395\n",
            "train_loss: 0.5347\n",
            "train_loss: 0.5329\n",
            "train_loss: 0.5294\n",
            "train_loss: 0.5231\n",
            "Finish training after 196 epochs\n",
            "CPU times: user 3min 15s, sys: 320 ms, total: 3min 15s\n",
            "Wall time: 2min 10s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# train2()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "plot_learning_curve(model_loss_record, title='MF model')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "01787b1a-86eb-4560-81ad-62766a56b2b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "%%time\n",
        "model_loss"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 4.77 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9119391322135926"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "model_loss_record['dev'][-1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.930759584903717"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "del model\n",
        "model = MF().to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "# plot_pred(dv_set, model, device)  # Show prediction on the validation set\n",
        "dev(dv_set, model, device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9119391322135926"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "c2e65627-13cd-46f9-e51b-5973eb26d865"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ],
      "metadata": {
        "id": "aQikz3IPiyPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# def save_pred(preds, file):\n",
        "#     ''' Save predictions to specified file '''\n",
        "#     print('Saving results to {}'.format(file))\n",
        "#     with open(file, 'w') as fp:\n",
        "#         writer = csv.writer(fp)\n",
        "#         writer.writerow(['id', 'tested_positive'])\n",
        "#         for i, p in enumerate(preds):\n",
        "#             writer.writerow([i, p])\n",
        "\n",
        "# preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "# save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "30dfbdf5-3b47-4bec-a993-e306f49f47a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "device = get_device()\n",
        "gg = torch.rand((10000, 10000)).to(device)\n",
        "tt = torch.rand((10000, 10000)).to(device)\n",
        "%time qq = gg @ tt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.7 s, sys: 180 ms, total: 13.8 s\n",
            "Wall time: 6.92 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hints**\n",
        "\n",
        "## **Simple Baseline**\n",
        "* Run sample code\n",
        "\n",
        "## **Medium Baseline**\n",
        "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
        "\n",
        "## **Strong Baseline**\n",
        "* Feature selection (what other features are useful?)\n",
        "* DNN architecture (layers? dimension? activation function?)\n",
        "* Training (mini-batch? optimizer? learning rate?)\n",
        "* L2 regularization\n",
        "* There are some mistakes in the sample code, can you find them?"
      ],
      "metadata": {
        "id": "nfrVxqJanGpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "This code is completely written by Heng-Jui Chang @ NTUEE.  \n",
        "Copying or reusing this code is required to specify the original author. \n",
        "\n",
        "E.g.  \n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ],
      "metadata": {
        "id": "9tmCwXgpot3t"
      }
    }
  ]
}