{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b1d710d4a2dd0e836743a9708dcf2dd87750cb6db75a03dbc0a1931aaec4e6cb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from utils.mylib import *\n",
        "from d2l import torch as d2l"
      ],
      "outputs": [],
      "metadata": {
        "id": "k-onQd4JNA5H",
        "outputId": "0317f255-2dd4-4eeb-822f-ed20d8ea782b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "init_Seed()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class MLDataset(Dataset):\n",
        "    \"\"\" Dataset for loading and preprocessing the MoviesLen dataset. \"\"\"\n",
        "    def __init__(self, path, mode='train', target_only=False):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        data = np.loadtxt(path, dtype='long')\n",
        "\n",
        "        # Convert data into PyTorch tensors\n",
        "        self.data = torch.LongTensor(data[:, :2])\n",
        "        self.target = torch.FloatTensor(data[:, 2])\n",
        "\n",
        "        print(f'Max user: {max(self.data[:, 0])}')\n",
        "        print(f'Max item: {max(self.data[:, 1])}')\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of MoviesLen Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users=6050, m_items=4001, n_factors=50):\n",
        "        super(MF, self).__init__()\n",
        "\n",
        "        self.U = torch.nn.Parameter(torch.rand(n_users, n_factors, requires_grad=True))\n",
        "\n",
        "        self.P = torch.rand(m_items, 500 * n_factors, requires_grad=False, device=device)\n",
        "        # self.Q = torch.nn.Parameter(torch.rand(n_factors * 500, n_factors, requires_grad=True))\n",
        "        \n",
        "        self.net = nn.Sequential(nn.Linear(500 * n_factors, n_factors), nn.ReLU(), nn.Linear(n_factors, n_factors))\n",
        "\n",
        "\n",
        "        self.init_net()\n",
        "\n",
        "        self.U.data.uniform_(-0.005, 0.005)\n",
        "        # self.Q.data.uniform_(-0.005, 0.005)\n",
        "        # self.Q.weight.data.uniform_(-0.005, 0.005)\n",
        "        \n",
        "        self.criterion = nn.MSELoss(reduction='sum')\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        # return torch.einsum('ij, ij -> i', [self.U[user], self.P[item] @ self.Q])\n",
        "        return torch.einsum('ij, ij -> i', [self.U[user], self.net(self.P[item])])\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)\n",
        "    \n",
        "    def init_net(self):\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                nn.init.normal_(m.weight, std=0.01)\n",
        "        self.net.apply(init_weights)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ],
      "metadata": {
        "id": "AlhTlkE7MDo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = MLDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(dataset, batch_size,\n",
        "        shuffle=(mode == 'train'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "outputs": [],
      "metadata": {
        "id": "hlhLk5t6MBX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train/Dev/Test**"
      ],
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# U = torch.rand(size=(1005, 20), requires_grad=True)\n",
        "# V = torch.rand(size=(2005, 20), requires_grad=True)\n",
        "# with torch.no_grad():\n",
        "#     U /= 100\n",
        "#     V /= 100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "MAM8QecJOyqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# def model(X):\n",
        "#     return torch.einsum('ij, ij -> i', [U[X[:, 0]], V[X[:, 1]]])\n",
        "\n",
        "# def squared_loss(y_hat, y):  #@save\n",
        "#     \"\"\"Squared loss.\"\"\"\n",
        "#     return (y_hat - y.reshape(y_hat.shape))**2 / 2\n",
        "\n",
        "# def objective(X, y):\n",
        "#     return squared_loss(model(X), y).sum() + (U[X[:, 0]].norm() ** 2 + V[X[:, 1]].norm() ** 2) * 0.01\n",
        "\n",
        "# def sgd(params, lr, batch_size):  #@save\n",
        "#     \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "#     with torch.no_grad():\n",
        "#         for param in params:\n",
        "#             param -= lr * param.grad\n",
        "#             param.grad.zero_()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def train(tr_set, dv_set, model, config):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []} \n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_epochs:\n",
        "        model.train()\n",
        "        for X, y in tr_set:\n",
        "            optimizer.zero_grad()    \n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = model(X[:, 0], X[:, 1])\n",
        "\n",
        "            mse_loss = model.cal_loss(y_hat, y)\n",
        "            mse_loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "            # loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item() / len(y))\n",
        "\n",
        "            # print(\"train_loss: {:.4f}\".format(mse_loss.detach().cpu().item()))\n",
        "            print(\"train_loss: {:.4f}\".format(mse_loss.detach().cpu().item() / len(y)))\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch += 1\n",
        "        \n",
        "        # print(\"train_loss: {:.4f}\".format(np.mean(loss_record['train'][-100:])))\n",
        "\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            min_mse = dev_mse\n",
        "            early_stop_cnt = 0\n",
        "            print(\"Saving model (epoch = {:4d}  loss = {:.4f} )\".format(epoch, dev_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "        \n",
        "        # if epoch > 10:\n",
        "        #     break\n",
        "        \n",
        "        loss_record['dev'].append(dev_mse)\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            break\n",
        "\n",
        "    print(\"Finish training after {} epochs\".format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation**"
      ],
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for X, y in dv_set:                         # iterate through the dataloader\n",
        "        X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(X[:, 0], X[:, 1])                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        # total_loss += mse_loss.detach().cpu().item() * len(y)    # accumulate loss\n",
        "        total_loss += mse_loss.detach().cpu().item()     # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "yrxrD3YsN3U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**"
      ],
      "metadata": {
        "id": "g0pdrhQAO41L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "outputs": [],
      "metadata": {
        "id": "aSBMRFlYN5tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ],
      "metadata": {
        "id": "SvckkF5dvf0j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = False               \n",
        "\n",
        "config = {\n",
        "    'n_epochs': 500,              # maximum number of epochs\n",
        "    'batch_size': 50000,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.004,                # learning rate\n",
        "        # 'weight_decay': 0.001\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 3,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth',  # your model will be saved here\n",
        "    'D': 50\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "NPXpdumwPjE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and model**"
      ],
      "metadata": {
        "id": "6j1eOV3TOH-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "ML100K_train = '../data/ML100K/ML100K_copy1_train.txt'\n",
        "ML100K_test = '../data/ML100K/ML100K_copy1_test.txt'\n",
        "\n",
        "ML1M_train = '../data/ML1M/ML1M_copy1_train.txt'\n",
        "ML1M_test = '../data/ML1M/ML1M_copy1_test.txt'\n",
        "\n",
        "tr_set = prep_dataloader(ML1M_train, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(ML1M_test, 'dev', config['batch_size'], target_only=target_only)\n",
        "# tt_set = prep_dataloader(\"data/ML100K/ML100K_copy1_test.txt\", 'test', config['batch_size'], target_only=target_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the train set of MoviesLen Dataset (600126 samples found, each dim = 2)\n",
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the dev set of MoviesLen Dataset (200041 samples found, each dim = 2)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "model = MF(n_factors=config['D']).to(device) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Training!**"
      ],
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "model.U.device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrEbUxazQAAZ",
        "outputId": "2190df3d-28a1-4849-a9a8-5179cff3130d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "%%time\n",
        "for _ in range(3):\n",
        "    model_loss, model_loss_record = train(tr_set, dv_set, model, config)\n",
        "    model.P = model.net(model.P).detach()\n",
        "    model.net = nn.Sequential(nn.Linear(config['D'], config['D']), nn.ReLU(), nn.Linear(config['D'], config['D']))\n",
        "    model = model.to(device)\n",
        "    model.init_net()\n",
        "    print(\"ending a era !!!\")\n",
        "\n",
        "# model_loss, model_loss_record = train(tr_set, dv_set, model, config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GG\n",
            "train_loss: 14.0722\n",
            "GG\n",
            "train_loss: 13.6791\n",
            "GG\n",
            "train_loss: 10.5648\n",
            "GG\n",
            "train_loss: 5.2651\n",
            "GG\n",
            "train_loss: 1.4082\n",
            "GG\n",
            "train_loss: 7.7563\n",
            "GG\n",
            "train_loss: 2.8222\n",
            "GG\n",
            "train_loss: 1.2835\n",
            "GG\n",
            "train_loss: 3.0040\n",
            "GG\n",
            "train_loss: 4.1878\n",
            "GG\n",
            "train_loss: 4.0640\n",
            "GG\n",
            "train_loss: 3.0569\n",
            "GG\n",
            "train_loss: 2.0347\n",
            "Saving model (epoch =    1  loss = 1.5166 )\n",
            "GG\n",
            "train_loss: 1.4872\n",
            "GG\n",
            "train_loss: 1.6425\n",
            "GG\n",
            "train_loss: 1.9372\n",
            "GG\n",
            "train_loss: 1.7861\n",
            "GG\n",
            "train_loss: 1.3400\n",
            "GG\n",
            "train_loss: 1.1087\n",
            "GG\n",
            "train_loss: 1.2791\n",
            "GG\n",
            "train_loss: 1.5125\n",
            "GG\n",
            "train_loss: 1.5857\n",
            "GG\n",
            "train_loss: 1.4672\n",
            "GG\n",
            "train_loss: 1.3157\n",
            "GG\n",
            "train_loss: 1.2715\n",
            "GG\n",
            "train_loss: 1.4025\n",
            "GG\n",
            "train_loss: 1.5279\n",
            "GG\n",
            "train_loss: 1.4658\n",
            "GG\n",
            "train_loss: 1.2068\n",
            "GG\n",
            "train_loss: 1.1352\n",
            "GG\n",
            "train_loss: 1.2716\n",
            "GG\n",
            "train_loss: 1.3944\n",
            "GG\n",
            "train_loss: 1.3705\n",
            "GG\n",
            "train_loss: 1.2334\n",
            "GG\n",
            "train_loss: 1.1559\n",
            "GG\n",
            "train_loss: 1.2218\n",
            "GG\n",
            "train_loss: 1.2936\n",
            "GG\n",
            "train_loss: 1.2207\n",
            "GG\n",
            "train_loss: 1.3610\n",
            "Saving model (epoch =    3  loss = 1.1205 )\n",
            "GG\n",
            "train_loss: 1.1122\n",
            "GG\n",
            "train_loss: 1.1286\n",
            "GG\n",
            "train_loss: 1.1703\n",
            "GG\n",
            "train_loss: 1.1772\n",
            "GG\n",
            "train_loss: 1.1419\n",
            "GG\n",
            "train_loss: 1.1391\n",
            "GG\n",
            "train_loss: 1.1492\n",
            "GG\n",
            "train_loss: 1.1351\n",
            "GG\n",
            "train_loss: 1.1202\n",
            "GG\n",
            "train_loss: 1.0883\n",
            "GG\n",
            "train_loss: 1.1027\n",
            "GG\n",
            "train_loss: 1.1157\n",
            "GG\n",
            "train_loss: 1.2046\n",
            "GG\n",
            "train_loss: 1.0986\n",
            "GG\n",
            "train_loss: 1.0964\n",
            "GG\n",
            "train_loss: 1.1156\n",
            "GG\n",
            "train_loss: 1.1029\n",
            "GG\n",
            "train_loss: 1.1232\n",
            "GG\n",
            "train_loss: 1.1039\n",
            "GG\n",
            "train_loss: 1.1004\n",
            "GG\n",
            "train_loss: 1.1088\n",
            "GG\n",
            "train_loss: 1.0938\n",
            "GG\n",
            "train_loss: 1.0983\n",
            "GG\n",
            "train_loss: 1.1041\n",
            "GG\n",
            "train_loss: 1.1149\n",
            "GG\n",
            "train_loss: 0.9958\n",
            "Saving model (epoch =    5  loss = 1.1013 )\n",
            "GG\n",
            "train_loss: 1.0844\n",
            "GG\n",
            "train_loss: 1.0816\n",
            "GG\n",
            "train_loss: 1.0836\n",
            "GG\n",
            "train_loss: 1.0981\n",
            "GG\n",
            "train_loss: 1.0924\n",
            "GG\n",
            "train_loss: 1.0921\n",
            "GG\n",
            "train_loss: 1.0870\n",
            "GG\n",
            "train_loss: 1.0979\n",
            "GG\n",
            "train_loss: 1.0849\n",
            "GG\n",
            "train_loss: 1.0833\n",
            "GG\n",
            "train_loss: 1.0865\n",
            "GG\n",
            "train_loss: 1.0915\n",
            "GG\n",
            "train_loss: 1.1175\n",
            "Saving model (epoch =    6  loss = 1.0854 )\n",
            "GG\n",
            "train_loss: 1.0631\n",
            "GG\n",
            "train_loss: 1.0715\n",
            "GG\n",
            "train_loss: 1.0739\n",
            "GG\n",
            "train_loss: 1.0831\n",
            "GG\n",
            "train_loss: 1.0883\n",
            "GG\n",
            "train_loss: 1.0770\n",
            "GG\n",
            "train_loss: 1.0780\n",
            "GG\n",
            "train_loss: 1.0888\n",
            "GG\n",
            "train_loss: 1.0801\n",
            "GG\n",
            "train_loss: 1.0777\n",
            "GG\n",
            "train_loss: 1.0912\n",
            "GG\n",
            "train_loss: 1.0887\n",
            "GG\n",
            "train_loss: 1.0562\n",
            "Saving model (epoch =    7  loss = 1.0793 )\n",
            "GG\n",
            "train_loss: 1.0588\n",
            "GG\n",
            "train_loss: 1.0642\n",
            "GG\n",
            "train_loss: 1.0815\n",
            "GG\n",
            "train_loss: 1.0713\n",
            "GG\n",
            "train_loss: 1.0705\n",
            "GG\n",
            "train_loss: 1.0837\n",
            "GG\n",
            "train_loss: 1.0746\n",
            "GG\n",
            "train_loss: 1.0766\n",
            "GG\n",
            "train_loss: 1.0901\n",
            "GG\n",
            "train_loss: 1.0794\n",
            "GG\n",
            "train_loss: 1.0678\n",
            "GG\n",
            "train_loss: 1.0762\n",
            "GG\n",
            "train_loss: 0.8643\n",
            "Saving model (epoch =    8  loss = 1.0728 )\n",
            "GG\n",
            "train_loss: 1.0458\n",
            "GG\n",
            "train_loss: 1.0701\n",
            "GG\n",
            "train_loss: 1.0720\n",
            "GG\n",
            "train_loss: 1.0694\n",
            "GG\n",
            "train_loss: 1.0819\n",
            "GG\n",
            "train_loss: 1.0775\n",
            "GG\n",
            "train_loss: 1.0751\n",
            "GG\n",
            "train_loss: 1.0724\n",
            "GG\n",
            "train_loss: 1.0722\n",
            "GG\n",
            "train_loss: 1.0594\n",
            "GG\n",
            "train_loss: 1.0683\n",
            "GG\n",
            "train_loss: 1.0717\n",
            "GG\n",
            "train_loss: 0.8849\n",
            "Saving model (epoch =    9  loss = 1.0665 )\n",
            "GG\n",
            "train_loss: 1.0371\n",
            "GG\n",
            "train_loss: 1.0519\n",
            "GG\n",
            "train_loss: 1.0665\n",
            "GG\n",
            "train_loss: 1.0696\n",
            "GG\n",
            "train_loss: 1.0608\n",
            "GG\n",
            "train_loss: 1.0670\n",
            "GG\n",
            "train_loss: 1.0771\n",
            "GG\n",
            "train_loss: 1.0655\n",
            "GG\n",
            "train_loss: 1.0812\n",
            "GG\n",
            "train_loss: 1.0647\n",
            "GG\n",
            "train_loss: 1.0542\n",
            "GG\n",
            "train_loss: 1.0722\n",
            "GG\n",
            "train_loss: 1.2046\n",
            "Saving model (epoch =   10  loss = 1.0632 )\n",
            "GG\n",
            "train_loss: 1.0452\n",
            "GG\n",
            "train_loss: 1.0386\n",
            "GG\n",
            "train_loss: 1.0543\n",
            "GG\n",
            "train_loss: 1.0525\n",
            "GG\n",
            "train_loss: 1.0587\n",
            "GG\n",
            "train_loss: 1.0655\n",
            "GG\n",
            "train_loss: 1.0454\n",
            "GG\n",
            "train_loss: 1.0683\n",
            "GG\n",
            "train_loss: 1.0632\n",
            "GG\n",
            "train_loss: 1.0597\n",
            "GG\n",
            "train_loss: 1.0496\n",
            "GG\n",
            "train_loss: 1.0709\n",
            "GG\n",
            "train_loss: 0.8729\n",
            "Saving model (epoch =   11  loss = 1.0523 )\n",
            "GG\n",
            "train_loss: 1.0400\n",
            "GG\n",
            "train_loss: 1.0396\n",
            "GG\n",
            "train_loss: 1.0453\n",
            "GG\n",
            "train_loss: 1.0498\n",
            "GG\n",
            "train_loss: 1.0463\n",
            "GG\n",
            "train_loss: 1.0510\n",
            "GG\n",
            "train_loss: 1.0566\n",
            "GG\n",
            "train_loss: 1.0488\n",
            "GG\n",
            "train_loss: 1.0637\n",
            "GG\n",
            "train_loss: 1.0524\n",
            "GG\n",
            "train_loss: 1.0468\n",
            "GG\n",
            "train_loss: 1.0530\n",
            "GG\n",
            "train_loss: 0.9595\n",
            "Saving model (epoch =   12  loss = 1.0470 )\n",
            "GG\n",
            "train_loss: 1.0357\n",
            "GG\n",
            "train_loss: 1.0239\n",
            "GG\n",
            "train_loss: 1.0341\n",
            "GG\n",
            "train_loss: 1.0483\n",
            "GG\n",
            "train_loss: 1.0518\n",
            "GG\n",
            "train_loss: 1.0429\n",
            "GG\n",
            "train_loss: 1.0378\n",
            "GG\n",
            "train_loss: 1.0539\n",
            "GG\n",
            "train_loss: 1.0473\n",
            "GG\n",
            "train_loss: 1.0463\n",
            "GG\n",
            "train_loss: 1.0575\n",
            "GG\n",
            "train_loss: 1.0359\n",
            "GG\n",
            "train_loss: 0.9917\n",
            "Saving model (epoch =   13  loss = 1.0403 )\n",
            "GG\n",
            "train_loss: 1.0152\n",
            "GG\n",
            "train_loss: 1.0274\n",
            "GG\n",
            "train_loss: 1.0363\n",
            "GG\n",
            "train_loss: 1.0280\n",
            "GG\n",
            "train_loss: 1.0329\n",
            "GG\n",
            "train_loss: 1.0435\n",
            "GG\n",
            "train_loss: 1.0544\n",
            "GG\n",
            "train_loss: 1.0454\n",
            "GG\n",
            "train_loss: 1.0394\n",
            "GG\n",
            "train_loss: 1.0405\n",
            "GG\n",
            "train_loss: 1.0427\n",
            "GG\n",
            "train_loss: 1.0341\n",
            "GG\n",
            "train_loss: 0.8713\n",
            "Saving model (epoch =   14  loss = 1.0331 )\n",
            "GG\n",
            "train_loss: 1.0176\n",
            "GG\n",
            "train_loss: 1.0173\n",
            "GG\n",
            "train_loss: 1.0199\n",
            "GG\n",
            "train_loss: 1.0273\n",
            "GG\n",
            "train_loss: 1.0261\n",
            "GG\n",
            "train_loss: 1.0346\n",
            "GG\n",
            "train_loss: 1.0290\n",
            "GG\n",
            "train_loss: 1.0396\n",
            "GG\n",
            "train_loss: 1.0382\n",
            "GG\n",
            "train_loss: 1.0259\n",
            "GG\n",
            "train_loss: 1.0348\n",
            "GG\n",
            "train_loss: 1.0352\n",
            "GG\n",
            "train_loss: 1.1185\n",
            "Saving model (epoch =   15  loss = 1.0263 )\n",
            "GG\n",
            "train_loss: 1.0032\n",
            "GG\n",
            "train_loss: 1.0039\n",
            "GG\n",
            "train_loss: 1.0057\n",
            "GG\n",
            "train_loss: 1.0160\n",
            "GG\n",
            "train_loss: 1.0164\n",
            "GG\n",
            "train_loss: 1.0328\n",
            "GG\n",
            "train_loss: 1.0277\n",
            "GG\n",
            "train_loss: 1.0328\n",
            "GG\n",
            "train_loss: 1.0212\n",
            "GG\n",
            "train_loss: 1.0237\n",
            "GG\n",
            "train_loss: 1.0279\n",
            "GG\n",
            "train_loss: 1.0204\n",
            "GG\n",
            "train_loss: 0.9783\n",
            "Saving model (epoch =   16  loss = 1.0155 )\n",
            "GG\n",
            "train_loss: 0.9851\n",
            "GG\n",
            "train_loss: 1.0007\n",
            "GG\n",
            "train_loss: 1.0026\n",
            "GG\n",
            "train_loss: 1.0051\n",
            "GG\n",
            "train_loss: 1.0107\n",
            "GG\n",
            "train_loss: 1.0136\n",
            "GG\n",
            "train_loss: 1.0208\n",
            "GG\n",
            "train_loss: 1.0178\n",
            "GG\n",
            "train_loss: 1.0249\n",
            "GG\n",
            "train_loss: 1.0297\n",
            "GG\n",
            "train_loss: 0.9985\n",
            "GG\n",
            "train_loss: 1.0139\n",
            "GG\n",
            "train_loss: 1.2170\n",
            "Saving model (epoch =   17  loss = 1.0078 )\n",
            "GG\n",
            "train_loss: 0.9849\n",
            "GG\n",
            "train_loss: 0.9838\n",
            "GG\n",
            "train_loss: 0.9999\n",
            "GG\n",
            "train_loss: 0.9873\n",
            "GG\n",
            "train_loss: 0.9953\n",
            "GG\n",
            "train_loss: 1.0068\n",
            "GG\n",
            "train_loss: 1.0230\n",
            "GG\n",
            "train_loss: 1.0029\n",
            "GG\n",
            "train_loss: 1.0058\n",
            "GG\n",
            "train_loss: 1.0130\n",
            "GG\n",
            "train_loss: 1.0163\n",
            "GG\n",
            "train_loss: 0.9988\n",
            "GG\n",
            "train_loss: 1.0371\n",
            "Saving model (epoch =   18  loss = 1.0003 )\n",
            "GG\n",
            "train_loss: 0.9825\n",
            "GG\n",
            "train_loss: 0.9773\n",
            "GG\n",
            "train_loss: 0.9855\n",
            "GG\n",
            "train_loss: 0.9891\n",
            "GG\n",
            "train_loss: 0.9915\n",
            "GG\n",
            "train_loss: 0.9808\n",
            "GG\n",
            "train_loss: 1.0030\n",
            "GG\n",
            "train_loss: 1.0078\n",
            "GG\n",
            "train_loss: 1.0043\n",
            "GG\n",
            "train_loss: 0.9964\n",
            "GG\n",
            "train_loss: 0.9945\n",
            "GG\n",
            "train_loss: 0.9935\n",
            "GG\n",
            "train_loss: 0.9185\n",
            "Saving model (epoch =   19  loss = 0.9913 )\n",
            "GG\n",
            "train_loss: 0.9655\n",
            "GG\n",
            "train_loss: 0.9749\n",
            "GG\n",
            "train_loss: 0.9806\n",
            "GG\n",
            "train_loss: 0.9781\n",
            "GG\n",
            "train_loss: 0.9840\n",
            "GG\n",
            "train_loss: 0.9859\n",
            "GG\n",
            "train_loss: 0.9874\n",
            "GG\n",
            "train_loss: 0.9954\n",
            "GG\n",
            "train_loss: 0.9992\n",
            "GG\n",
            "train_loss: 0.9850\n",
            "GG\n",
            "train_loss: 0.9839\n",
            "GG\n",
            "train_loss: 0.9850\n",
            "GG\n",
            "train_loss: 0.7400\n",
            "Saving model (epoch =   20  loss = 0.9826 )\n",
            "GG\n",
            "train_loss: 0.9544\n",
            "GG\n",
            "train_loss: 0.9676\n",
            "GG\n",
            "train_loss: 0.9607\n",
            "GG\n",
            "train_loss: 0.9680\n",
            "GG\n",
            "train_loss: 0.9665\n",
            "GG\n",
            "train_loss: 0.9640\n",
            "GG\n",
            "train_loss: 0.9807\n",
            "GG\n",
            "train_loss: 0.9792\n",
            "GG\n",
            "train_loss: 0.9803\n",
            "GG\n",
            "train_loss: 0.9788\n",
            "GG\n",
            "train_loss: 0.9858\n",
            "GG\n",
            "train_loss: 0.9795\n",
            "GG\n",
            "train_loss: 1.0916\n",
            "Saving model (epoch =   21  loss = 0.9722 )\n",
            "GG\n",
            "train_loss: 0.9615\n",
            "GG\n",
            "train_loss: 0.9636\n",
            "GG\n",
            "train_loss: 0.9481\n",
            "GG\n",
            "train_loss: 0.9525\n",
            "GG\n",
            "train_loss: 0.9552\n",
            "GG\n",
            "train_loss: 0.9639\n",
            "GG\n",
            "train_loss: 0.9588\n",
            "GG\n",
            "train_loss: 0.9691\n",
            "GG\n",
            "train_loss: 0.9584\n",
            "GG\n",
            "train_loss: 0.9696\n",
            "GG\n",
            "train_loss: 0.9798\n",
            "GG\n",
            "train_loss: 0.9727\n",
            "GG\n",
            "train_loss: 0.9701\n",
            "Saving model (epoch =   22  loss = 0.9668 )\n",
            "GG\n",
            "train_loss: 0.9457\n",
            "GG\n",
            "train_loss: 0.9391\n",
            "GG\n",
            "train_loss: 0.9473\n",
            "GG\n",
            "train_loss: 0.9457\n",
            "GG\n",
            "train_loss: 0.9534\n",
            "GG\n",
            "train_loss: 0.9560\n",
            "GG\n",
            "train_loss: 0.9521\n",
            "GG\n",
            "train_loss: 0.9509\n",
            "GG\n",
            "train_loss: 0.9702\n",
            "GG\n",
            "train_loss: 0.9615\n",
            "GG\n",
            "train_loss: 0.9661\n",
            "GG\n",
            "train_loss: 0.9569\n",
            "GG\n",
            "train_loss: 1.0429\n",
            "Saving model (epoch =   23  loss = 0.9632 )\n",
            "GG\n",
            "train_loss: 0.9296\n",
            "GG\n",
            "train_loss: 0.9399\n",
            "GG\n",
            "train_loss: 0.9530\n",
            "GG\n",
            "train_loss: 0.9344\n",
            "GG\n",
            "train_loss: 0.9408\n",
            "GG\n",
            "train_loss: 0.9474\n",
            "GG\n",
            "train_loss: 0.9445\n",
            "GG\n",
            "train_loss: 0.9533\n",
            "GG\n",
            "train_loss: 0.9435\n",
            "GG\n",
            "train_loss: 0.9661\n",
            "GG\n",
            "train_loss: 0.9568\n",
            "GG\n",
            "train_loss: 0.9559\n",
            "GG\n",
            "train_loss: 0.8257\n",
            "Saving model (epoch =   24  loss = 0.9470 )\n",
            "GG\n",
            "train_loss: 0.9334\n",
            "GG\n",
            "train_loss: 0.9154\n",
            "GG\n",
            "train_loss: 0.9241\n",
            "GG\n",
            "train_loss: 0.9199\n",
            "GG\n",
            "train_loss: 0.9400\n",
            "GG\n",
            "train_loss: 0.9288\n",
            "GG\n",
            "train_loss: 0.9298\n",
            "GG\n",
            "train_loss: 0.9596\n",
            "GG\n",
            "train_loss: 0.9360\n",
            "GG\n",
            "train_loss: 0.9525\n",
            "GG\n",
            "train_loss: 0.9419\n",
            "GG\n",
            "train_loss: 0.9479\n",
            "GG\n",
            "train_loss: 0.9545\n",
            "Saving model (epoch =   25  loss = 0.9391 )\n",
            "GG\n",
            "train_loss: 0.9091\n",
            "GG\n",
            "train_loss: 0.9204\n",
            "GG\n",
            "train_loss: 0.9151\n",
            "GG\n",
            "train_loss: 0.9263\n",
            "GG\n",
            "train_loss: 0.9222\n",
            "GG\n",
            "train_loss: 0.9373\n",
            "GG\n",
            "train_loss: 0.9243\n",
            "GG\n",
            "train_loss: 0.9239\n",
            "GG\n",
            "train_loss: 0.9350\n",
            "GG\n",
            "train_loss: 0.9345\n",
            "GG\n",
            "train_loss: 0.9369\n",
            "GG\n",
            "train_loss: 0.9346\n",
            "GG\n",
            "train_loss: 1.0791\n",
            "Saving model (epoch =   26  loss = 0.9315 )\n",
            "GG\n",
            "train_loss: 0.9007\n",
            "GG\n",
            "train_loss: 0.9091\n",
            "GG\n",
            "train_loss: 0.9142\n",
            "GG\n",
            "train_loss: 0.9105\n",
            "GG\n",
            "train_loss: 0.9125\n",
            "GG\n",
            "train_loss: 0.9325\n",
            "GG\n",
            "train_loss: 0.9158\n",
            "GG\n",
            "train_loss: 0.9221\n",
            "GG\n",
            "train_loss: 0.9252\n",
            "GG\n",
            "train_loss: 0.9248\n",
            "GG\n",
            "train_loss: 0.9218\n",
            "GG\n",
            "train_loss: 0.9328\n",
            "GG\n",
            "train_loss: 0.9816\n",
            "Saving model (epoch =   27  loss = 0.9303 )\n",
            "GG\n",
            "train_loss: 0.9019\n",
            "GG\n",
            "train_loss: 0.9069\n",
            "GG\n",
            "train_loss: 0.9095\n",
            "GG\n",
            "train_loss: 0.9230\n",
            "GG\n",
            "train_loss: 0.9114\n",
            "GG\n",
            "train_loss: 0.9104\n",
            "GG\n",
            "train_loss: 0.9261\n",
            "GG\n",
            "train_loss: 0.9210\n",
            "GG\n",
            "train_loss: 0.9207\n",
            "GG\n",
            "train_loss: 0.9409\n",
            "GG\n",
            "train_loss: 0.9243\n",
            "GG\n",
            "train_loss: 0.9212\n",
            "GG\n",
            "train_loss: 0.9809\n",
            "GG\n",
            "train_loss: 0.9929\n",
            "GG\n",
            "train_loss: 1.0674\n",
            "GG\n",
            "train_loss: 0.9159\n",
            "GG\n",
            "train_loss: 0.9758\n",
            "GG\n",
            "train_loss: 1.0179\n",
            "GG\n",
            "train_loss: 0.9003\n",
            "GG\n",
            "train_loss: 0.9897\n",
            "GG\n",
            "train_loss: 0.9650\n",
            "GG\n",
            "train_loss: 0.9224\n",
            "GG\n",
            "train_loss: 0.9799\n",
            "GG\n",
            "train_loss: 0.9434\n",
            "GG\n",
            "train_loss: 0.9320\n",
            "GG\n",
            "train_loss: 1.0263\n",
            "GG\n",
            "train_loss: 1.1963\n",
            "GG\n",
            "train_loss: 1.1312\n",
            "GG\n",
            "train_loss: 0.9438\n",
            "GG\n",
            "train_loss: 1.1737\n",
            "GG\n",
            "train_loss: 0.8958\n",
            "GG\n",
            "train_loss: 1.1321\n",
            "GG\n",
            "train_loss: 0.9122\n",
            "GG\n",
            "train_loss: 1.0677\n",
            "GG\n",
            "train_loss: 0.9220\n",
            "GG\n",
            "train_loss: 1.0476\n",
            "GG\n",
            "train_loss: 0.9315\n",
            "GG\n",
            "train_loss: 1.0120\n",
            "GG\n",
            "train_loss: 0.9482\n",
            "GG\n",
            "train_loss: 1.1078\n",
            "GG\n",
            "train_loss: 0.8975\n",
            "GG\n",
            "train_loss: 1.0584\n",
            "GG\n",
            "train_loss: 0.9145\n",
            "GG\n",
            "train_loss: 1.0151\n",
            "GG\n",
            "train_loss: 0.9611\n",
            "GG\n",
            "train_loss: 0.9548\n",
            "GG\n",
            "train_loss: 0.9861\n",
            "GG\n",
            "train_loss: 0.9198\n",
            "GG\n",
            "train_loss: 0.9991\n",
            "GG\n",
            "train_loss: 0.9225\n",
            "GG\n",
            "train_loss: 0.9732\n",
            "GG\n",
            "train_loss: 0.9999\n",
            "Saving model (epoch =   31  loss = 0.9224 )\n",
            "GG\n",
            "train_loss: 0.8834\n",
            "GG\n",
            "train_loss: 0.9257\n",
            "GG\n",
            "train_loss: 0.9159\n",
            "GG\n",
            "train_loss: 0.9157\n",
            "GG\n",
            "train_loss: 0.9188\n",
            "GG\n",
            "train_loss: 0.9131\n",
            "GG\n",
            "train_loss: 0.9237\n",
            "GG\n",
            "train_loss: 0.9099\n",
            "GG\n",
            "train_loss: 0.9250\n",
            "GG\n",
            "train_loss: 0.9059\n",
            "GG\n",
            "train_loss: 0.9240\n",
            "GG\n",
            "train_loss: 0.9145\n",
            "GG\n",
            "train_loss: 0.7974\n",
            "GG\n",
            "train_loss: 0.9157\n",
            "GG\n",
            "train_loss: 0.8954\n",
            "GG\n",
            "train_loss: 0.9061\n",
            "GG\n",
            "train_loss: 0.9004\n",
            "GG\n",
            "train_loss: 0.9070\n",
            "GG\n",
            "train_loss: 0.9116\n",
            "GG\n",
            "train_loss: 0.9102\n",
            "GG\n",
            "train_loss: 0.9141\n",
            "GG\n",
            "train_loss: 0.8945\n",
            "GG\n",
            "train_loss: 0.9241\n",
            "GG\n",
            "train_loss: 0.9092\n",
            "GG\n",
            "train_loss: 0.9150\n",
            "GG\n",
            "train_loss: 0.7964\n",
            "Saving model (epoch =   33  loss = 0.9123 )\n",
            "GG\n",
            "train_loss: 0.8946\n",
            "GG\n",
            "train_loss: 0.8971\n",
            "GG\n",
            "train_loss: 0.8888\n",
            "GG\n",
            "train_loss: 0.8892\n",
            "GG\n",
            "train_loss: 0.8927\n",
            "GG\n",
            "train_loss: 0.9002\n",
            "GG\n",
            "train_loss: 0.8970\n",
            "GG\n",
            "train_loss: 0.9042\n",
            "GG\n",
            "train_loss: 0.9070\n",
            "GG\n",
            "train_loss: 0.9089\n",
            "GG\n",
            "train_loss: 0.9018\n",
            "GG\n",
            "train_loss: 0.9003\n",
            "GG\n",
            "train_loss: 0.9735\n",
            "Saving model (epoch =   34  loss = 0.9071 )\n",
            "GG\n",
            "train_loss: 0.8886\n",
            "GG\n",
            "train_loss: 0.8807\n",
            "GG\n",
            "train_loss: 0.8804\n",
            "GG\n",
            "train_loss: 0.8907\n",
            "GG\n",
            "train_loss: 0.8890\n",
            "GG\n",
            "train_loss: 0.8915\n",
            "GG\n",
            "train_loss: 0.8958\n",
            "GG\n",
            "train_loss: 0.8932\n",
            "GG\n",
            "train_loss: 0.9015\n",
            "GG\n",
            "train_loss: 0.9035\n",
            "GG\n",
            "train_loss: 0.9002\n",
            "GG\n",
            "train_loss: 0.8973\n",
            "GG\n",
            "train_loss: 0.9059\n",
            "Saving model (epoch =   35  loss = 0.9031 )\n",
            "GG\n",
            "train_loss: 0.8754\n",
            "GG\n",
            "train_loss: 0.8862\n",
            "GG\n",
            "train_loss: 0.8825\n",
            "GG\n",
            "train_loss: 0.8792\n",
            "GG\n",
            "train_loss: 0.8916\n",
            "GG\n",
            "train_loss: 0.8923\n",
            "GG\n",
            "train_loss: 0.8817\n",
            "GG\n",
            "train_loss: 0.8904\n",
            "GG\n",
            "train_loss: 0.8936\n",
            "GG\n",
            "train_loss: 0.8910\n",
            "GG\n",
            "train_loss: 0.8913\n",
            "GG\n",
            "train_loss: 0.8921\n",
            "GG\n",
            "train_loss: 0.9275\n",
            "GG\n",
            "train_loss: 0.8832\n",
            "GG\n",
            "train_loss: 0.8702\n",
            "GG\n",
            "train_loss: 0.8866\n",
            "GG\n",
            "train_loss: 0.8787\n",
            "GG\n",
            "train_loss: 0.8870\n",
            "GG\n",
            "train_loss: 0.8857\n",
            "GG\n",
            "train_loss: 0.8941\n",
            "GG\n",
            "train_loss: 0.9002\n",
            "GG\n",
            "train_loss: 0.8801\n",
            "GG\n",
            "train_loss: 0.8822\n",
            "GG\n",
            "train_loss: 0.8920\n",
            "GG\n",
            "train_loss: 0.8942\n",
            "GG\n",
            "train_loss: 0.9697\n",
            "Saving model (epoch =   37  loss = 0.8966 )\n",
            "GG\n",
            "train_loss: 0.8784\n",
            "GG\n",
            "train_loss: 0.8696\n",
            "GG\n",
            "train_loss: 0.8710\n",
            "GG\n",
            "train_loss: 0.8711\n",
            "GG\n",
            "train_loss: 0.8830\n",
            "GG\n",
            "train_loss: 0.8760\n",
            "GG\n",
            "train_loss: 0.8843\n",
            "GG\n",
            "train_loss: 0.8888\n",
            "GG\n",
            "train_loss: 0.8919\n",
            "GG\n",
            "train_loss: 0.8881\n",
            "GG\n",
            "train_loss: 0.8865\n",
            "GG\n",
            "train_loss: 0.8922\n",
            "GG\n",
            "train_loss: 0.8080\n",
            "GG\n",
            "train_loss: 0.8688\n",
            "GG\n",
            "train_loss: 0.8685\n",
            "GG\n",
            "train_loss: 0.8739\n",
            "GG\n",
            "train_loss: 0.8816\n",
            "GG\n",
            "train_loss: 0.8772\n",
            "GG\n",
            "train_loss: 0.8787\n",
            "GG\n",
            "train_loss: 0.8750\n",
            "GG\n",
            "train_loss: 0.8803\n",
            "GG\n",
            "train_loss: 0.8848\n",
            "GG\n",
            "train_loss: 0.8902\n",
            "GG\n",
            "train_loss: 0.8797\n",
            "GG\n",
            "train_loss: 0.8880\n",
            "GG\n",
            "train_loss: 1.0024\n",
            "GG\n",
            "train_loss: 0.8929\n",
            "GG\n",
            "train_loss: 0.8730\n",
            "GG\n",
            "train_loss: 0.8690\n",
            "GG\n",
            "train_loss: 0.8778\n",
            "GG\n",
            "train_loss: 0.8820\n",
            "GG\n",
            "train_loss: 0.8796\n",
            "GG\n",
            "train_loss: 0.8735\n",
            "GG\n",
            "train_loss: 0.8793\n",
            "GG\n",
            "train_loss: 0.8798\n",
            "GG\n",
            "train_loss: 0.8805\n",
            "GG\n",
            "train_loss: 0.8823\n",
            "GG\n",
            "train_loss: 0.8796\n",
            "GG\n",
            "train_loss: 0.9874\n",
            "GG\n",
            "train_loss: 0.8705\n",
            "GG\n",
            "train_loss: 0.8676\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8791\n",
            "GG\n",
            "train_loss: 0.8708\n",
            "GG\n",
            "train_loss: 0.8803\n",
            "GG\n",
            "train_loss: 0.8665\n",
            "GG\n",
            "train_loss: 0.8894\n",
            "GG\n",
            "train_loss: 0.8670\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8747\n",
            "GG\n",
            "train_loss: 0.8841\n",
            "GG\n",
            "train_loss: 0.8020\n",
            "Finish training after 41 epochs\n",
            "ending a era !!!\n",
            "GG\n",
            "train_loss: 14.1429\n",
            "GG\n",
            "train_loss: 13.8688\n",
            "GG\n",
            "train_loss: 13.5952\n",
            "GG\n",
            "train_loss: 13.0603\n",
            "GG\n",
            "train_loss: 12.4052\n",
            "GG\n",
            "train_loss: 11.4588\n",
            "GG\n",
            "train_loss: 10.1355\n",
            "GG\n",
            "train_loss: 8.5806\n",
            "GG\n",
            "train_loss: 6.6926\n",
            "GG\n",
            "train_loss: 4.7343\n",
            "GG\n",
            "train_loss: 2.8911\n",
            "GG\n",
            "train_loss: 1.4436\n",
            "GG\n",
            "train_loss: 0.9130\n",
            "Saving model (epoch =    1  loss = 1.7753 )\n",
            "GG\n",
            "train_loss: 1.7356\n",
            "GG\n",
            "train_loss: 3.1685\n",
            "GG\n",
            "train_loss: 3.7337\n",
            "GG\n",
            "train_loss: 3.2300\n",
            "GG\n",
            "train_loss: 2.3259\n",
            "GG\n",
            "train_loss: 1.4907\n",
            "GG\n",
            "train_loss: 0.9880\n",
            "GG\n",
            "train_loss: 0.8798\n",
            "GG\n",
            "train_loss: 1.0489\n",
            "GG\n",
            "train_loss: 1.3249\n",
            "GG\n",
            "train_loss: 1.5828\n",
            "GG\n",
            "train_loss: 1.7638\n",
            "GG\n",
            "train_loss: 1.8438\n",
            "GG\n",
            "train_loss: 1.8702\n",
            "GG\n",
            "train_loss: 1.8083\n",
            "GG\n",
            "train_loss: 1.6224\n",
            "GG\n",
            "train_loss: 1.4078\n",
            "GG\n",
            "train_loss: 1.1850\n",
            "GG\n",
            "train_loss: 0.9939\n",
            "GG\n",
            "train_loss: 0.9100\n",
            "GG\n",
            "train_loss: 0.9194\n",
            "GG\n",
            "train_loss: 1.0229\n",
            "GG\n",
            "train_loss: 1.1373\n",
            "GG\n",
            "train_loss: 1.2036\n",
            "GG\n",
            "train_loss: 1.2213\n",
            "GG\n",
            "train_loss: 1.1371\n",
            "Saving model (epoch =    3  loss = 1.1081 )\n",
            "GG\n",
            "train_loss: 1.0857\n",
            "GG\n",
            "train_loss: 1.0016\n",
            "GG\n",
            "train_loss: 0.9101\n",
            "GG\n",
            "train_loss: 0.8797\n",
            "GG\n",
            "train_loss: 0.8741\n",
            "GG\n",
            "train_loss: 0.9017\n",
            "GG\n",
            "train_loss: 0.9435\n",
            "GG\n",
            "train_loss: 0.9807\n",
            "GG\n",
            "train_loss: 0.9911\n",
            "GG\n",
            "train_loss: 0.9702\n",
            "GG\n",
            "train_loss: 0.9572\n",
            "GG\n",
            "train_loss: 0.9274\n",
            "GG\n",
            "train_loss: 0.6464\n",
            "Saving model (epoch =    4  loss = 0.8931 )\n",
            "GG\n",
            "train_loss: 0.8768\n",
            "GG\n",
            "train_loss: 0.8720\n",
            "GG\n",
            "train_loss: 0.8763\n",
            "GG\n",
            "train_loss: 0.8916\n",
            "GG\n",
            "train_loss: 0.8966\n",
            "GG\n",
            "train_loss: 0.9040\n",
            "GG\n",
            "train_loss: 0.9016\n",
            "GG\n",
            "train_loss: 0.9007\n",
            "GG\n",
            "train_loss: 0.8715\n",
            "GG\n",
            "train_loss: 0.8844\n",
            "GG\n",
            "train_loss: 0.8681\n",
            "GG\n",
            "train_loss: 0.8910\n",
            "GG\n",
            "train_loss: 0.6693\n",
            "GG\n",
            "train_loss: 0.8851\n",
            "GG\n",
            "train_loss: 0.8794\n",
            "GG\n",
            "train_loss: 0.8804\n",
            "GG\n",
            "train_loss: 0.8779\n",
            "GG\n",
            "train_loss: 0.8781\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.8714\n",
            "GG\n",
            "train_loss: 0.8779\n",
            "GG\n",
            "train_loss: 0.8886\n",
            "GG\n",
            "train_loss: 0.8877\n",
            "GG\n",
            "train_loss: 0.8829\n",
            "GG\n",
            "train_loss: 0.8881\n",
            "GG\n",
            "train_loss: 0.9462\n",
            "Saving model (epoch =    6  loss = 0.8911 )\n",
            "GG\n",
            "train_loss: 0.8632\n",
            "GG\n",
            "train_loss: 0.8736\n",
            "GG\n",
            "train_loss: 0.8627\n",
            "GG\n",
            "train_loss: 0.8657\n",
            "GG\n",
            "train_loss: 0.8732\n",
            "GG\n",
            "train_loss: 0.8782\n",
            "GG\n",
            "train_loss: 0.8801\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8764\n",
            "GG\n",
            "train_loss: 0.8842\n",
            "GG\n",
            "train_loss: 0.8882\n",
            "GG\n",
            "train_loss: 0.8748\n",
            "GG\n",
            "train_loss: 1.1476\n",
            "Saving model (epoch =    7  loss = 0.8875 )\n",
            "GG\n",
            "train_loss: 0.8600\n",
            "GG\n",
            "train_loss: 0.8738\n",
            "GG\n",
            "train_loss: 0.8682\n",
            "GG\n",
            "train_loss: 0.8653\n",
            "GG\n",
            "train_loss: 0.8735\n",
            "GG\n",
            "train_loss: 0.8676\n",
            "GG\n",
            "train_loss: 0.8703\n",
            "GG\n",
            "train_loss: 0.8783\n",
            "GG\n",
            "train_loss: 0.8754\n",
            "GG\n",
            "train_loss: 0.8835\n",
            "GG\n",
            "train_loss: 0.8883\n",
            "GG\n",
            "train_loss: 0.8875\n",
            "GG\n",
            "train_loss: 0.6770\n",
            "Saving model (epoch =    8  loss = 0.8873 )\n",
            "GG\n",
            "train_loss: 0.8687\n",
            "GG\n",
            "train_loss: 0.8660\n",
            "GG\n",
            "train_loss: 0.8561\n",
            "GG\n",
            "train_loss: 0.8686\n",
            "GG\n",
            "train_loss: 0.8679\n",
            "GG\n",
            "train_loss: 0.8776\n",
            "GG\n",
            "train_loss: 0.8794\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8759\n",
            "GG\n",
            "train_loss: 0.8855\n",
            "GG\n",
            "train_loss: 0.8853\n",
            "GG\n",
            "train_loss: 0.8821\n",
            "GG\n",
            "train_loss: 0.7911\n",
            "Saving model (epoch =    9  loss = 0.8873 )\n",
            "GG\n",
            "train_loss: 0.8671\n",
            "GG\n",
            "train_loss: 0.8655\n",
            "GG\n",
            "train_loss: 0.8614\n",
            "GG\n",
            "train_loss: 0.8641\n",
            "GG\n",
            "train_loss: 0.8724\n",
            "GG\n",
            "train_loss: 0.8695\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8906\n",
            "GG\n",
            "train_loss: 0.8953\n",
            "GG\n",
            "train_loss: 0.8856\n",
            "GG\n",
            "train_loss: 0.7649\n",
            "Saving model (epoch =   10  loss = 0.8869 )\n",
            "GG\n",
            "train_loss: 0.8539\n",
            "GG\n",
            "train_loss: 0.8609\n",
            "GG\n",
            "train_loss: 0.8713\n",
            "GG\n",
            "train_loss: 0.8648\n",
            "GG\n",
            "train_loss: 0.8783\n",
            "GG\n",
            "train_loss: 0.8729\n",
            "GG\n",
            "train_loss: 0.8734\n",
            "GG\n",
            "train_loss: 0.8760\n",
            "GG\n",
            "train_loss: 0.8853\n",
            "GG\n",
            "train_loss: 0.8757\n",
            "GG\n",
            "train_loss: 0.8878\n",
            "GG\n",
            "train_loss: 0.8815\n",
            "GG\n",
            "train_loss: 0.9300\n",
            "Saving model (epoch =   11  loss = 0.8867 )\n",
            "GG\n",
            "train_loss: 0.8651\n",
            "GG\n",
            "train_loss: 0.8624\n",
            "GG\n",
            "train_loss: 0.8543\n",
            "GG\n",
            "train_loss: 0.8671\n",
            "GG\n",
            "train_loss: 0.8702\n",
            "GG\n",
            "train_loss: 0.8790\n",
            "GG\n",
            "train_loss: 0.8663\n",
            "GG\n",
            "train_loss: 0.8812\n",
            "GG\n",
            "train_loss: 0.8887\n",
            "GG\n",
            "train_loss: 0.8800\n",
            "GG\n",
            "train_loss: 0.8843\n",
            "GG\n",
            "train_loss: 0.8829\n",
            "GG\n",
            "train_loss: 0.6683\n",
            "Saving model (epoch =   12  loss = 0.8863 )\n",
            "GG\n",
            "train_loss: 0.8589\n",
            "GG\n",
            "train_loss: 0.8556\n",
            "GG\n",
            "train_loss: 0.8667\n",
            "GG\n",
            "train_loss: 0.8651\n",
            "GG\n",
            "train_loss: 0.8732\n",
            "GG\n",
            "train_loss: 0.8708\n",
            "GG\n",
            "train_loss: 0.8857\n",
            "GG\n",
            "train_loss: 0.8761\n",
            "GG\n",
            "train_loss: 0.8768\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8871\n",
            "GG\n",
            "train_loss: 0.8868\n",
            "GG\n",
            "train_loss: 0.6996\n",
            "GG\n",
            "train_loss: 0.8553\n",
            "GG\n",
            "train_loss: 0.8643\n",
            "GG\n",
            "train_loss: 0.8544\n",
            "GG\n",
            "train_loss: 0.8719\n",
            "GG\n",
            "train_loss: 0.8715\n",
            "GG\n",
            "train_loss: 0.8816\n",
            "GG\n",
            "train_loss: 0.8757\n",
            "GG\n",
            "train_loss: 0.8798\n",
            "GG\n",
            "train_loss: 0.8741\n",
            "GG\n",
            "train_loss: 0.8785\n",
            "GG\n",
            "train_loss: 0.8875\n",
            "GG\n",
            "train_loss: 0.8838\n",
            "GG\n",
            "train_loss: 0.9083\n",
            "Saving model (epoch =   14  loss = 0.8859 )\n",
            "GG\n",
            "train_loss: 0.8624\n",
            "GG\n",
            "train_loss: 0.8640\n",
            "GG\n",
            "train_loss: 0.8630\n",
            "GG\n",
            "train_loss: 0.8703\n",
            "GG\n",
            "train_loss: 0.8576\n",
            "GG\n",
            "train_loss: 0.8846\n",
            "GG\n",
            "train_loss: 0.8729\n",
            "GG\n",
            "train_loss: 0.8737\n",
            "GG\n",
            "train_loss: 0.8887\n",
            "GG\n",
            "train_loss: 0.8758\n",
            "GG\n",
            "train_loss: 0.8821\n",
            "GG\n",
            "train_loss: 0.8886\n",
            "GG\n",
            "train_loss: 0.8085\n",
            "GG\n",
            "train_loss: 0.8652\n",
            "GG\n",
            "train_loss: 0.8594\n",
            "GG\n",
            "train_loss: 0.8619\n",
            "GG\n",
            "train_loss: 0.8622\n",
            "GG\n",
            "train_loss: 0.8746\n",
            "GG\n",
            "train_loss: 0.8707\n",
            "GG\n",
            "train_loss: 0.8686\n",
            "GG\n",
            "train_loss: 0.8776\n",
            "GG\n",
            "train_loss: 0.8717\n",
            "GG\n",
            "train_loss: 0.8866\n",
            "GG\n",
            "train_loss: 0.8856\n",
            "GG\n",
            "train_loss: 0.8856\n",
            "GG\n",
            "train_loss: 0.9744\n",
            "GG\n",
            "train_loss: 0.8633\n",
            "GG\n",
            "train_loss: 0.8657\n",
            "GG\n",
            "train_loss: 0.8619\n",
            "GG\n",
            "train_loss: 0.8655\n",
            "GG\n",
            "train_loss: 0.8756\n",
            "GG\n",
            "train_loss: 0.8771\n",
            "GG\n",
            "train_loss: 0.8735\n",
            "GG\n",
            "train_loss: 0.8627\n",
            "GG\n",
            "train_loss: 0.8808\n",
            "GG\n",
            "train_loss: 0.8882\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8754\n",
            "GG\n",
            "train_loss: 0.8925\n",
            "Saving model (epoch =   17  loss = 0.8857 )\n",
            "GG\n",
            "train_loss: 0.8649\n",
            "GG\n",
            "train_loss: 0.8656\n",
            "GG\n",
            "train_loss: 0.8626\n",
            "GG\n",
            "train_loss: 0.8698\n",
            "GG\n",
            "train_loss: 0.8749\n",
            "GG\n",
            "train_loss: 0.8728\n",
            "GG\n",
            "train_loss: 0.8742\n",
            "GG\n",
            "train_loss: 0.8779\n",
            "GG\n",
            "train_loss: 0.8706\n",
            "GG\n",
            "train_loss: 0.8751\n",
            "GG\n",
            "train_loss: 0.8828\n",
            "GG\n",
            "train_loss: 0.8755\n",
            "GG\n",
            "train_loss: 0.9743\n",
            "GG\n",
            "train_loss: 0.8619\n",
            "GG\n",
            "train_loss: 0.8540\n",
            "GG\n",
            "train_loss: 0.8679\n",
            "GG\n",
            "train_loss: 0.8612\n",
            "GG\n",
            "train_loss: 0.8658\n",
            "GG\n",
            "train_loss: 0.8813\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 0.8804\n",
            "GG\n",
            "train_loss: 0.8800\n",
            "GG\n",
            "train_loss: 0.8831\n",
            "GG\n",
            "train_loss: 0.8839\n",
            "GG\n",
            "train_loss: 0.9598\n",
            "Saving model (epoch =   19  loss = 0.8855 )\n",
            "GG\n",
            "train_loss: 0.8618\n",
            "GG\n",
            "train_loss: 0.8521\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8629\n",
            "GG\n",
            "train_loss: 0.8741\n",
            "GG\n",
            "train_loss: 0.8633\n",
            "GG\n",
            "train_loss: 0.8824\n",
            "GG\n",
            "train_loss: 0.8849\n",
            "GG\n",
            "train_loss: 0.8779\n",
            "GG\n",
            "train_loss: 0.8751\n",
            "GG\n",
            "train_loss: 0.8775\n",
            "GG\n",
            "train_loss: 0.8837\n",
            "GG\n",
            "train_loss: 0.7749\n",
            "GG\n",
            "train_loss: 0.8591\n",
            "GG\n",
            "train_loss: 0.8570\n",
            "GG\n",
            "train_loss: 0.8584\n",
            "GG\n",
            "train_loss: 0.8578\n",
            "GG\n",
            "train_loss: 0.8718\n",
            "GG\n",
            "train_loss: 0.8656\n",
            "GG\n",
            "train_loss: 0.8834\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 0.8781\n",
            "GG\n",
            "train_loss: 0.8822\n",
            "GG\n",
            "train_loss: 0.8776\n",
            "GG\n",
            "train_loss: 0.8845\n",
            "GG\n",
            "train_loss: 0.9379\n",
            "GG\n",
            "train_loss: 0.8573\n",
            "GG\n",
            "train_loss: 0.8598\n",
            "GG\n",
            "train_loss: 0.8687\n",
            "GG\n",
            "train_loss: 0.8702\n",
            "GG\n",
            "train_loss: 0.8686\n",
            "GG\n",
            "train_loss: 0.8590\n",
            "GG\n",
            "train_loss: 0.8679\n",
            "GG\n",
            "train_loss: 0.8801\n",
            "GG\n",
            "train_loss: 0.8764\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8816\n",
            "GG\n",
            "train_loss: 0.8794\n",
            "GG\n",
            "train_loss: 0.7620\n",
            "Saving model (epoch =   22  loss = 0.8851 )\n",
            "GG\n",
            "train_loss: 0.8588\n",
            "GG\n",
            "train_loss: 0.8571\n",
            "GG\n",
            "train_loss: 0.8549\n",
            "GG\n",
            "train_loss: 0.8755\n",
            "GG\n",
            "train_loss: 0.8718\n",
            "GG\n",
            "train_loss: 0.8690\n",
            "GG\n",
            "train_loss: 0.8749\n",
            "GG\n",
            "train_loss: 0.8677\n",
            "GG\n",
            "train_loss: 0.8773\n",
            "GG\n",
            "train_loss: 0.8865\n",
            "GG\n",
            "train_loss: 0.8784\n",
            "GG\n",
            "train_loss: 0.8756\n",
            "GG\n",
            "train_loss: 0.9433\n",
            "GG\n",
            "train_loss: 0.8572\n",
            "GG\n",
            "train_loss: 0.8591\n",
            "GG\n",
            "train_loss: 0.8659\n",
            "GG\n",
            "train_loss: 0.8641\n",
            "GG\n",
            "train_loss: 0.8621\n",
            "GG\n",
            "train_loss: 0.8680\n",
            "GG\n",
            "train_loss: 0.8720\n",
            "GG\n",
            "train_loss: 0.8857\n",
            "GG\n",
            "train_loss: 0.8781\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8808\n",
            "GG\n",
            "train_loss: 0.8753\n",
            "GG\n",
            "train_loss: 1.0177\n",
            "GG\n",
            "train_loss: 0.8717\n",
            "GG\n",
            "train_loss: 0.8589\n",
            "GG\n",
            "train_loss: 0.8617\n",
            "GG\n",
            "train_loss: 0.8658\n",
            "GG\n",
            "train_loss: 0.8676\n",
            "GG\n",
            "train_loss: 0.8638\n",
            "GG\n",
            "train_loss: 0.8722\n",
            "GG\n",
            "train_loss: 0.8743\n",
            "GG\n",
            "train_loss: 0.8677\n",
            "GG\n",
            "train_loss: 0.8886\n",
            "GG\n",
            "train_loss: 0.8745\n",
            "GG\n",
            "train_loss: 0.8826\n",
            "GG\n",
            "train_loss: 1.0731\n",
            "Saving model (epoch =   25  loss = 0.8850 )\n",
            "GG\n",
            "train_loss: 0.8614\n",
            "GG\n",
            "train_loss: 0.8635\n",
            "GG\n",
            "train_loss: 0.8610\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8629\n",
            "GG\n",
            "train_loss: 0.8734\n",
            "GG\n",
            "train_loss: 0.8689\n",
            "GG\n",
            "train_loss: 0.8703\n",
            "GG\n",
            "train_loss: 0.8781\n",
            "GG\n",
            "train_loss: 0.8741\n",
            "GG\n",
            "train_loss: 0.8771\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.7359\n",
            "Saving model (epoch =   26  loss = 0.8849 )\n",
            "GG\n",
            "train_loss: 0.8577\n",
            "GG\n",
            "train_loss: 0.8542\n",
            "GG\n",
            "train_loss: 0.8644\n",
            "GG\n",
            "train_loss: 0.8721\n",
            "GG\n",
            "train_loss: 0.8605\n",
            "GG\n",
            "train_loss: 0.8633\n",
            "GG\n",
            "train_loss: 0.8795\n",
            "GG\n",
            "train_loss: 0.8803\n",
            "GG\n",
            "train_loss: 0.8786\n",
            "GG\n",
            "train_loss: 0.8768\n",
            "GG\n",
            "train_loss: 0.8668\n",
            "GG\n",
            "train_loss: 0.8770\n",
            "GG\n",
            "train_loss: 0.8193\n",
            "GG\n",
            "train_loss: 0.8565\n",
            "GG\n",
            "train_loss: 0.8640\n",
            "GG\n",
            "train_loss: 0.8557\n",
            "GG\n",
            "train_loss: 0.8600\n",
            "GG\n",
            "train_loss: 0.8723\n",
            "GG\n",
            "train_loss: 0.8590\n",
            "GG\n",
            "train_loss: 0.8732\n",
            "GG\n",
            "train_loss: 0.8782\n",
            "GG\n",
            "train_loss: 0.8782\n",
            "GG\n",
            "train_loss: 0.8705\n",
            "GG\n",
            "train_loss: 0.8791\n",
            "GG\n",
            "train_loss: 0.8830\n",
            "GG\n",
            "train_loss: 0.9730\n",
            "Saving model (epoch =   28  loss = 0.8848 )\n",
            "GG\n",
            "train_loss: 0.8583\n",
            "GG\n",
            "train_loss: 0.8552\n",
            "GG\n",
            "train_loss: 0.8684\n",
            "GG\n",
            "train_loss: 0.8699\n",
            "GG\n",
            "train_loss: 0.8600\n",
            "GG\n",
            "train_loss: 0.8616\n",
            "GG\n",
            "train_loss: 0.8715\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 0.8776\n",
            "GG\n",
            "train_loss: 0.8759\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.8794\n",
            "GG\n",
            "train_loss: 0.8397\n",
            "GG\n",
            "train_loss: 0.8601\n",
            "GG\n",
            "train_loss: 0.8556\n",
            "GG\n",
            "train_loss: 0.8566\n",
            "GG\n",
            "train_loss: 0.8661\n",
            "GG\n",
            "train_loss: 0.8654\n",
            "GG\n",
            "train_loss: 0.8666\n",
            "GG\n",
            "train_loss: 0.8792\n",
            "GG\n",
            "train_loss: 0.8709\n",
            "GG\n",
            "train_loss: 0.8803\n",
            "GG\n",
            "train_loss: 0.8840\n",
            "GG\n",
            "train_loss: 0.8730\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8712\n",
            "Saving model (epoch =   30  loss = 0.8848 )\n",
            "GG\n",
            "train_loss: 0.8660\n",
            "GG\n",
            "train_loss: 0.8529\n",
            "GG\n",
            "train_loss: 0.8630\n",
            "GG\n",
            "train_loss: 0.8652\n",
            "GG\n",
            "train_loss: 0.8641\n",
            "GG\n",
            "train_loss: 0.8694\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8577\n",
            "GG\n",
            "train_loss: 0.8727\n",
            "GG\n",
            "train_loss: 0.8735\n",
            "GG\n",
            "train_loss: 0.8831\n",
            "GG\n",
            "train_loss: 0.8729\n",
            "GG\n",
            "train_loss: 0.9097\n",
            "Saving model (epoch =   31  loss = 0.8845 )\n",
            "GG\n",
            "train_loss: 0.8539\n",
            "GG\n",
            "train_loss: 0.8557\n",
            "GG\n",
            "train_loss: 0.8634\n",
            "GG\n",
            "train_loss: 0.8620\n",
            "GG\n",
            "train_loss: 0.8639\n",
            "GG\n",
            "train_loss: 0.8651\n",
            "GG\n",
            "train_loss: 0.8782\n",
            "GG\n",
            "train_loss: 0.8686\n",
            "GG\n",
            "train_loss: 0.8727\n",
            "GG\n",
            "train_loss: 0.8708\n",
            "GG\n",
            "train_loss: 0.8816\n",
            "GG\n",
            "train_loss: 0.8831\n",
            "GG\n",
            "train_loss: 0.8934\n",
            "GG\n",
            "train_loss: 0.8681\n",
            "GG\n",
            "train_loss: 0.8671\n",
            "GG\n",
            "train_loss: 0.8646\n",
            "GG\n",
            "train_loss: 0.8603\n",
            "GG\n",
            "train_loss: 0.8560\n",
            "GG\n",
            "train_loss: 0.8743\n",
            "GG\n",
            "train_loss: 0.8611\n",
            "GG\n",
            "train_loss: 0.8737\n",
            "GG\n",
            "train_loss: 0.8758\n",
            "GG\n",
            "train_loss: 0.8748\n",
            "GG\n",
            "train_loss: 0.8657\n",
            "GG\n",
            "train_loss: 0.8737\n",
            "GG\n",
            "train_loss: 0.8779\n",
            "GG\n",
            "train_loss: 0.8696\n",
            "GG\n",
            "train_loss: 0.8589\n",
            "GG\n",
            "train_loss: 0.8574\n",
            "GG\n",
            "train_loss: 0.8616\n",
            "GG\n",
            "train_loss: 0.8647\n",
            "GG\n",
            "train_loss: 0.8540\n",
            "GG\n",
            "train_loss: 0.8663\n",
            "GG\n",
            "train_loss: 0.8681\n",
            "GG\n",
            "train_loss: 0.8797\n",
            "GG\n",
            "train_loss: 0.8773\n",
            "GG\n",
            "train_loss: 0.8842\n",
            "GG\n",
            "train_loss: 0.8757\n",
            "GG\n",
            "train_loss: 0.8489\n",
            "GG\n",
            "train_loss: 0.8549\n",
            "GG\n",
            "train_loss: 0.8578\n",
            "GG\n",
            "train_loss: 0.8629\n",
            "GG\n",
            "train_loss: 0.8515\n",
            "GG\n",
            "train_loss: 0.8648\n",
            "GG\n",
            "train_loss: 0.8686\n",
            "GG\n",
            "train_loss: 0.8652\n",
            "GG\n",
            "train_loss: 0.8765\n",
            "GG\n",
            "train_loss: 0.8743\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8814\n",
            "GG\n",
            "train_loss: 0.8821\n",
            "GG\n",
            "train_loss: 0.7138\n",
            "Finish training after 35 epochs\n",
            "ending a era !!!\n",
            "GG\n",
            "train_loss: 14.0602\n",
            "GG\n",
            "train_loss: 13.7807\n",
            "GG\n",
            "train_loss: 13.4215\n",
            "GG\n",
            "train_loss: 12.9195\n",
            "GG\n",
            "train_loss: 12.2215\n",
            "GG\n",
            "train_loss: 11.3313\n",
            "GG\n",
            "train_loss: 10.1606\n",
            "GG\n",
            "train_loss: 8.9189\n",
            "GG\n",
            "train_loss: 7.4291\n",
            "GG\n",
            "train_loss: 5.8144\n",
            "GG\n",
            "train_loss: 4.1920\n",
            "GG\n",
            "train_loss: 2.6638\n",
            "GG\n",
            "train_loss: 1.4828\n",
            "Saving model (epoch =    1  loss = 0.9455 )\n",
            "GG\n",
            "train_loss: 0.9267\n",
            "GG\n",
            "train_loss: 1.0482\n",
            "GG\n",
            "train_loss: 1.7639\n",
            "GG\n",
            "train_loss: 2.5196\n",
            "GG\n",
            "train_loss: 2.7979\n",
            "GG\n",
            "train_loss: 2.5132\n",
            "GG\n",
            "train_loss: 1.9939\n",
            "GG\n",
            "train_loss: 1.4382\n",
            "GG\n",
            "train_loss: 1.0456\n",
            "GG\n",
            "train_loss: 0.8845\n",
            "GG\n",
            "train_loss: 0.9105\n",
            "GG\n",
            "train_loss: 1.0545\n",
            "GG\n",
            "train_loss: 1.3346\n",
            "GG\n",
            "train_loss: 1.4256\n",
            "GG\n",
            "train_loss: 1.5756\n",
            "GG\n",
            "train_loss: 1.6027\n",
            "GG\n",
            "train_loss: 1.5875\n",
            "GG\n",
            "train_loss: 1.4769\n",
            "GG\n",
            "train_loss: 1.3380\n",
            "GG\n",
            "train_loss: 1.1626\n",
            "GG\n",
            "train_loss: 1.0151\n",
            "GG\n",
            "train_loss: 0.9107\n",
            "GG\n",
            "train_loss: 0.8791\n",
            "GG\n",
            "train_loss: 0.9202\n",
            "GG\n",
            "train_loss: 0.9937\n",
            "GG\n",
            "train_loss: 1.0104\n",
            "GG\n",
            "train_loss: 1.1431\n",
            "GG\n",
            "train_loss: 1.1978\n",
            "GG\n",
            "train_loss: 1.1555\n",
            "GG\n",
            "train_loss: 1.0761\n",
            "GG\n",
            "train_loss: 0.9742\n",
            "GG\n",
            "train_loss: 0.9098\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 0.8753\n",
            "GG\n",
            "train_loss: 0.9093\n",
            "GG\n",
            "train_loss: 0.9432\n",
            "GG\n",
            "train_loss: 0.9788\n",
            "GG\n",
            "train_loss: 0.9932\n",
            "GG\n",
            "train_loss: 1.1719\n",
            "GG\n",
            "train_loss: 0.9691\n",
            "GG\n",
            "train_loss: 0.9500\n",
            "GG\n",
            "train_loss: 0.9220\n",
            "GG\n",
            "train_loss: 0.8975\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8652\n",
            "GG\n",
            "train_loss: 0.8683\n",
            "GG\n",
            "train_loss: 0.8984\n",
            "GG\n",
            "train_loss: 0.9166\n",
            "GG\n",
            "train_loss: 0.9172\n",
            "GG\n",
            "train_loss: 0.9117\n",
            "GG\n",
            "train_loss: 0.9023\n",
            "GG\n",
            "train_loss: 0.8326\n",
            "Saving model (epoch =    5  loss = 0.8947 )\n",
            "GG\n",
            "train_loss: 0.8754\n",
            "GG\n",
            "train_loss: 0.8697\n",
            "GG\n",
            "train_loss: 0.8570\n",
            "GG\n",
            "train_loss: 0.8626\n",
            "GG\n",
            "train_loss: 0.8584\n",
            "GG\n",
            "train_loss: 0.8696\n",
            "GG\n",
            "train_loss: 0.8771\n",
            "GG\n",
            "train_loss: 0.8889\n",
            "GG\n",
            "train_loss: 0.8752\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.8849\n",
            "GG\n",
            "train_loss: 0.8805\n",
            "GG\n",
            "train_loss: 0.8090\n",
            "Saving model (epoch =    6  loss = 0.8861 )\n",
            "GG\n",
            "train_loss: 0.8602\n",
            "GG\n",
            "train_loss: 0.8646\n",
            "GG\n",
            "train_loss: 0.8611\n",
            "GG\n",
            "train_loss: 0.8682\n",
            "GG\n",
            "train_loss: 0.8678\n",
            "GG\n",
            "train_loss: 0.8773\n",
            "GG\n",
            "train_loss: 0.8661\n",
            "GG\n",
            "train_loss: 0.8695\n",
            "GG\n",
            "train_loss: 0.8637\n",
            "GG\n",
            "train_loss: 0.8708\n",
            "GG\n",
            "train_loss: 0.8823\n",
            "GG\n",
            "train_loss: 0.8771\n",
            "GG\n",
            "train_loss: 0.8555\n",
            "GG\n",
            "train_loss: 0.8734\n",
            "GG\n",
            "train_loss: 0.8727\n",
            "GG\n",
            "train_loss: 0.8511\n",
            "GG\n",
            "train_loss: 0.8673\n",
            "GG\n",
            "train_loss: 0.8672\n",
            "GG\n",
            "train_loss: 0.8619\n",
            "GG\n",
            "train_loss: 0.8627\n",
            "GG\n",
            "train_loss: 0.8716\n",
            "GG\n",
            "train_loss: 0.8745\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8769\n",
            "GG\n",
            "train_loss: 0.8719\n",
            "GG\n",
            "train_loss: 0.9173\n",
            "Saving model (epoch =    8  loss = 0.8854 )\n",
            "GG\n",
            "train_loss: 0.8561\n",
            "GG\n",
            "train_loss: 0.8676\n",
            "GG\n",
            "train_loss: 0.8569\n",
            "GG\n",
            "train_loss: 0.8718\n",
            "GG\n",
            "train_loss: 0.8669\n",
            "GG\n",
            "train_loss: 0.8731\n",
            "GG\n",
            "train_loss: 0.8786\n",
            "GG\n",
            "train_loss: 0.8660\n",
            "GG\n",
            "train_loss: 0.8703\n",
            "GG\n",
            "train_loss: 0.8679\n",
            "GG\n",
            "train_loss: 0.8684\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 0.8604\n",
            "GG\n",
            "train_loss: 0.8620\n",
            "GG\n",
            "train_loss: 0.8629\n",
            "GG\n",
            "train_loss: 0.8663\n",
            "GG\n",
            "train_loss: 0.8715\n",
            "GG\n",
            "train_loss: 0.8698\n",
            "GG\n",
            "train_loss: 0.8647\n",
            "GG\n",
            "train_loss: 0.8724\n",
            "GG\n",
            "train_loss: 0.8612\n",
            "GG\n",
            "train_loss: 0.8622\n",
            "GG\n",
            "train_loss: 0.8784\n",
            "GG\n",
            "train_loss: 0.8730\n",
            "GG\n",
            "train_loss: 0.8810\n",
            "GG\n",
            "train_loss: 0.8571\n",
            "Saving model (epoch =   10  loss = 0.8851 )\n",
            "GG\n",
            "train_loss: 0.8622\n",
            "GG\n",
            "train_loss: 0.8585\n",
            "GG\n",
            "train_loss: 0.8658\n",
            "GG\n",
            "train_loss: 0.8525\n",
            "GG\n",
            "train_loss: 0.8682\n",
            "GG\n",
            "train_loss: 0.8628\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8672\n",
            "GG\n",
            "train_loss: 0.8806\n",
            "GG\n",
            "train_loss: 0.8854\n",
            "GG\n",
            "train_loss: 0.8773\n",
            "GG\n",
            "train_loss: 0.8655\n",
            "GG\n",
            "train_loss: 0.7825\n",
            "GG\n",
            "train_loss: 0.8500\n",
            "GG\n",
            "train_loss: 0.8627\n",
            "GG\n",
            "train_loss: 0.8617\n",
            "GG\n",
            "train_loss: 0.8591\n",
            "GG\n",
            "train_loss: 0.8736\n",
            "GG\n",
            "train_loss: 0.8696\n",
            "GG\n",
            "train_loss: 0.8726\n",
            "GG\n",
            "train_loss: 0.8690\n",
            "GG\n",
            "train_loss: 0.8759\n",
            "GG\n",
            "train_loss: 0.8798\n",
            "GG\n",
            "train_loss: 0.8747\n",
            "GG\n",
            "train_loss: 0.8700\n",
            "GG\n",
            "train_loss: 0.7880\n",
            "GG\n",
            "train_loss: 0.8573\n",
            "GG\n",
            "train_loss: 0.8530\n",
            "GG\n",
            "train_loss: 0.8631\n",
            "GG\n",
            "train_loss: 0.8653\n",
            "GG\n",
            "train_loss: 0.8666\n",
            "GG\n",
            "train_loss: 0.8633\n",
            "GG\n",
            "train_loss: 0.8637\n",
            "GG\n",
            "train_loss: 0.8719\n",
            "GG\n",
            "train_loss: 0.8717\n",
            "GG\n",
            "train_loss: 0.8790\n",
            "GG\n",
            "train_loss: 0.8800\n",
            "GG\n",
            "train_loss: 0.8837\n",
            "GG\n",
            "train_loss: 0.8977\n",
            "Saving model (epoch =   13  loss = 0.8850 )\n",
            "GG\n",
            "train_loss: 0.8506\n",
            "GG\n",
            "train_loss: 0.8673\n",
            "GG\n",
            "train_loss: 0.8648\n",
            "GG\n",
            "train_loss: 0.8636\n",
            "GG\n",
            "train_loss: 0.8618\n",
            "GG\n",
            "train_loss: 0.8601\n",
            "GG\n",
            "train_loss: 0.8670\n",
            "GG\n",
            "train_loss: 0.8787\n",
            "GG\n",
            "train_loss: 0.8772\n",
            "GG\n",
            "train_loss: 0.8766\n",
            "GG\n",
            "train_loss: 0.8805\n",
            "GG\n",
            "train_loss: 0.8747\n",
            "GG\n",
            "train_loss: 0.7524\n",
            "GG\n",
            "train_loss: 0.8617\n",
            "GG\n",
            "train_loss: 0.8588\n",
            "GG\n",
            "train_loss: 0.8658\n",
            "GG\n",
            "train_loss: 0.8637\n",
            "GG\n",
            "train_loss: 0.8703\n",
            "GG\n",
            "train_loss: 0.8735\n",
            "GG\n",
            "train_loss: 0.8704\n",
            "GG\n",
            "train_loss: 0.8669\n",
            "GG\n",
            "train_loss: 0.8721\n",
            "GG\n",
            "train_loss: 0.8632\n",
            "GG\n",
            "train_loss: 0.8838\n",
            "GG\n",
            "train_loss: 0.8748\n",
            "GG\n",
            "train_loss: 0.9032\n",
            "GG\n",
            "train_loss: 0.8604\n",
            "GG\n",
            "train_loss: 0.8510\n",
            "GG\n",
            "train_loss: 0.8531\n",
            "GG\n",
            "train_loss: 0.8642\n",
            "GG\n",
            "train_loss: 0.8694\n",
            "GG\n",
            "train_loss: 0.8743\n",
            "GG\n",
            "train_loss: 0.8640\n",
            "GG\n",
            "train_loss: 0.8654\n",
            "GG\n",
            "train_loss: 0.8695\n",
            "GG\n",
            "train_loss: 0.8804\n",
            "GG\n",
            "train_loss: 0.8850\n",
            "GG\n",
            "train_loss: 0.8775\n",
            "GG\n",
            "train_loss: 1.0923\n",
            "Saving model (epoch =   16  loss = 0.8849 )\n",
            "GG\n",
            "train_loss: 0.8656\n",
            "GG\n",
            "train_loss: 0.8625\n",
            "GG\n",
            "train_loss: 0.8676\n",
            "GG\n",
            "train_loss: 0.8541\n",
            "GG\n",
            "train_loss: 0.8615\n",
            "GG\n",
            "train_loss: 0.8651\n",
            "GG\n",
            "train_loss: 0.8710\n",
            "GG\n",
            "train_loss: 0.8642\n",
            "GG\n",
            "train_loss: 0.8792\n",
            "GG\n",
            "train_loss: 0.8744\n",
            "GG\n",
            "train_loss: 0.8725\n",
            "GG\n",
            "train_loss: 0.8774\n",
            "GG\n",
            "train_loss: 1.1323\n",
            "GG\n",
            "train_loss: 0.8583\n",
            "GG\n",
            "train_loss: 0.8673\n",
            "GG\n",
            "train_loss: 0.8583\n",
            "GG\n",
            "train_loss: 0.8684\n",
            "GG\n",
            "train_loss: 0.8610\n",
            "GG\n",
            "train_loss: 0.8651\n",
            "GG\n",
            "train_loss: 0.8698\n",
            "GG\n",
            "train_loss: 0.8764\n",
            "GG\n",
            "train_loss: 0.8660\n",
            "GG\n",
            "train_loss: 0.8685\n",
            "GG\n",
            "train_loss: 0.8780\n",
            "GG\n",
            "train_loss: 0.8783\n",
            "GG\n",
            "train_loss: 0.9883\n",
            "GG\n",
            "train_loss: 0.8617\n",
            "GG\n",
            "train_loss: 0.8629\n",
            "GG\n",
            "train_loss: 0.8566\n",
            "GG\n",
            "train_loss: 0.8669\n",
            "GG\n",
            "train_loss: 0.8644\n",
            "GG\n",
            "train_loss: 0.8654\n",
            "GG\n",
            "train_loss: 0.8674\n",
            "GG\n",
            "train_loss: 0.8691\n",
            "GG\n",
            "train_loss: 0.8762\n",
            "GG\n",
            "train_loss: 0.8704\n",
            "GG\n",
            "train_loss: 0.8721\n",
            "GG\n",
            "train_loss: 0.8835\n",
            "GG\n",
            "train_loss: 0.7180\n",
            "GG\n",
            "train_loss: 0.8551\n",
            "GG\n",
            "train_loss: 0.8655\n",
            "GG\n",
            "train_loss: 0.8624\n",
            "GG\n",
            "train_loss: 0.8662\n",
            "GG\n",
            "train_loss: 0.8636\n",
            "GG\n",
            "train_loss: 0.8789\n",
            "GG\n",
            "train_loss: 0.8755\n",
            "GG\n",
            "train_loss: 0.8663\n",
            "GG\n",
            "train_loss: 0.8694\n",
            "GG\n",
            "train_loss: 0.8738\n",
            "GG\n",
            "train_loss: 0.8750\n",
            "GG\n",
            "train_loss: 0.8659\n",
            "GG\n",
            "train_loss: 0.8538\n",
            "Finish training after 20 epochs\n",
            "ending a era !!!\n",
            "CPU times: user 23min 7s, sys: 12.5 s, total: 23min 19s\n",
            "Wall time: 12min 16s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "plot_learning_curve(model_loss_record, title='MF model')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "01787b1a-86eb-4560-81ad-62766a56b2b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "np.sqrt(model_loss)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9407012121104069"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "np.sqrt(model_loss_record['dev'][-1])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9407294127624053"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# del model\n",
        "# model = MF().to(device)\n",
        "# ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "# model.load_state_dict(ckpt)\n",
        "# # plot_pred(dv_set, model, device)  # Show prediction on the validation set\n",
        "# dev(dv_set, model, device)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "c2e65627-13cd-46f9-e51b-5973eb26d865"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ],
      "metadata": {
        "id": "aQikz3IPiyPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "# def save_pred(preds, file):\n",
        "#     ''' Save predictions to specified file '''\n",
        "#     print('Saving results to {}'.format(file))\n",
        "#     with open(file, 'w') as fp:\n",
        "#         writer = csv.writer(fp)\n",
        "#         writer.writerow(['id', 'tested_positive'])\n",
        "#         for i, p in enumerate(preds):\n",
        "#             writer.writerow([i, p])\n",
        "\n",
        "# preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "# save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "30dfbdf5-3b47-4bec-a993-e306f49f47a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "for X, y in dv_set:                         # iterate through the dataloader\n",
        "    X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "    with torch.no_grad():                   # disable gradient calculation\n",
        "        pred = model(X[:, 0], X[:, 1])                     # forward pass (compute output)\n",
        "        print(((pred -y) ** 2).mean())\n",
        "\n",
        "# total_loss = total_loss / len(dv_set.dataset)   \n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14.1575, device='cuda:0')\n",
            "tensor(14.3941, device='cuda:0')\n",
            "tensor(14.1919, device='cuda:0')\n",
            "tensor(14.5844, device='cuda:0')\n",
            "tensor(14.5960, device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# U = torch.nn.Parameter(torch.rand(2, 3, requires_grad=True))\n",
        "# P = torch.nn.Parameter(torch.rand(2, 6, requires_grad=False))\n",
        "# net = nn.Sequential(nn.Linear(6, 3), nn.ReLU())\n",
        "# torch.einsum('ij, ij -> i', [U[[0, 1]], net(P[[0, 1]])])\n",
        "# gg = torch.Tensor([0.3, 0.7])\n",
        "# nn.Sigmoid()(gg)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}