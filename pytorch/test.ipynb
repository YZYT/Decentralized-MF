{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b1d710d4a2dd0e836743a9708dcf2dd87750cb6db75a03dbc0a1931aaec4e6cb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils import data\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from utils.mylib import *\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "config = {\n",
        "    'sava_path': 'models/MF_1M.pth',\n",
        "    'batch_size': 500\n",
        "}\n",
        "init_Seed()\n",
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "\n",
        "data_train = '../data/ML1M/ML1M_copy1_train.txt'\n",
        "data_test = '../data/ML1M/ML1M_copy1_test.txt'\n",
        "\n",
        "tr_set = prep_dataloader(data_train, 'train', config['batch_size'])\n",
        "dv_set = prep_dataloader(data_test, 'dev', config['batch_size'])\n",
        "# tt_set = prep_dataloader(\"data/ML100K/ML100K_copy1_test.txt\", 'test', config['batch_size'], target_only=target_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n",
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the train set of MoviesLen Dataset (600126 samples found, each dim = 2)\n",
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the dev set of MoviesLen Dataset (200041 samples found, each dim = 2)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "model = MF().to(device)\n",
        "ckpt = torch.load(config['sava_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "# plot_pred(dv_set, model, device)  # Show prediction on the validation set\n",
        "print(dev(dv_set, model, device))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7528189614542324\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "config = {\n",
        "    'n_epochs': 2000,              # maximum number of epochs\n",
        "    'batch_size': 50,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.01,                # learning rate\n",
        "        # 'weight_decay': 0.001\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 3,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth',  # your model will be saved here\n",
        "    'D': 50\n",
        "}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class Envoy(nn.Module):\n",
        "    def __init__(self, n_factors=20):\n",
        "        super(Envoy, self).__init__()\n",
        "        \n",
        "        self.net = nn.Sequential(nn.Linear(n_factors, 1000 * n_factors), nn.ReLU(), nn.Linear(1000 * n_factors, n_factors))\n",
        "\n",
        "        self.init_net()\n",
        "        \n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # return torch.einsum('ij, ij -> i', [self.U[user], self.P[item] @ self.Q])\n",
        "        return self.net(X)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)\n",
        "    \n",
        "    def init_net(self):\n",
        "        def init_weights(m):\n",
        "            if type(m) == nn.Linear:\n",
        "                nn.init.normal_(m.weight, std=0.1)\n",
        "        self.net.apply(init_weights)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def prep_dataloader(X, Y, batch_size, n_jobs=0, mode='train'):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = data.TensorDataset(X, Y)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=(mode == 'train'), num_workers=n_jobs)                           # Construct dataloader"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "V = torch.load(\"./VTensor.pt\").data.to('cpu')\n",
        "V_tilde = torch.rand(4050, 20)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# tr_set = prep_dataloader(V_tilde[:500], V[:500], config['batch_size'], n_jobs=10, mode='train')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def train(V_tilde, V):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config['step_size'], gamma=config['gamma'])\n",
        "\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []} \n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    cnt = 0\n",
        "\n",
        "    while epoch < n_epochs:\n",
        "        model.train()\n",
        "\n",
        "        optimizer.zero_grad()    \n",
        "        X, Y = V_tilde.to(device), V.to(device)\n",
        "        Y_hat = model(X)\n",
        "\n",
        "        mse_loss = model.cal_loss(Y_hat, Y)\n",
        "        \n",
        "        mse_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        print(\"epoch: {:4d} train_loss: {:.4f}\".format(epoch, np.sqrt(mse_loss.detach().cpu().item())))\n",
        "\n",
        "        cnt += 1\n",
        "        if cnt == 50:\n",
        "            cnt = 0\n",
        "            V_tilde = model(X).detach()\n",
        "\n",
        "        # for X, Y in tr_set:\n",
        "        #     optimizer.zero_grad()    \n",
        "        #     X, Y = X.to(device), Y.to(device)\n",
        "        #     Y_hat = model(X)\n",
        "\n",
        "        #     mse_loss = model.cal_loss(Y_hat, Y)\n",
        "            \n",
        "        #     mse_loss.backward()\n",
        "            \n",
        "        #     optimizer.step()\n",
        "\n",
        "        #     loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        #     print(\"train_loss: {:.4f}\".format(np.sqrt(mse_loss.detach().cpu().item())))\n",
        "\n",
        "        # scheduler.step()\n",
        "        \n",
        "        epoch += 1\n",
        "\n",
        "        # dev_mse = dev(tr_set, model, device)\n",
        "\n",
        "        # print(\"epoch = {:4d} dev_loss: {:.4f}\".format(epoch, np.sqrt(dev_mse)))\n",
        "\n",
        "        # if dev_mse < min_mse:\n",
        "        #     min_mse = dev_mse\n",
        "        #     early_stop_cnt = 0\n",
        "        #     print(\"Saving model (epoch = {:4d}  loss = {:.4f} )\".format(epoch, np.sqrt(dev_mse)))\n",
        "        #     torch.save(model.state_dict(), config['save_path'])\n",
        "        # else:\n",
        "        #     early_stop_cnt += 1\n",
        "        \n",
        "        \n",
        "        # loss_record['dev'].append(dev_mse)\n",
        "\n",
        "        # if early_stop_cnt > config['early_stop']:\n",
        "        #     break\n",
        "\n",
        "    print(\"Finish training after {} epochs\".format(epoch))\n",
        "    return min_mse, loss_record\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for X, y in dv_set:                         # iterate through the dataloader\n",
        "        X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(X)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(y)    # accumulate loss\n",
        "        # total_loss += mse_loss.detach().cpu().item()     # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "model = Envoy().to(device)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "t = train(V_tilde, V)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:    0 train_loss: 0.9509\n",
            "epoch:    1 train_loss: 3.3020\n",
            "epoch:    2 train_loss: 1.1894\n",
            "epoch:    3 train_loss: 1.2126\n",
            "epoch:    4 train_loss: 1.6466\n",
            "epoch:    5 train_loss: 1.4960\n",
            "epoch:    6 train_loss: 1.1448\n",
            "epoch:    7 train_loss: 0.8274\n",
            "epoch:    8 train_loss: 0.6225\n",
            "epoch:    9 train_loss: 0.5318\n",
            "epoch:   10 train_loss: 0.5170\n",
            "epoch:   11 train_loss: 0.5323\n",
            "epoch:   12 train_loss: 0.5495\n",
            "epoch:   13 train_loss: 0.5575\n",
            "epoch:   14 train_loss: 0.5535\n",
            "epoch:   15 train_loss: 0.5387\n",
            "epoch:   16 train_loss: 0.5154\n",
            "epoch:   17 train_loss: 0.4863\n",
            "epoch:   18 train_loss: 0.4543\n",
            "epoch:   19 train_loss: 0.4222\n",
            "epoch:   20 train_loss: 0.3924\n",
            "epoch:   21 train_loss: 0.3664\n",
            "epoch:   22 train_loss: 0.3452\n",
            "epoch:   23 train_loss: 0.3295\n",
            "epoch:   24 train_loss: 0.3193\n",
            "epoch:   25 train_loss: 0.3138\n",
            "epoch:   26 train_loss: 0.3118\n",
            "epoch:   27 train_loss: 0.3122\n",
            "epoch:   28 train_loss: 0.3136\n",
            "epoch:   29 train_loss: 0.3152\n",
            "epoch:   30 train_loss: 0.3167\n",
            "epoch:   31 train_loss: 0.3177\n",
            "epoch:   32 train_loss: 0.3179\n",
            "epoch:   33 train_loss: 0.3176\n",
            "epoch:   34 train_loss: 0.3168\n",
            "epoch:   35 train_loss: 0.3157\n",
            "epoch:   36 train_loss: 0.3142\n",
            "epoch:   37 train_loss: 0.3123\n",
            "epoch:   38 train_loss: 0.3103\n",
            "epoch:   39 train_loss: 0.3082\n",
            "epoch:   40 train_loss: 0.3061\n",
            "epoch:   41 train_loss: 0.3039\n",
            "epoch:   42 train_loss: 0.3020\n",
            "epoch:   43 train_loss: 0.3003\n",
            "epoch:   44 train_loss: 0.2988\n",
            "epoch:   45 train_loss: 0.2977\n",
            "epoch:   46 train_loss: 0.2969\n",
            "epoch:   47 train_loss: 0.2965\n",
            "epoch:   48 train_loss: 0.2963\n",
            "epoch:   49 train_loss: 0.2961\n",
            "epoch:   50 train_loss: 2.0664\n",
            "epoch:   51 train_loss: 0.5960\n",
            "epoch:   52 train_loss: 1.6792\n",
            "epoch:   53 train_loss: 1.5489\n",
            "epoch:   54 train_loss: 0.8788\n",
            "epoch:   55 train_loss: 0.5505\n",
            "epoch:   56 train_loss: 0.9214\n",
            "epoch:   57 train_loss: 1.0947\n",
            "epoch:   58 train_loss: 0.9445\n",
            "epoch:   59 train_loss: 0.6278\n",
            "epoch:   60 train_loss: 0.3880\n",
            "epoch:   61 train_loss: 0.4648\n",
            "epoch:   62 train_loss: 0.6281\n",
            "epoch:   63 train_loss: 0.7001\n",
            "epoch:   64 train_loss: 0.6673\n",
            "epoch:   65 train_loss: 0.5660\n",
            "epoch:   66 train_loss: 0.4486\n",
            "epoch:   67 train_loss: 0.3664\n",
            "epoch:   68 train_loss: 0.3463\n",
            "epoch:   69 train_loss: 0.3705\n",
            "epoch:   70 train_loss: 0.4048\n",
            "epoch:   71 train_loss: 0.4288\n",
            "epoch:   72 train_loss: 0.4349\n",
            "epoch:   73 train_loss: 0.4228\n",
            "epoch:   74 train_loss: 0.3974\n",
            "epoch:   75 train_loss: 0.3663\n",
            "epoch:   76 train_loss: 0.3382\n",
            "epoch:   77 train_loss: 0.3191\n",
            "epoch:   78 train_loss: 0.3109\n",
            "epoch:   79 train_loss: 0.3110\n",
            "epoch:   80 train_loss: 0.3153\n",
            "epoch:   81 train_loss: 0.3202\n",
            "epoch:   82 train_loss: 0.3241\n",
            "epoch:   83 train_loss: 0.3262\n",
            "epoch:   84 train_loss: 0.3263\n",
            "epoch:   85 train_loss: 0.3243\n",
            "epoch:   86 train_loss: 0.3202\n",
            "epoch:   87 train_loss: 0.3144\n",
            "epoch:   88 train_loss: 0.3076\n",
            "epoch:   89 train_loss: 0.3008\n",
            "epoch:   90 train_loss: 0.2952\n",
            "epoch:   91 train_loss: 0.2916\n",
            "epoch:   92 train_loss: 0.2902\n",
            "epoch:   93 train_loss: 0.2906\n",
            "epoch:   94 train_loss: 0.2922\n",
            "epoch:   95 train_loss: 0.2942\n",
            "epoch:   96 train_loss: 0.2960\n",
            "epoch:   97 train_loss: 0.2970\n",
            "epoch:   98 train_loss: 0.2970\n",
            "epoch:   99 train_loss: 0.2962\n",
            "epoch:  100 train_loss: 0.2944\n",
            "epoch:  101 train_loss: 0.2920\n",
            "epoch:  102 train_loss: 0.2895\n",
            "epoch:  103 train_loss: 0.2876\n",
            "epoch:  104 train_loss: 0.2865\n",
            "epoch:  105 train_loss: 0.2861\n",
            "epoch:  106 train_loss: 0.2862\n",
            "epoch:  107 train_loss: 0.2864\n",
            "epoch:  108 train_loss: 0.2867\n",
            "epoch:  109 train_loss: 0.2871\n",
            "epoch:  110 train_loss: 0.2875\n",
            "epoch:  111 train_loss: 0.2877\n",
            "epoch:  112 train_loss: 0.2877\n",
            "epoch:  113 train_loss: 0.2875\n",
            "epoch:  114 train_loss: 0.2870\n",
            "epoch:  115 train_loss: 0.2865\n",
            "epoch:  116 train_loss: 0.2859\n",
            "epoch:  117 train_loss: 0.2856\n",
            "epoch:  118 train_loss: 0.2854\n",
            "epoch:  119 train_loss: 0.2853\n",
            "epoch:  120 train_loss: 0.2852\n",
            "epoch:  121 train_loss: 0.2853\n",
            "epoch:  122 train_loss: 0.2853\n",
            "epoch:  123 train_loss: 0.2854\n",
            "epoch:  124 train_loss: 0.2855\n",
            "epoch:  125 train_loss: 0.2856\n",
            "epoch:  126 train_loss: 0.2855\n",
            "epoch:  127 train_loss: 0.2854\n",
            "epoch:  128 train_loss: 0.2853\n",
            "epoch:  129 train_loss: 0.2851\n",
            "epoch:  130 train_loss: 0.2850\n",
            "epoch:  131 train_loss: 0.2849\n",
            "epoch:  132 train_loss: 0.2849\n",
            "epoch:  133 train_loss: 0.2849\n",
            "epoch:  134 train_loss: 0.2849\n",
            "epoch:  135 train_loss: 0.2849\n",
            "epoch:  136 train_loss: 0.2849\n",
            "epoch:  137 train_loss: 0.2849\n",
            "epoch:  138 train_loss: 0.2849\n",
            "epoch:  139 train_loss: 0.2849\n",
            "epoch:  140 train_loss: 0.2849\n",
            "epoch:  141 train_loss: 0.2848\n",
            "epoch:  142 train_loss: 0.2848\n",
            "epoch:  143 train_loss: 0.2847\n",
            "epoch:  144 train_loss: 0.2847\n",
            "epoch:  145 train_loss: 0.2847\n",
            "epoch:  146 train_loss: 0.2847\n",
            "epoch:  147 train_loss: 0.2846\n",
            "epoch:  148 train_loss: 0.2846\n",
            "epoch:  149 train_loss: 0.2846\n",
            "epoch:  150 train_loss: 0.2855\n",
            "epoch:  151 train_loss: 0.2851\n",
            "epoch:  152 train_loss: 0.2846\n",
            "epoch:  153 train_loss: 0.2844\n",
            "epoch:  154 train_loss: 0.2845\n",
            "epoch:  155 train_loss: 0.2848\n",
            "epoch:  156 train_loss: 0.2849\n",
            "epoch:  157 train_loss: 0.2848\n",
            "epoch:  158 train_loss: 0.2846\n",
            "epoch:  159 train_loss: 0.2845\n",
            "epoch:  160 train_loss: 0.2844\n",
            "epoch:  161 train_loss: 0.2845\n",
            "epoch:  162 train_loss: 0.2845\n",
            "epoch:  163 train_loss: 0.2845\n",
            "epoch:  164 train_loss: 0.2844\n",
            "epoch:  165 train_loss: 0.2844\n",
            "epoch:  166 train_loss: 0.2844\n",
            "epoch:  167 train_loss: 0.2844\n",
            "epoch:  168 train_loss: 0.2845\n",
            "epoch:  169 train_loss: 0.2844\n",
            "epoch:  170 train_loss: 0.2844\n",
            "epoch:  171 train_loss: 0.2844\n",
            "epoch:  172 train_loss: 0.2843\n",
            "epoch:  173 train_loss: 0.2843\n",
            "epoch:  174 train_loss: 0.2843\n",
            "epoch:  175 train_loss: 0.2843\n",
            "epoch:  176 train_loss: 0.2843\n",
            "epoch:  177 train_loss: 0.2843\n",
            "epoch:  178 train_loss: 0.2843\n",
            "epoch:  179 train_loss: 0.2843\n",
            "epoch:  180 train_loss: 0.2843\n",
            "epoch:  181 train_loss: 0.2843\n",
            "epoch:  182 train_loss: 0.2843\n",
            "epoch:  183 train_loss: 0.2843\n",
            "epoch:  184 train_loss: 0.2842\n",
            "epoch:  185 train_loss: 0.2842\n",
            "epoch:  186 train_loss: 0.2842\n",
            "epoch:  187 train_loss: 0.2842\n",
            "epoch:  188 train_loss: 0.2842\n",
            "epoch:  189 train_loss: 0.2842\n",
            "epoch:  190 train_loss: 0.2842\n",
            "epoch:  191 train_loss: 0.2842\n",
            "epoch:  192 train_loss: 0.2842\n",
            "epoch:  193 train_loss: 0.2842\n",
            "epoch:  194 train_loss: 0.2842\n",
            "epoch:  195 train_loss: 0.2842\n",
            "epoch:  196 train_loss: 0.2842\n",
            "epoch:  197 train_loss: 0.2842\n",
            "epoch:  198 train_loss: 0.2842\n",
            "epoch:  199 train_loss: 0.2842\n",
            "epoch:  200 train_loss: 0.2842\n",
            "epoch:  201 train_loss: 0.2842\n",
            "epoch:  202 train_loss: 0.2842\n",
            "epoch:  203 train_loss: 0.2842\n",
            "epoch:  204 train_loss: 0.2842\n",
            "epoch:  205 train_loss: 0.2842\n",
            "epoch:  206 train_loss: 0.2842\n",
            "epoch:  207 train_loss: 0.2842\n",
            "epoch:  208 train_loss: 0.2842\n",
            "epoch:  209 train_loss: 0.2842\n",
            "epoch:  210 train_loss: 0.2842\n",
            "epoch:  211 train_loss: 0.2842\n",
            "epoch:  212 train_loss: 0.2842\n",
            "epoch:  213 train_loss: 0.2842\n",
            "epoch:  214 train_loss: 0.2842\n",
            "epoch:  215 train_loss: 0.2842\n",
            "epoch:  216 train_loss: 0.2842\n",
            "epoch:  217 train_loss: 0.2842\n",
            "epoch:  218 train_loss: 0.2842\n",
            "epoch:  219 train_loss: 0.2842\n",
            "epoch:  220 train_loss: 0.2842\n",
            "epoch:  221 train_loss: 0.2842\n",
            "epoch:  222 train_loss: 0.2842\n",
            "epoch:  223 train_loss: 0.2842\n",
            "epoch:  224 train_loss: 0.2842\n",
            "epoch:  225 train_loss: 0.2842\n",
            "epoch:  226 train_loss: 0.2842\n",
            "epoch:  227 train_loss: 0.2842\n",
            "epoch:  228 train_loss: 0.2842\n",
            "epoch:  229 train_loss: 0.2842\n",
            "epoch:  230 train_loss: 0.2842\n",
            "epoch:  231 train_loss: 0.2842\n",
            "epoch:  232 train_loss: 0.2842\n",
            "epoch:  233 train_loss: 0.2841\n",
            "epoch:  234 train_loss: 0.2841\n",
            "epoch:  235 train_loss: 0.2841\n",
            "epoch:  236 train_loss: 0.2841\n",
            "epoch:  237 train_loss: 0.2841\n",
            "epoch:  238 train_loss: 0.2841\n",
            "epoch:  239 train_loss: 0.2841\n",
            "epoch:  240 train_loss: 0.2841\n",
            "epoch:  241 train_loss: 0.2841\n",
            "epoch:  242 train_loss: 0.2841\n",
            "epoch:  243 train_loss: 0.2841\n",
            "epoch:  244 train_loss: 0.2841\n",
            "epoch:  245 train_loss: 0.2841\n",
            "epoch:  246 train_loss: 0.2841\n",
            "epoch:  247 train_loss: 0.2841\n",
            "epoch:  248 train_loss: 0.2841\n",
            "epoch:  249 train_loss: 0.2841\n",
            "epoch:  250 train_loss: 0.2842\n",
            "epoch:  251 train_loss: 0.2842\n",
            "epoch:  252 train_loss: 0.2842\n",
            "epoch:  253 train_loss: 0.2842\n",
            "epoch:  254 train_loss: 0.2842\n",
            "epoch:  255 train_loss: 0.2842\n",
            "epoch:  256 train_loss: 0.2842\n",
            "epoch:  257 train_loss: 0.2842\n",
            "epoch:  258 train_loss: 0.2842\n",
            "epoch:  259 train_loss: 0.2842\n",
            "epoch:  260 train_loss: 0.2842\n",
            "epoch:  261 train_loss: 0.2842\n",
            "epoch:  262 train_loss: 0.2842\n",
            "epoch:  263 train_loss: 0.2842\n",
            "epoch:  264 train_loss: 0.2842\n",
            "epoch:  265 train_loss: 0.2842\n",
            "epoch:  266 train_loss: 0.2842\n",
            "epoch:  267 train_loss: 0.2842\n",
            "epoch:  268 train_loss: 0.2842\n",
            "epoch:  269 train_loss: 0.2842\n",
            "epoch:  270 train_loss: 0.2842\n",
            "epoch:  271 train_loss: 0.2842\n",
            "epoch:  272 train_loss: 0.2842\n",
            "epoch:  273 train_loss: 0.2842\n",
            "epoch:  274 train_loss: 0.2842\n",
            "epoch:  275 train_loss: 0.2842\n",
            "epoch:  276 train_loss: 0.2842\n",
            "epoch:  277 train_loss: 0.2842\n",
            "epoch:  278 train_loss: 0.2842\n",
            "epoch:  279 train_loss: 0.2842\n",
            "epoch:  280 train_loss: 0.2842\n",
            "epoch:  281 train_loss: 0.2842\n",
            "epoch:  282 train_loss: 0.2842\n",
            "epoch:  283 train_loss: 0.2842\n",
            "epoch:  284 train_loss: 0.2842\n",
            "epoch:  285 train_loss: 0.2842\n",
            "epoch:  286 train_loss: 0.2842\n",
            "epoch:  287 train_loss: 0.2842\n",
            "epoch:  288 train_loss: 0.2842\n",
            "epoch:  289 train_loss: 0.2842\n",
            "epoch:  290 train_loss: 0.2842\n",
            "epoch:  291 train_loss: 0.2842\n",
            "epoch:  292 train_loss: 0.2842\n",
            "epoch:  293 train_loss: 0.2842\n",
            "epoch:  294 train_loss: 0.2842\n",
            "epoch:  295 train_loss: 0.2842\n",
            "epoch:  296 train_loss: 0.2842\n",
            "epoch:  297 train_loss: 0.2842\n",
            "epoch:  298 train_loss: 0.2842\n",
            "epoch:  299 train_loss: 0.2842\n",
            "epoch:  300 train_loss: 0.2842\n",
            "epoch:  301 train_loss: 0.2842\n",
            "epoch:  302 train_loss: 0.2842\n",
            "epoch:  303 train_loss: 0.2842\n",
            "epoch:  304 train_loss: 0.2842\n",
            "epoch:  305 train_loss: 0.2842\n",
            "epoch:  306 train_loss: 0.2842\n",
            "epoch:  307 train_loss: 0.2842\n",
            "epoch:  308 train_loss: 0.2842\n",
            "epoch:  309 train_loss: 0.2842\n",
            "epoch:  310 train_loss: 0.2842\n",
            "epoch:  311 train_loss: 0.2842\n",
            "epoch:  312 train_loss: 0.2842\n",
            "epoch:  313 train_loss: 0.2842\n",
            "epoch:  314 train_loss: 0.2842\n",
            "epoch:  315 train_loss: 0.2842\n",
            "epoch:  316 train_loss: 0.2842\n",
            "epoch:  317 train_loss: 0.2842\n",
            "epoch:  318 train_loss: 0.2842\n",
            "epoch:  319 train_loss: 0.2842\n",
            "epoch:  320 train_loss: 0.2842\n",
            "epoch:  321 train_loss: 0.2842\n",
            "epoch:  322 train_loss: 0.2842\n",
            "epoch:  323 train_loss: 0.2842\n",
            "epoch:  324 train_loss: 0.2842\n",
            "epoch:  325 train_loss: 0.2842\n",
            "epoch:  326 train_loss: 0.2842\n",
            "epoch:  327 train_loss: 0.2842\n",
            "epoch:  328 train_loss: 0.2842\n",
            "epoch:  329 train_loss: 0.2842\n",
            "epoch:  330 train_loss: 0.2842\n",
            "epoch:  331 train_loss: 0.2842\n",
            "epoch:  332 train_loss: 0.2842\n",
            "epoch:  333 train_loss: 0.2842\n",
            "epoch:  334 train_loss: 0.2842\n",
            "epoch:  335 train_loss: 0.2842\n",
            "epoch:  336 train_loss: 0.2842\n",
            "epoch:  337 train_loss: 0.2842\n",
            "epoch:  338 train_loss: 0.2842\n",
            "epoch:  339 train_loss: 0.2842\n",
            "epoch:  340 train_loss: 0.2842\n",
            "epoch:  341 train_loss: 0.2842\n",
            "epoch:  342 train_loss: 0.2842\n",
            "epoch:  343 train_loss: 0.2842\n",
            "epoch:  344 train_loss: 0.2842\n",
            "epoch:  345 train_loss: 0.2842\n",
            "epoch:  346 train_loss: 0.2842\n",
            "epoch:  347 train_loss: 0.2842\n",
            "epoch:  348 train_loss: 0.2842\n",
            "epoch:  349 train_loss: 0.2842\n",
            "epoch:  350 train_loss: 0.2842\n",
            "epoch:  351 train_loss: 0.2842\n",
            "epoch:  352 train_loss: 0.2842\n",
            "epoch:  353 train_loss: 0.2842\n",
            "epoch:  354 train_loss: 0.2842\n",
            "epoch:  355 train_loss: 0.2842\n",
            "epoch:  356 train_loss: 0.2842\n",
            "epoch:  357 train_loss: 0.2842\n",
            "epoch:  358 train_loss: 0.2842\n",
            "epoch:  359 train_loss: 0.2842\n",
            "epoch:  360 train_loss: 0.2842\n",
            "epoch:  361 train_loss: 0.2842\n",
            "epoch:  362 train_loss: 0.2842\n",
            "epoch:  363 train_loss: 0.2842\n",
            "epoch:  364 train_loss: 0.2842\n",
            "epoch:  365 train_loss: 0.2842\n",
            "epoch:  366 train_loss: 0.2842\n",
            "epoch:  367 train_loss: 0.2842\n",
            "epoch:  368 train_loss: 0.2842\n",
            "epoch:  369 train_loss: 0.2842\n",
            "epoch:  370 train_loss: 0.2842\n",
            "epoch:  371 train_loss: 0.2842\n",
            "epoch:  372 train_loss: 0.2842\n",
            "epoch:  373 train_loss: 0.2842\n",
            "epoch:  374 train_loss: 0.2842\n",
            "epoch:  375 train_loss: 0.2842\n",
            "epoch:  376 train_loss: 0.2842\n",
            "epoch:  377 train_loss: 0.2842\n",
            "epoch:  378 train_loss: 0.2842\n",
            "epoch:  379 train_loss: 0.2842\n",
            "epoch:  380 train_loss: 0.2842\n",
            "epoch:  381 train_loss: 0.2842\n",
            "epoch:  382 train_loss: 0.2842\n",
            "epoch:  383 train_loss: 0.2842\n",
            "epoch:  384 train_loss: 0.2842\n",
            "epoch:  385 train_loss: 0.2842\n",
            "epoch:  386 train_loss: 0.2842\n",
            "epoch:  387 train_loss: 0.2842\n",
            "epoch:  388 train_loss: 0.2842\n",
            "epoch:  389 train_loss: 0.2842\n",
            "epoch:  390 train_loss: 0.2842\n",
            "epoch:  391 train_loss: 0.2842\n",
            "epoch:  392 train_loss: 0.2842\n",
            "epoch:  393 train_loss: 0.2842\n",
            "epoch:  394 train_loss: 0.2842\n",
            "epoch:  395 train_loss: 0.2842\n",
            "epoch:  396 train_loss: 0.2842\n",
            "epoch:  397 train_loss: 0.2842\n",
            "epoch:  398 train_loss: 0.2842\n",
            "epoch:  399 train_loss: 0.2842\n",
            "epoch:  400 train_loss: 0.2842\n",
            "epoch:  401 train_loss: 0.2842\n",
            "epoch:  402 train_loss: 0.2842\n",
            "epoch:  403 train_loss: 0.2842\n",
            "epoch:  404 train_loss: 0.2842\n",
            "epoch:  405 train_loss: 0.2842\n",
            "epoch:  406 train_loss: 0.2842\n",
            "epoch:  407 train_loss: 0.2842\n",
            "epoch:  408 train_loss: 0.2842\n",
            "epoch:  409 train_loss: 0.2842\n",
            "epoch:  410 train_loss: 0.2842\n",
            "epoch:  411 train_loss: 0.2842\n",
            "epoch:  412 train_loss: 0.2842\n",
            "epoch:  413 train_loss: 0.2842\n",
            "epoch:  414 train_loss: 0.2842\n",
            "epoch:  415 train_loss: 0.2842\n",
            "epoch:  416 train_loss: 0.2842\n",
            "epoch:  417 train_loss: 0.2842\n",
            "epoch:  418 train_loss: 0.2842\n",
            "epoch:  419 train_loss: 0.2842\n",
            "epoch:  420 train_loss: 0.2842\n",
            "epoch:  421 train_loss: 0.2842\n",
            "epoch:  422 train_loss: 0.2842\n",
            "epoch:  423 train_loss: 0.2842\n",
            "epoch:  424 train_loss: 0.2842\n",
            "epoch:  425 train_loss: 0.2842\n",
            "epoch:  426 train_loss: 0.2842\n",
            "epoch:  427 train_loss: 0.2842\n",
            "epoch:  428 train_loss: 0.2842\n",
            "epoch:  429 train_loss: 0.2842\n",
            "epoch:  430 train_loss: 0.2842\n",
            "epoch:  431 train_loss: 0.2842\n",
            "epoch:  432 train_loss: 0.2842\n",
            "epoch:  433 train_loss: 0.2842\n",
            "epoch:  434 train_loss: 0.2842\n",
            "epoch:  435 train_loss: 0.2842\n",
            "epoch:  436 train_loss: 0.2842\n",
            "epoch:  437 train_loss: 0.2842\n",
            "epoch:  438 train_loss: 0.2842\n",
            "epoch:  439 train_loss: 0.2842\n",
            "epoch:  440 train_loss: 0.2842\n",
            "epoch:  441 train_loss: 0.2842\n",
            "epoch:  442 train_loss: 0.2842\n",
            "epoch:  443 train_loss: 0.2842\n",
            "epoch:  444 train_loss: 0.2842\n",
            "epoch:  445 train_loss: 0.2842\n",
            "epoch:  446 train_loss: 0.2842\n",
            "epoch:  447 train_loss: 0.2842\n",
            "epoch:  448 train_loss: 0.2842\n",
            "epoch:  449 train_loss: 0.2842\n",
            "epoch:  450 train_loss: 0.2843\n",
            "epoch:  451 train_loss: 0.2843\n",
            "epoch:  452 train_loss: 0.2843\n",
            "epoch:  453 train_loss: 0.2843\n",
            "epoch:  454 train_loss: 0.2843\n",
            "epoch:  455 train_loss: 0.2843\n",
            "epoch:  456 train_loss: 0.2843\n",
            "epoch:  457 train_loss: 0.2843\n",
            "epoch:  458 train_loss: 0.2843\n",
            "epoch:  459 train_loss: 0.2843\n",
            "epoch:  460 train_loss: 0.2843\n",
            "epoch:  461 train_loss: 0.2843\n",
            "epoch:  462 train_loss: 0.2843\n",
            "epoch:  463 train_loss: 0.2843\n",
            "epoch:  464 train_loss: 0.2843\n",
            "epoch:  465 train_loss: 0.2843\n",
            "epoch:  466 train_loss: 0.2843\n",
            "epoch:  467 train_loss: 0.2843\n",
            "epoch:  468 train_loss: 0.2843\n",
            "epoch:  469 train_loss: 0.2843\n",
            "epoch:  470 train_loss: 0.2843\n",
            "epoch:  471 train_loss: 0.2843\n",
            "epoch:  472 train_loss: 0.2843\n",
            "epoch:  473 train_loss: 0.2843\n",
            "epoch:  474 train_loss: 0.2843\n",
            "epoch:  475 train_loss: 0.2843\n",
            "epoch:  476 train_loss: 0.2843\n",
            "epoch:  477 train_loss: 0.2843\n",
            "epoch:  478 train_loss: 0.2843\n",
            "epoch:  479 train_loss: 0.2843\n",
            "epoch:  480 train_loss: 0.2843\n",
            "epoch:  481 train_loss: 0.2843\n",
            "epoch:  482 train_loss: 0.2843\n",
            "epoch:  483 train_loss: 0.2843\n",
            "epoch:  484 train_loss: 0.2843\n",
            "epoch:  485 train_loss: 0.2843\n",
            "epoch:  486 train_loss: 0.2843\n",
            "epoch:  487 train_loss: 0.2843\n",
            "epoch:  488 train_loss: 0.2843\n",
            "epoch:  489 train_loss: 0.2843\n",
            "epoch:  490 train_loss: 0.2843\n",
            "epoch:  491 train_loss: 0.2843\n",
            "epoch:  492 train_loss: 0.2843\n",
            "epoch:  493 train_loss: 0.2843\n",
            "epoch:  494 train_loss: 0.2843\n",
            "epoch:  495 train_loss: 0.2843\n",
            "epoch:  496 train_loss: 0.2843\n",
            "epoch:  497 train_loss: 0.2843\n",
            "epoch:  498 train_loss: 0.2843\n",
            "epoch:  499 train_loss: 0.2843\n",
            "epoch:  500 train_loss: 0.2843\n",
            "epoch:  501 train_loss: 0.2843\n",
            "epoch:  502 train_loss: 0.2843\n",
            "epoch:  503 train_loss: 0.2843\n",
            "epoch:  504 train_loss: 0.2843\n",
            "epoch:  505 train_loss: 0.2843\n",
            "epoch:  506 train_loss: 0.2843\n",
            "epoch:  507 train_loss: 0.2843\n",
            "epoch:  508 train_loss: 0.2843\n",
            "epoch:  509 train_loss: 0.2843\n",
            "epoch:  510 train_loss: 0.2843\n",
            "epoch:  511 train_loss: 0.2843\n",
            "epoch:  512 train_loss: 0.2843\n",
            "epoch:  513 train_loss: 0.2843\n",
            "epoch:  514 train_loss: 0.2843\n",
            "epoch:  515 train_loss: 0.2843\n",
            "epoch:  516 train_loss: 0.2843\n",
            "epoch:  517 train_loss: 0.2843\n",
            "epoch:  518 train_loss: 0.2843\n",
            "epoch:  519 train_loss: 0.2843\n",
            "epoch:  520 train_loss: 0.2843\n",
            "epoch:  521 train_loss: 0.2843\n",
            "epoch:  522 train_loss: 0.2843\n",
            "epoch:  523 train_loss: 0.2843\n",
            "epoch:  524 train_loss: 0.2843\n",
            "epoch:  525 train_loss: 0.2843\n",
            "epoch:  526 train_loss: 0.2843\n",
            "epoch:  527 train_loss: 0.2843\n",
            "epoch:  528 train_loss: 0.2843\n",
            "epoch:  529 train_loss: 0.2843\n",
            "epoch:  530 train_loss: 0.2843\n",
            "epoch:  531 train_loss: 0.2843\n",
            "epoch:  532 train_loss: 0.2843\n",
            "epoch:  533 train_loss: 0.2843\n",
            "epoch:  534 train_loss: 0.2843\n",
            "epoch:  535 train_loss: 0.2843\n",
            "epoch:  536 train_loss: 0.2843\n",
            "epoch:  537 train_loss: 0.2843\n",
            "epoch:  538 train_loss: 0.2843\n",
            "epoch:  539 train_loss: 0.2843\n",
            "epoch:  540 train_loss: 0.2843\n",
            "epoch:  541 train_loss: 0.2843\n",
            "epoch:  542 train_loss: 0.2843\n",
            "epoch:  543 train_loss: 0.2843\n",
            "epoch:  544 train_loss: 0.2843\n",
            "epoch:  545 train_loss: 0.2843\n",
            "epoch:  546 train_loss: 0.2843\n",
            "epoch:  547 train_loss: 0.2843\n",
            "epoch:  548 train_loss: 0.2843\n",
            "epoch:  549 train_loss: 0.2843\n",
            "epoch:  550 train_loss: 0.2843\n",
            "epoch:  551 train_loss: 0.2843\n",
            "epoch:  552 train_loss: 0.2843\n",
            "epoch:  553 train_loss: 0.2843\n",
            "epoch:  554 train_loss: 0.2843\n",
            "epoch:  555 train_loss: 0.2843\n",
            "epoch:  556 train_loss: 0.2843\n",
            "epoch:  557 train_loss: 0.2843\n",
            "epoch:  558 train_loss: 0.2843\n",
            "epoch:  559 train_loss: 0.2843\n",
            "epoch:  560 train_loss: 0.2843\n",
            "epoch:  561 train_loss: 0.2843\n",
            "epoch:  562 train_loss: 0.2843\n",
            "epoch:  563 train_loss: 0.2843\n",
            "epoch:  564 train_loss: 0.2843\n",
            "epoch:  565 train_loss: 0.2843\n",
            "epoch:  566 train_loss: 0.2843\n",
            "epoch:  567 train_loss: 0.2843\n",
            "epoch:  568 train_loss: 0.2843\n",
            "epoch:  569 train_loss: 0.2843\n",
            "epoch:  570 train_loss: 0.2843\n",
            "epoch:  571 train_loss: 0.2843\n",
            "epoch:  572 train_loss: 0.2843\n",
            "epoch:  573 train_loss: 0.2843\n",
            "epoch:  574 train_loss: 0.2843\n",
            "epoch:  575 train_loss: 0.2843\n",
            "epoch:  576 train_loss: 0.2843\n",
            "epoch:  577 train_loss: 0.2843\n",
            "epoch:  578 train_loss: 0.2843\n",
            "epoch:  579 train_loss: 0.2843\n",
            "epoch:  580 train_loss: 0.2843\n",
            "epoch:  581 train_loss: 0.2843\n",
            "epoch:  582 train_loss: 0.2843\n",
            "epoch:  583 train_loss: 0.2843\n",
            "epoch:  584 train_loss: 0.2843\n",
            "epoch:  585 train_loss: 0.2843\n",
            "epoch:  586 train_loss: 0.2843\n",
            "epoch:  587 train_loss: 0.2843\n",
            "epoch:  588 train_loss: 0.2843\n",
            "epoch:  589 train_loss: 0.2843\n",
            "epoch:  590 train_loss: 0.2843\n",
            "epoch:  591 train_loss: 0.2843\n",
            "epoch:  592 train_loss: 0.2843\n",
            "epoch:  593 train_loss: 0.2843\n",
            "epoch:  594 train_loss: 0.2843\n",
            "epoch:  595 train_loss: 0.2843\n",
            "epoch:  596 train_loss: 0.2843\n",
            "epoch:  597 train_loss: 0.2843\n",
            "epoch:  598 train_loss: 0.2843\n",
            "epoch:  599 train_loss: 0.2843\n",
            "epoch:  600 train_loss: 0.2843\n",
            "epoch:  601 train_loss: 0.2843\n",
            "epoch:  602 train_loss: 0.2843\n",
            "epoch:  603 train_loss: 0.2843\n",
            "epoch:  604 train_loss: 0.2843\n",
            "epoch:  605 train_loss: 0.2843\n",
            "epoch:  606 train_loss: 0.2843\n",
            "epoch:  607 train_loss: 0.2843\n",
            "epoch:  608 train_loss: 0.2843\n",
            "epoch:  609 train_loss: 0.2843\n",
            "epoch:  610 train_loss: 0.2843\n",
            "epoch:  611 train_loss: 0.2843\n",
            "epoch:  612 train_loss: 0.2843\n",
            "epoch:  613 train_loss: 0.2843\n",
            "epoch:  614 train_loss: 0.2843\n",
            "epoch:  615 train_loss: 0.2843\n",
            "epoch:  616 train_loss: 0.2843\n",
            "epoch:  617 train_loss: 0.2843\n",
            "epoch:  618 train_loss: 0.2843\n",
            "epoch:  619 train_loss: 0.2843\n",
            "epoch:  620 train_loss: 0.2843\n",
            "epoch:  621 train_loss: 0.2843\n",
            "epoch:  622 train_loss: 0.2843\n",
            "epoch:  623 train_loss: 0.2843\n",
            "epoch:  624 train_loss: 0.2843\n",
            "epoch:  625 train_loss: 0.2843\n",
            "epoch:  626 train_loss: 0.2843\n",
            "epoch:  627 train_loss: 0.2843\n",
            "epoch:  628 train_loss: 0.2843\n",
            "epoch:  629 train_loss: 0.2843\n",
            "epoch:  630 train_loss: 0.2843\n",
            "epoch:  631 train_loss: 0.2843\n",
            "epoch:  632 train_loss: 0.2843\n",
            "epoch:  633 train_loss: 0.2843\n",
            "epoch:  634 train_loss: 0.2843\n",
            "epoch:  635 train_loss: 0.2843\n",
            "epoch:  636 train_loss: 0.2843\n",
            "epoch:  637 train_loss: 0.2843\n",
            "epoch:  638 train_loss: 0.2843\n",
            "epoch:  639 train_loss: 0.2843\n",
            "epoch:  640 train_loss: 0.2843\n",
            "epoch:  641 train_loss: 0.2843\n",
            "epoch:  642 train_loss: 0.2843\n",
            "epoch:  643 train_loss: 0.2843\n",
            "epoch:  644 train_loss: 0.2843\n",
            "epoch:  645 train_loss: 0.2843\n",
            "epoch:  646 train_loss: 0.2843\n",
            "epoch:  647 train_loss: 0.2843\n",
            "epoch:  648 train_loss: 0.2843\n",
            "epoch:  649 train_loss: 0.2843\n",
            "epoch:  650 train_loss: 0.2843\n",
            "epoch:  651 train_loss: 0.2843\n",
            "epoch:  652 train_loss: 0.2843\n",
            "epoch:  653 train_loss: 0.2843\n",
            "epoch:  654 train_loss: 0.2843\n",
            "epoch:  655 train_loss: 0.2843\n",
            "epoch:  656 train_loss: 0.2843\n",
            "epoch:  657 train_loss: 0.2843\n",
            "epoch:  658 train_loss: 0.2843\n",
            "epoch:  659 train_loss: 0.2843\n",
            "epoch:  660 train_loss: 0.2843\n",
            "epoch:  661 train_loss: 0.2843\n",
            "epoch:  662 train_loss: 0.2843\n",
            "epoch:  663 train_loss: 0.2843\n",
            "epoch:  664 train_loss: 0.2843\n",
            "epoch:  665 train_loss: 0.2843\n",
            "epoch:  666 train_loss: 0.2843\n",
            "epoch:  667 train_loss: 0.2843\n",
            "epoch:  668 train_loss: 0.2843\n",
            "epoch:  669 train_loss: 0.2843\n",
            "epoch:  670 train_loss: 0.2843\n",
            "epoch:  671 train_loss: 0.2843\n",
            "epoch:  672 train_loss: 0.2843\n",
            "epoch:  673 train_loss: 0.2843\n",
            "epoch:  674 train_loss: 0.2843\n",
            "epoch:  675 train_loss: 0.2843\n",
            "epoch:  676 train_loss: 0.2843\n",
            "epoch:  677 train_loss: 0.2843\n",
            "epoch:  678 train_loss: 0.2843\n",
            "epoch:  679 train_loss: 0.2843\n",
            "epoch:  680 train_loss: 0.2843\n",
            "epoch:  681 train_loss: 0.2843\n",
            "epoch:  682 train_loss: 0.2843\n",
            "epoch:  683 train_loss: 0.2843\n",
            "epoch:  684 train_loss: 0.2843\n",
            "epoch:  685 train_loss: 0.2843\n",
            "epoch:  686 train_loss: 0.2843\n",
            "epoch:  687 train_loss: 0.2843\n",
            "epoch:  688 train_loss: 0.2843\n",
            "epoch:  689 train_loss: 0.2843\n",
            "epoch:  690 train_loss: 0.2843\n",
            "epoch:  691 train_loss: 0.2843\n",
            "epoch:  692 train_loss: 0.2843\n",
            "epoch:  693 train_loss: 0.2843\n",
            "epoch:  694 train_loss: 0.2843\n",
            "epoch:  695 train_loss: 0.2843\n",
            "epoch:  696 train_loss: 0.2843\n",
            "epoch:  697 train_loss: 0.2843\n",
            "epoch:  698 train_loss: 0.2843\n",
            "epoch:  699 train_loss: 0.2843\n",
            "epoch:  700 train_loss: 0.2843\n",
            "epoch:  701 train_loss: 0.2843\n",
            "epoch:  702 train_loss: 0.2843\n",
            "epoch:  703 train_loss: 0.2843\n",
            "epoch:  704 train_loss: 0.2843\n",
            "epoch:  705 train_loss: 0.2843\n",
            "epoch:  706 train_loss: 0.2843\n",
            "epoch:  707 train_loss: 0.2843\n",
            "epoch:  708 train_loss: 0.2843\n",
            "epoch:  709 train_loss: 0.2843\n",
            "epoch:  710 train_loss: 0.2843\n",
            "epoch:  711 train_loss: 0.2843\n",
            "epoch:  712 train_loss: 0.2843\n",
            "epoch:  713 train_loss: 0.2843\n",
            "epoch:  714 train_loss: 0.2843\n",
            "epoch:  715 train_loss: 0.2843\n",
            "epoch:  716 train_loss: 0.2843\n",
            "epoch:  717 train_loss: 0.2843\n",
            "epoch:  718 train_loss: 0.2843\n",
            "epoch:  719 train_loss: 0.2843\n",
            "epoch:  720 train_loss: 0.2843\n",
            "epoch:  721 train_loss: 0.2843\n",
            "epoch:  722 train_loss: 0.2843\n",
            "epoch:  723 train_loss: 0.2843\n",
            "epoch:  724 train_loss: 0.2843\n",
            "epoch:  725 train_loss: 0.2843\n",
            "epoch:  726 train_loss: 0.2843\n",
            "epoch:  727 train_loss: 0.2843\n",
            "epoch:  728 train_loss: 0.2843\n",
            "epoch:  729 train_loss: 0.2843\n",
            "epoch:  730 train_loss: 0.2843\n",
            "epoch:  731 train_loss: 0.2843\n",
            "epoch:  732 train_loss: 0.2843\n",
            "epoch:  733 train_loss: 0.2843\n",
            "epoch:  734 train_loss: 0.2843\n",
            "epoch:  735 train_loss: 0.2843\n",
            "epoch:  736 train_loss: 0.2843\n",
            "epoch:  737 train_loss: 0.2843\n",
            "epoch:  738 train_loss: 0.2843\n",
            "epoch:  739 train_loss: 0.2843\n",
            "epoch:  740 train_loss: 0.2843\n",
            "epoch:  741 train_loss: 0.2843\n",
            "epoch:  742 train_loss: 0.2843\n",
            "epoch:  743 train_loss: 0.2843\n",
            "epoch:  744 train_loss: 0.2843\n",
            "epoch:  745 train_loss: 0.2843\n",
            "epoch:  746 train_loss: 0.2843\n",
            "epoch:  747 train_loss: 0.2843\n",
            "epoch:  748 train_loss: 0.2843\n",
            "epoch:  749 train_loss: 0.2843\n",
            "epoch:  750 train_loss: 0.2843\n",
            "epoch:  751 train_loss: 0.2843\n",
            "epoch:  752 train_loss: 0.2843\n",
            "epoch:  753 train_loss: 0.2843\n",
            "epoch:  754 train_loss: 0.2843\n",
            "epoch:  755 train_loss: 0.2843\n",
            "epoch:  756 train_loss: 0.2843\n",
            "epoch:  757 train_loss: 0.2843\n",
            "epoch:  758 train_loss: 0.2843\n",
            "epoch:  759 train_loss: 0.2843\n",
            "epoch:  760 train_loss: 0.2843\n",
            "epoch:  761 train_loss: 0.2843\n",
            "epoch:  762 train_loss: 0.2843\n",
            "epoch:  763 train_loss: 0.2843\n",
            "epoch:  764 train_loss: 0.2843\n",
            "epoch:  765 train_loss: 0.2843\n",
            "epoch:  766 train_loss: 0.2843\n",
            "epoch:  767 train_loss: 0.2843\n",
            "epoch:  768 train_loss: 0.2843\n",
            "epoch:  769 train_loss: 0.2843\n",
            "epoch:  770 train_loss: 0.2843\n",
            "epoch:  771 train_loss: 0.2843\n",
            "epoch:  772 train_loss: 0.2843\n",
            "epoch:  773 train_loss: 0.2843\n",
            "epoch:  774 train_loss: 0.2843\n",
            "epoch:  775 train_loss: 0.2843\n",
            "epoch:  776 train_loss: 0.2843\n",
            "epoch:  777 train_loss: 0.2843\n",
            "epoch:  778 train_loss: 0.2843\n",
            "epoch:  779 train_loss: 0.2843\n",
            "epoch:  780 train_loss: 0.2843\n",
            "epoch:  781 train_loss: 0.2843\n",
            "epoch:  782 train_loss: 0.2843\n",
            "epoch:  783 train_loss: 0.2843\n",
            "epoch:  784 train_loss: 0.2843\n",
            "epoch:  785 train_loss: 0.2843\n",
            "epoch:  786 train_loss: 0.2843\n",
            "epoch:  787 train_loss: 0.2843\n",
            "epoch:  788 train_loss: 0.2843\n",
            "epoch:  789 train_loss: 0.2843\n",
            "epoch:  790 train_loss: 0.2843\n",
            "epoch:  791 train_loss: 0.2843\n",
            "epoch:  792 train_loss: 0.2843\n",
            "epoch:  793 train_loss: 0.2843\n",
            "epoch:  794 train_loss: 0.2843\n",
            "epoch:  795 train_loss: 0.2843\n",
            "epoch:  796 train_loss: 0.2843\n",
            "epoch:  797 train_loss: 0.2843\n",
            "epoch:  798 train_loss: 0.2843\n",
            "epoch:  799 train_loss: 0.2843\n",
            "epoch:  800 train_loss: 0.2843\n",
            "epoch:  801 train_loss: 0.2843\n",
            "epoch:  802 train_loss: 0.2843\n",
            "epoch:  803 train_loss: 0.2843\n",
            "epoch:  804 train_loss: 0.2843\n",
            "epoch:  805 train_loss: 0.2843\n",
            "epoch:  806 train_loss: 0.2843\n",
            "epoch:  807 train_loss: 0.2843\n",
            "epoch:  808 train_loss: 0.2843\n",
            "epoch:  809 train_loss: 0.2843\n",
            "epoch:  810 train_loss: 0.2843\n",
            "epoch:  811 train_loss: 0.2843\n",
            "epoch:  812 train_loss: 0.2843\n",
            "epoch:  813 train_loss: 0.2843\n",
            "epoch:  814 train_loss: 0.2843\n",
            "epoch:  815 train_loss: 0.2843\n",
            "epoch:  816 train_loss: 0.2843\n",
            "epoch:  817 train_loss: 0.2843\n",
            "epoch:  818 train_loss: 0.2843\n",
            "epoch:  819 train_loss: 0.2843\n",
            "epoch:  820 train_loss: 0.2843\n",
            "epoch:  821 train_loss: 0.2843\n",
            "epoch:  822 train_loss: 0.2843\n",
            "epoch:  823 train_loss: 0.2843\n",
            "epoch:  824 train_loss: 0.2843\n",
            "epoch:  825 train_loss: 0.2843\n",
            "epoch:  826 train_loss: 0.2843\n",
            "epoch:  827 train_loss: 0.2843\n",
            "epoch:  828 train_loss: 0.2843\n",
            "epoch:  829 train_loss: 0.2843\n",
            "epoch:  830 train_loss: 0.2843\n",
            "epoch:  831 train_loss: 0.2843\n",
            "epoch:  832 train_loss: 0.2843\n",
            "epoch:  833 train_loss: 0.2843\n",
            "epoch:  834 train_loss: 0.2843\n",
            "epoch:  835 train_loss: 0.2843\n",
            "epoch:  836 train_loss: 0.2843\n",
            "epoch:  837 train_loss: 0.2843\n",
            "epoch:  838 train_loss: 0.2843\n",
            "epoch:  839 train_loss: 0.2843\n",
            "epoch:  840 train_loss: 0.2843\n",
            "epoch:  841 train_loss: 0.2843\n",
            "epoch:  842 train_loss: 0.2843\n",
            "epoch:  843 train_loss: 0.2843\n",
            "epoch:  844 train_loss: 0.2843\n",
            "epoch:  845 train_loss: 0.2843\n",
            "epoch:  846 train_loss: 0.2843\n",
            "epoch:  847 train_loss: 0.2843\n",
            "epoch:  848 train_loss: 0.2843\n",
            "epoch:  849 train_loss: 0.2843\n",
            "epoch:  850 train_loss: 0.2843\n",
            "epoch:  851 train_loss: 0.2843\n",
            "epoch:  852 train_loss: 0.2843\n",
            "epoch:  853 train_loss: 0.2843\n",
            "epoch:  854 train_loss: 0.2843\n",
            "epoch:  855 train_loss: 0.2843\n",
            "epoch:  856 train_loss: 0.2843\n",
            "epoch:  857 train_loss: 0.2843\n",
            "epoch:  858 train_loss: 0.2843\n",
            "epoch:  859 train_loss: 0.2843\n",
            "epoch:  860 train_loss: 0.2843\n",
            "epoch:  861 train_loss: 0.2843\n",
            "epoch:  862 train_loss: 0.2843\n",
            "epoch:  863 train_loss: 0.2843\n",
            "epoch:  864 train_loss: 0.2843\n",
            "epoch:  865 train_loss: 0.2843\n",
            "epoch:  866 train_loss: 0.2843\n",
            "epoch:  867 train_loss: 0.2843\n",
            "epoch:  868 train_loss: 0.2843\n",
            "epoch:  869 train_loss: 0.2843\n",
            "epoch:  870 train_loss: 0.2843\n",
            "epoch:  871 train_loss: 0.2843\n",
            "epoch:  872 train_loss: 0.2843\n",
            "epoch:  873 train_loss: 0.2843\n",
            "epoch:  874 train_loss: 0.2843\n",
            "epoch:  875 train_loss: 0.2843\n",
            "epoch:  876 train_loss: 0.2843\n",
            "epoch:  877 train_loss: 0.2843\n",
            "epoch:  878 train_loss: 0.2843\n",
            "epoch:  879 train_loss: 0.2843\n",
            "epoch:  880 train_loss: 0.2843\n",
            "epoch:  881 train_loss: 0.2843\n",
            "epoch:  882 train_loss: 0.2843\n",
            "epoch:  883 train_loss: 0.2843\n",
            "epoch:  884 train_loss: 0.2843\n",
            "epoch:  885 train_loss: 0.2843\n",
            "epoch:  886 train_loss: 0.2843\n",
            "epoch:  887 train_loss: 0.2843\n",
            "epoch:  888 train_loss: 0.2843\n",
            "epoch:  889 train_loss: 0.2843\n",
            "epoch:  890 train_loss: 0.2843\n",
            "epoch:  891 train_loss: 0.2843\n",
            "epoch:  892 train_loss: 0.2843\n",
            "epoch:  893 train_loss: 0.2843\n",
            "epoch:  894 train_loss: 0.2843\n",
            "epoch:  895 train_loss: 0.2843\n",
            "epoch:  896 train_loss: 0.2843\n",
            "epoch:  897 train_loss: 0.2843\n",
            "epoch:  898 train_loss: 0.2843\n",
            "epoch:  899 train_loss: 0.2843\n",
            "epoch:  900 train_loss: 0.2843\n",
            "epoch:  901 train_loss: 0.2843\n",
            "epoch:  902 train_loss: 0.2843\n",
            "epoch:  903 train_loss: 0.2843\n",
            "epoch:  904 train_loss: 0.2843\n",
            "epoch:  905 train_loss: 0.2843\n",
            "epoch:  906 train_loss: 0.2843\n",
            "epoch:  907 train_loss: 0.2843\n",
            "epoch:  908 train_loss: 0.2843\n",
            "epoch:  909 train_loss: 0.2843\n",
            "epoch:  910 train_loss: 0.2843\n",
            "epoch:  911 train_loss: 0.2843\n",
            "epoch:  912 train_loss: 0.2843\n",
            "epoch:  913 train_loss: 0.2843\n",
            "epoch:  914 train_loss: 0.2843\n",
            "epoch:  915 train_loss: 0.2843\n",
            "epoch:  916 train_loss: 0.2843\n",
            "epoch:  917 train_loss: 0.2843\n",
            "epoch:  918 train_loss: 0.2843\n",
            "epoch:  919 train_loss: 0.2843\n",
            "epoch:  920 train_loss: 0.2843\n",
            "epoch:  921 train_loss: 0.2843\n",
            "epoch:  922 train_loss: 0.2843\n",
            "epoch:  923 train_loss: 0.2843\n",
            "epoch:  924 train_loss: 0.2843\n",
            "epoch:  925 train_loss: 0.2843\n",
            "epoch:  926 train_loss: 0.2843\n",
            "epoch:  927 train_loss: 0.2843\n",
            "epoch:  928 train_loss: 0.2843\n",
            "epoch:  929 train_loss: 0.2843\n",
            "epoch:  930 train_loss: 0.2843\n",
            "epoch:  931 train_loss: 0.2843\n",
            "epoch:  932 train_loss: 0.2843\n",
            "epoch:  933 train_loss: 0.2843\n",
            "epoch:  934 train_loss: 0.2843\n",
            "epoch:  935 train_loss: 0.2843\n",
            "epoch:  936 train_loss: 0.2843\n",
            "epoch:  937 train_loss: 0.2843\n",
            "epoch:  938 train_loss: 0.2843\n",
            "epoch:  939 train_loss: 0.2843\n",
            "epoch:  940 train_loss: 0.2843\n",
            "epoch:  941 train_loss: 0.2843\n",
            "epoch:  942 train_loss: 0.2843\n",
            "epoch:  943 train_loss: 0.2843\n",
            "epoch:  944 train_loss: 0.2843\n",
            "epoch:  945 train_loss: 0.2843\n",
            "epoch:  946 train_loss: 0.2843\n",
            "epoch:  947 train_loss: 0.2843\n",
            "epoch:  948 train_loss: 0.2843\n",
            "epoch:  949 train_loss: 0.2843\n",
            "epoch:  950 train_loss: 0.2843\n",
            "epoch:  951 train_loss: 0.2843\n",
            "epoch:  952 train_loss: 0.2843\n",
            "epoch:  953 train_loss: 0.2843\n",
            "epoch:  954 train_loss: 0.2843\n",
            "epoch:  955 train_loss: 0.2843\n",
            "epoch:  956 train_loss: 0.2843\n",
            "epoch:  957 train_loss: 0.2843\n",
            "epoch:  958 train_loss: 0.2843\n",
            "epoch:  959 train_loss: 0.2843\n",
            "epoch:  960 train_loss: 0.2843\n",
            "epoch:  961 train_loss: 0.2843\n",
            "epoch:  962 train_loss: 0.2843\n",
            "epoch:  963 train_loss: 0.2843\n",
            "epoch:  964 train_loss: 0.2843\n",
            "epoch:  965 train_loss: 0.2843\n",
            "epoch:  966 train_loss: 0.2843\n",
            "epoch:  967 train_loss: 0.2843\n",
            "epoch:  968 train_loss: 0.2843\n",
            "epoch:  969 train_loss: 0.2843\n",
            "epoch:  970 train_loss: 0.2843\n",
            "epoch:  971 train_loss: 0.2843\n",
            "epoch:  972 train_loss: 0.2843\n",
            "epoch:  973 train_loss: 0.2843\n",
            "epoch:  974 train_loss: 0.2843\n",
            "epoch:  975 train_loss: 0.2843\n",
            "epoch:  976 train_loss: 0.2843\n",
            "epoch:  977 train_loss: 0.2843\n",
            "epoch:  978 train_loss: 0.2843\n",
            "epoch:  979 train_loss: 0.2843\n",
            "epoch:  980 train_loss: 0.2843\n",
            "epoch:  981 train_loss: 0.2843\n",
            "epoch:  982 train_loss: 0.2843\n",
            "epoch:  983 train_loss: 0.2843\n",
            "epoch:  984 train_loss: 0.2843\n",
            "epoch:  985 train_loss: 0.2843\n",
            "epoch:  986 train_loss: 0.2843\n",
            "epoch:  987 train_loss: 0.2843\n",
            "epoch:  988 train_loss: 0.2843\n",
            "epoch:  989 train_loss: 0.2843\n",
            "epoch:  990 train_loss: 0.2843\n",
            "epoch:  991 train_loss: 0.2843\n",
            "epoch:  992 train_loss: 0.2843\n",
            "epoch:  993 train_loss: 0.2843\n",
            "epoch:  994 train_loss: 0.2843\n",
            "epoch:  995 train_loss: 0.2843\n",
            "epoch:  996 train_loss: 0.2843\n",
            "epoch:  997 train_loss: 0.2843\n",
            "epoch:  998 train_loss: 0.2843\n",
            "epoch:  999 train_loss: 0.2843\n",
            "epoch: 1000 train_loss: 0.2843\n",
            "epoch: 1001 train_loss: 0.2843\n",
            "epoch: 1002 train_loss: 0.2843\n",
            "epoch: 1003 train_loss: 0.2843\n",
            "epoch: 1004 train_loss: 0.2843\n",
            "epoch: 1005 train_loss: 0.2843\n",
            "epoch: 1006 train_loss: 0.2843\n",
            "epoch: 1007 train_loss: 0.2843\n",
            "epoch: 1008 train_loss: 0.2843\n",
            "epoch: 1009 train_loss: 0.2843\n",
            "epoch: 1010 train_loss: 0.2843\n",
            "epoch: 1011 train_loss: 0.2843\n",
            "epoch: 1012 train_loss: 0.2843\n",
            "epoch: 1013 train_loss: 0.2843\n",
            "epoch: 1014 train_loss: 0.2843\n",
            "epoch: 1015 train_loss: 0.2843\n",
            "epoch: 1016 train_loss: 0.2843\n",
            "epoch: 1017 train_loss: 0.2843\n",
            "epoch: 1018 train_loss: 0.2843\n",
            "epoch: 1019 train_loss: 0.2843\n",
            "epoch: 1020 train_loss: 0.2843\n",
            "epoch: 1021 train_loss: 0.2843\n",
            "epoch: 1022 train_loss: 0.2843\n",
            "epoch: 1023 train_loss: 0.2843\n",
            "epoch: 1024 train_loss: 0.2843\n",
            "epoch: 1025 train_loss: 0.2843\n",
            "epoch: 1026 train_loss: 0.2843\n",
            "epoch: 1027 train_loss: 0.2843\n",
            "epoch: 1028 train_loss: 0.2843\n",
            "epoch: 1029 train_loss: 0.2843\n",
            "epoch: 1030 train_loss: 0.2843\n",
            "epoch: 1031 train_loss: 0.2843\n",
            "epoch: 1032 train_loss: 0.2843\n",
            "epoch: 1033 train_loss: 0.2843\n",
            "epoch: 1034 train_loss: 0.2843\n",
            "epoch: 1035 train_loss: 0.2843\n",
            "epoch: 1036 train_loss: 0.2843\n",
            "epoch: 1037 train_loss: 0.2843\n",
            "epoch: 1038 train_loss: 0.2843\n",
            "epoch: 1039 train_loss: 0.2843\n",
            "epoch: 1040 train_loss: 0.2843\n",
            "epoch: 1041 train_loss: 0.2843\n",
            "epoch: 1042 train_loss: 0.2843\n",
            "epoch: 1043 train_loss: 0.2843\n",
            "epoch: 1044 train_loss: 0.2843\n",
            "epoch: 1045 train_loss: 0.2843\n",
            "epoch: 1046 train_loss: 0.2843\n",
            "epoch: 1047 train_loss: 0.2843\n",
            "epoch: 1048 train_loss: 0.2843\n",
            "epoch: 1049 train_loss: 0.2843\n",
            "epoch: 1050 train_loss: 0.2843\n",
            "epoch: 1051 train_loss: 0.2843\n",
            "epoch: 1052 train_loss: 0.2843\n",
            "epoch: 1053 train_loss: 0.2843\n",
            "epoch: 1054 train_loss: 0.2843\n",
            "epoch: 1055 train_loss: 0.2843\n",
            "epoch: 1056 train_loss: 0.2843\n",
            "epoch: 1057 train_loss: 0.2843\n",
            "epoch: 1058 train_loss: 0.2843\n",
            "epoch: 1059 train_loss: 0.2843\n",
            "epoch: 1060 train_loss: 0.2843\n",
            "epoch: 1061 train_loss: 0.2843\n",
            "epoch: 1062 train_loss: 0.2843\n",
            "epoch: 1063 train_loss: 0.2843\n",
            "epoch: 1064 train_loss: 0.2843\n",
            "epoch: 1065 train_loss: 0.2843\n",
            "epoch: 1066 train_loss: 0.2843\n",
            "epoch: 1067 train_loss: 0.2843\n",
            "epoch: 1068 train_loss: 0.2843\n",
            "epoch: 1069 train_loss: 0.2843\n",
            "epoch: 1070 train_loss: 0.2843\n",
            "epoch: 1071 train_loss: 0.2843\n",
            "epoch: 1072 train_loss: 0.2843\n",
            "epoch: 1073 train_loss: 0.2843\n",
            "epoch: 1074 train_loss: 0.2843\n",
            "epoch: 1075 train_loss: 0.2843\n",
            "epoch: 1076 train_loss: 0.2843\n",
            "epoch: 1077 train_loss: 0.2843\n",
            "epoch: 1078 train_loss: 0.2843\n",
            "epoch: 1079 train_loss: 0.2843\n",
            "epoch: 1080 train_loss: 0.2843\n",
            "epoch: 1081 train_loss: 0.2843\n",
            "epoch: 1082 train_loss: 0.2843\n",
            "epoch: 1083 train_loss: 0.2843\n",
            "epoch: 1084 train_loss: 0.2843\n",
            "epoch: 1085 train_loss: 0.2843\n",
            "epoch: 1086 train_loss: 0.2843\n",
            "epoch: 1087 train_loss: 0.2843\n",
            "epoch: 1088 train_loss: 0.2843\n",
            "epoch: 1089 train_loss: 0.2843\n",
            "epoch: 1090 train_loss: 0.2843\n",
            "epoch: 1091 train_loss: 0.2843\n",
            "epoch: 1092 train_loss: 0.2843\n",
            "epoch: 1093 train_loss: 0.2843\n",
            "epoch: 1094 train_loss: 0.2843\n",
            "epoch: 1095 train_loss: 0.2843\n",
            "epoch: 1096 train_loss: 0.2843\n",
            "epoch: 1097 train_loss: 0.2843\n",
            "epoch: 1098 train_loss: 0.2843\n",
            "epoch: 1099 train_loss: 0.2843\n",
            "epoch: 1100 train_loss: 0.2843\n",
            "epoch: 1101 train_loss: 0.2843\n",
            "epoch: 1102 train_loss: 0.2843\n",
            "epoch: 1103 train_loss: 0.2843\n",
            "epoch: 1104 train_loss: 0.2843\n",
            "epoch: 1105 train_loss: 0.2843\n",
            "epoch: 1106 train_loss: 0.2843\n",
            "epoch: 1107 train_loss: 0.2843\n",
            "epoch: 1108 train_loss: 0.2843\n",
            "epoch: 1109 train_loss: 0.2843\n",
            "epoch: 1110 train_loss: 0.2843\n",
            "epoch: 1111 train_loss: 0.2843\n",
            "epoch: 1112 train_loss: 0.2843\n",
            "epoch: 1113 train_loss: 0.2843\n",
            "epoch: 1114 train_loss: 0.2843\n",
            "epoch: 1115 train_loss: 0.2843\n",
            "epoch: 1116 train_loss: 0.2843\n",
            "epoch: 1117 train_loss: 0.2843\n",
            "epoch: 1118 train_loss: 0.2843\n",
            "epoch: 1119 train_loss: 0.2843\n",
            "epoch: 1120 train_loss: 0.2843\n",
            "epoch: 1121 train_loss: 0.2843\n",
            "epoch: 1122 train_loss: 0.2843\n",
            "epoch: 1123 train_loss: 0.2843\n",
            "epoch: 1124 train_loss: 0.2843\n",
            "epoch: 1125 train_loss: 0.2843\n",
            "epoch: 1126 train_loss: 0.2843\n",
            "epoch: 1127 train_loss: 0.2843\n",
            "epoch: 1128 train_loss: 0.2843\n",
            "epoch: 1129 train_loss: 0.2843\n",
            "epoch: 1130 train_loss: 0.2843\n",
            "epoch: 1131 train_loss: 0.2843\n",
            "epoch: 1132 train_loss: 0.2843\n",
            "epoch: 1133 train_loss: 0.2843\n",
            "epoch: 1134 train_loss: 0.2843\n",
            "epoch: 1135 train_loss: 0.2843\n",
            "epoch: 1136 train_loss: 0.2843\n",
            "epoch: 1137 train_loss: 0.2843\n",
            "epoch: 1138 train_loss: 0.2843\n",
            "epoch: 1139 train_loss: 0.2843\n",
            "epoch: 1140 train_loss: 0.2843\n",
            "epoch: 1141 train_loss: 0.2843\n",
            "epoch: 1142 train_loss: 0.2843\n",
            "epoch: 1143 train_loss: 0.2843\n",
            "epoch: 1144 train_loss: 0.2843\n",
            "epoch: 1145 train_loss: 0.2843\n",
            "epoch: 1146 train_loss: 0.2843\n",
            "epoch: 1147 train_loss: 0.2843\n",
            "epoch: 1148 train_loss: 0.2843\n",
            "epoch: 1149 train_loss: 0.2843\n",
            "epoch: 1150 train_loss: 0.2843\n",
            "epoch: 1151 train_loss: 0.2843\n",
            "epoch: 1152 train_loss: 0.2843\n",
            "epoch: 1153 train_loss: 0.2843\n",
            "epoch: 1154 train_loss: 0.2843\n",
            "epoch: 1155 train_loss: 0.2843\n",
            "epoch: 1156 train_loss: 0.2843\n",
            "epoch: 1157 train_loss: 0.2843\n",
            "epoch: 1158 train_loss: 0.2843\n",
            "epoch: 1159 train_loss: 0.2843\n",
            "epoch: 1160 train_loss: 0.2843\n",
            "epoch: 1161 train_loss: 0.2843\n",
            "epoch: 1162 train_loss: 0.2843\n",
            "epoch: 1163 train_loss: 0.2843\n",
            "epoch: 1164 train_loss: 0.2843\n",
            "epoch: 1165 train_loss: 0.2843\n",
            "epoch: 1166 train_loss: 0.2843\n",
            "epoch: 1167 train_loss: 0.2843\n",
            "epoch: 1168 train_loss: 0.2843\n",
            "epoch: 1169 train_loss: 0.2843\n",
            "epoch: 1170 train_loss: 0.2843\n",
            "epoch: 1171 train_loss: 0.2843\n",
            "epoch: 1172 train_loss: 0.2843\n",
            "epoch: 1173 train_loss: 0.2843\n",
            "epoch: 1174 train_loss: 0.2843\n",
            "epoch: 1175 train_loss: 0.2843\n",
            "epoch: 1176 train_loss: 0.2843\n",
            "epoch: 1177 train_loss: 0.2843\n",
            "epoch: 1178 train_loss: 0.2843\n",
            "epoch: 1179 train_loss: 0.2843\n",
            "epoch: 1180 train_loss: 0.2843\n",
            "epoch: 1181 train_loss: 0.2843\n",
            "epoch: 1182 train_loss: 0.2843\n",
            "epoch: 1183 train_loss: 0.2843\n",
            "epoch: 1184 train_loss: 0.2843\n",
            "epoch: 1185 train_loss: 0.2843\n",
            "epoch: 1186 train_loss: 0.2843\n",
            "epoch: 1187 train_loss: 0.2843\n",
            "epoch: 1188 train_loss: 0.2843\n",
            "epoch: 1189 train_loss: 0.2843\n",
            "epoch: 1190 train_loss: 0.2843\n",
            "epoch: 1191 train_loss: 0.2843\n",
            "epoch: 1192 train_loss: 0.2843\n",
            "epoch: 1193 train_loss: 0.2843\n",
            "epoch: 1194 train_loss: 0.2843\n",
            "epoch: 1195 train_loss: 0.2843\n",
            "epoch: 1196 train_loss: 0.2843\n",
            "epoch: 1197 train_loss: 0.2843\n",
            "epoch: 1198 train_loss: 0.2843\n",
            "epoch: 1199 train_loss: 0.2843\n",
            "epoch: 1200 train_loss: 0.2843\n",
            "epoch: 1201 train_loss: 0.2843\n",
            "epoch: 1202 train_loss: 0.2843\n",
            "epoch: 1203 train_loss: 0.2843\n",
            "epoch: 1204 train_loss: 0.2843\n",
            "epoch: 1205 train_loss: 0.2843\n",
            "epoch: 1206 train_loss: 0.2843\n",
            "epoch: 1207 train_loss: 0.2843\n",
            "epoch: 1208 train_loss: 0.2843\n",
            "epoch: 1209 train_loss: 0.2843\n",
            "epoch: 1210 train_loss: 0.2843\n",
            "epoch: 1211 train_loss: 0.2843\n",
            "epoch: 1212 train_loss: 0.2843\n",
            "epoch: 1213 train_loss: 0.2843\n",
            "epoch: 1214 train_loss: 0.2843\n",
            "epoch: 1215 train_loss: 0.2843\n",
            "epoch: 1216 train_loss: 0.2843\n",
            "epoch: 1217 train_loss: 0.2843\n",
            "epoch: 1218 train_loss: 0.2843\n",
            "epoch: 1219 train_loss: 0.2843\n",
            "epoch: 1220 train_loss: 0.2843\n",
            "epoch: 1221 train_loss: 0.2843\n",
            "epoch: 1222 train_loss: 0.2843\n",
            "epoch: 1223 train_loss: 0.2843\n",
            "epoch: 1224 train_loss: 0.2843\n",
            "epoch: 1225 train_loss: 0.2843\n",
            "epoch: 1226 train_loss: 0.2843\n",
            "epoch: 1227 train_loss: 0.2843\n",
            "epoch: 1228 train_loss: 0.2843\n",
            "epoch: 1229 train_loss: 0.2843\n",
            "epoch: 1230 train_loss: 0.2843\n",
            "epoch: 1231 train_loss: 0.2843\n",
            "epoch: 1232 train_loss: 0.2843\n",
            "epoch: 1233 train_loss: 0.2843\n",
            "epoch: 1234 train_loss: 0.2843\n",
            "epoch: 1235 train_loss: 0.2843\n",
            "epoch: 1236 train_loss: 0.2843\n",
            "epoch: 1237 train_loss: 0.2843\n",
            "epoch: 1238 train_loss: 0.2843\n",
            "epoch: 1239 train_loss: 0.2843\n",
            "epoch: 1240 train_loss: 0.2843\n",
            "epoch: 1241 train_loss: 0.2843\n",
            "epoch: 1242 train_loss: 0.2843\n",
            "epoch: 1243 train_loss: 0.2843\n",
            "epoch: 1244 train_loss: 0.2843\n",
            "epoch: 1245 train_loss: 0.2843\n",
            "epoch: 1246 train_loss: 0.2843\n",
            "epoch: 1247 train_loss: 0.2843\n",
            "epoch: 1248 train_loss: 0.2843\n",
            "epoch: 1249 train_loss: 0.2843\n",
            "epoch: 1250 train_loss: 0.2843\n",
            "epoch: 1251 train_loss: 0.2843\n",
            "epoch: 1252 train_loss: 0.2843\n",
            "epoch: 1253 train_loss: 0.2843\n",
            "epoch: 1254 train_loss: 0.2843\n",
            "epoch: 1255 train_loss: 0.2843\n",
            "epoch: 1256 train_loss: 0.2843\n",
            "epoch: 1257 train_loss: 0.2843\n",
            "epoch: 1258 train_loss: 0.2843\n",
            "epoch: 1259 train_loss: 0.2843\n",
            "epoch: 1260 train_loss: 0.2843\n",
            "epoch: 1261 train_loss: 0.2843\n",
            "epoch: 1262 train_loss: 0.2843\n",
            "epoch: 1263 train_loss: 0.2843\n",
            "epoch: 1264 train_loss: 0.2843\n",
            "epoch: 1265 train_loss: 0.2843\n",
            "epoch: 1266 train_loss: 0.2843\n",
            "epoch: 1267 train_loss: 0.2843\n",
            "epoch: 1268 train_loss: 0.2843\n",
            "epoch: 1269 train_loss: 0.2843\n",
            "epoch: 1270 train_loss: 0.2843\n",
            "epoch: 1271 train_loss: 0.2843\n",
            "epoch: 1272 train_loss: 0.2843\n",
            "epoch: 1273 train_loss: 0.2843\n",
            "epoch: 1274 train_loss: 0.2843\n",
            "epoch: 1275 train_loss: 0.2843\n",
            "epoch: 1276 train_loss: 0.2843\n",
            "epoch: 1277 train_loss: 0.2843\n",
            "epoch: 1278 train_loss: 0.2843\n",
            "epoch: 1279 train_loss: 0.2843\n",
            "epoch: 1280 train_loss: 0.2843\n",
            "epoch: 1281 train_loss: 0.2843\n",
            "epoch: 1282 train_loss: 0.2843\n",
            "epoch: 1283 train_loss: 0.2843\n",
            "epoch: 1284 train_loss: 0.2843\n",
            "epoch: 1285 train_loss: 0.2843\n",
            "epoch: 1286 train_loss: 0.2843\n",
            "epoch: 1287 train_loss: 0.2843\n",
            "epoch: 1288 train_loss: 0.2843\n",
            "epoch: 1289 train_loss: 0.2843\n",
            "epoch: 1290 train_loss: 0.2843\n",
            "epoch: 1291 train_loss: 0.2843\n",
            "epoch: 1292 train_loss: 0.2843\n",
            "epoch: 1293 train_loss: 0.2843\n",
            "epoch: 1294 train_loss: 0.2843\n",
            "epoch: 1295 train_loss: 0.2843\n",
            "epoch: 1296 train_loss: 0.2843\n",
            "epoch: 1297 train_loss: 0.2843\n",
            "epoch: 1298 train_loss: 0.2843\n",
            "epoch: 1299 train_loss: 0.2843\n",
            "epoch: 1300 train_loss: 0.2843\n",
            "epoch: 1301 train_loss: 0.2843\n",
            "epoch: 1302 train_loss: 0.2843\n",
            "epoch: 1303 train_loss: 0.2843\n",
            "epoch: 1304 train_loss: 0.2843\n",
            "epoch: 1305 train_loss: 0.2843\n",
            "epoch: 1306 train_loss: 0.2843\n",
            "epoch: 1307 train_loss: 0.2843\n",
            "epoch: 1308 train_loss: 0.2843\n",
            "epoch: 1309 train_loss: 0.2843\n",
            "epoch: 1310 train_loss: 0.2843\n",
            "epoch: 1311 train_loss: 0.2843\n",
            "epoch: 1312 train_loss: 0.2843\n",
            "epoch: 1313 train_loss: 0.2843\n",
            "epoch: 1314 train_loss: 0.2843\n",
            "epoch: 1315 train_loss: 0.2843\n",
            "epoch: 1316 train_loss: 0.2843\n",
            "epoch: 1317 train_loss: 0.2843\n",
            "epoch: 1318 train_loss: 0.2843\n",
            "epoch: 1319 train_loss: 0.2843\n",
            "epoch: 1320 train_loss: 0.2843\n",
            "epoch: 1321 train_loss: 0.2843\n",
            "epoch: 1322 train_loss: 0.2843\n",
            "epoch: 1323 train_loss: 0.2843\n",
            "epoch: 1324 train_loss: 0.2843\n",
            "epoch: 1325 train_loss: 0.2843\n",
            "epoch: 1326 train_loss: 0.2843\n",
            "epoch: 1327 train_loss: 0.2843\n",
            "epoch: 1328 train_loss: 0.2843\n",
            "epoch: 1329 train_loss: 0.2843\n",
            "epoch: 1330 train_loss: 0.2843\n",
            "epoch: 1331 train_loss: 0.2843\n",
            "epoch: 1332 train_loss: 0.2843\n",
            "epoch: 1333 train_loss: 0.2843\n",
            "epoch: 1334 train_loss: 0.2843\n",
            "epoch: 1335 train_loss: 0.2843\n",
            "epoch: 1336 train_loss: 0.2843\n",
            "epoch: 1337 train_loss: 0.2843\n",
            "epoch: 1338 train_loss: 0.2843\n",
            "epoch: 1339 train_loss: 0.2843\n",
            "epoch: 1340 train_loss: 0.2843\n",
            "epoch: 1341 train_loss: 0.2843\n",
            "epoch: 1342 train_loss: 0.2843\n",
            "epoch: 1343 train_loss: 0.2843\n",
            "epoch: 1344 train_loss: 0.2843\n",
            "epoch: 1345 train_loss: 0.2843\n",
            "epoch: 1346 train_loss: 0.2843\n",
            "epoch: 1347 train_loss: 0.2843\n",
            "epoch: 1348 train_loss: 0.2843\n",
            "epoch: 1349 train_loss: 0.2843\n",
            "epoch: 1350 train_loss: 0.2843\n",
            "epoch: 1351 train_loss: 0.2843\n",
            "epoch: 1352 train_loss: 0.2843\n",
            "epoch: 1353 train_loss: 0.2843\n",
            "epoch: 1354 train_loss: 0.2843\n",
            "epoch: 1355 train_loss: 0.2843\n",
            "epoch: 1356 train_loss: 0.2843\n",
            "epoch: 1357 train_loss: 0.2843\n",
            "epoch: 1358 train_loss: 0.2843\n",
            "epoch: 1359 train_loss: 0.2843\n",
            "epoch: 1360 train_loss: 0.2843\n",
            "epoch: 1361 train_loss: 0.2843\n",
            "epoch: 1362 train_loss: 0.2843\n",
            "epoch: 1363 train_loss: 0.2843\n",
            "epoch: 1364 train_loss: 0.2843\n",
            "epoch: 1365 train_loss: 0.2843\n",
            "epoch: 1366 train_loss: 0.2843\n",
            "epoch: 1367 train_loss: 0.2843\n",
            "epoch: 1368 train_loss: 0.2843\n",
            "epoch: 1369 train_loss: 0.2843\n",
            "epoch: 1370 train_loss: 0.2843\n",
            "epoch: 1371 train_loss: 0.2843\n",
            "epoch: 1372 train_loss: 0.2843\n",
            "epoch: 1373 train_loss: 0.2843\n",
            "epoch: 1374 train_loss: 0.2843\n",
            "epoch: 1375 train_loss: 0.2843\n",
            "epoch: 1376 train_loss: 0.2843\n",
            "epoch: 1377 train_loss: 0.2843\n",
            "epoch: 1378 train_loss: 0.2843\n",
            "epoch: 1379 train_loss: 0.2843\n",
            "epoch: 1380 train_loss: 0.2843\n",
            "epoch: 1381 train_loss: 0.2843\n",
            "epoch: 1382 train_loss: 0.2843\n",
            "epoch: 1383 train_loss: 0.2843\n",
            "epoch: 1384 train_loss: 0.2843\n",
            "epoch: 1385 train_loss: 0.2843\n",
            "epoch: 1386 train_loss: 0.2843\n",
            "epoch: 1387 train_loss: 0.2843\n",
            "epoch: 1388 train_loss: 0.2843\n",
            "epoch: 1389 train_loss: 0.2843\n",
            "epoch: 1390 train_loss: 0.2843\n",
            "epoch: 1391 train_loss: 0.2843\n",
            "epoch: 1392 train_loss: 0.2843\n",
            "epoch: 1393 train_loss: 0.2843\n",
            "epoch: 1394 train_loss: 0.2843\n",
            "epoch: 1395 train_loss: 0.2843\n",
            "epoch: 1396 train_loss: 0.2843\n",
            "epoch: 1397 train_loss: 0.2843\n",
            "epoch: 1398 train_loss: 0.2843\n",
            "epoch: 1399 train_loss: 0.2843\n",
            "epoch: 1400 train_loss: 0.2843\n",
            "epoch: 1401 train_loss: 0.2843\n",
            "epoch: 1402 train_loss: 0.2843\n",
            "epoch: 1403 train_loss: 0.2843\n",
            "epoch: 1404 train_loss: 0.2843\n",
            "epoch: 1405 train_loss: 0.2843\n",
            "epoch: 1406 train_loss: 0.2843\n",
            "epoch: 1407 train_loss: 0.2843\n",
            "epoch: 1408 train_loss: 0.2843\n",
            "epoch: 1409 train_loss: 0.2843\n",
            "epoch: 1410 train_loss: 0.2843\n",
            "epoch: 1411 train_loss: 0.2843\n",
            "epoch: 1412 train_loss: 0.2843\n",
            "epoch: 1413 train_loss: 0.2843\n",
            "epoch: 1414 train_loss: 0.2843\n",
            "epoch: 1415 train_loss: 0.2843\n",
            "epoch: 1416 train_loss: 0.2843\n",
            "epoch: 1417 train_loss: 0.2843\n",
            "epoch: 1418 train_loss: 0.2843\n",
            "epoch: 1419 train_loss: 0.2843\n",
            "epoch: 1420 train_loss: 0.2843\n",
            "epoch: 1421 train_loss: 0.2843\n",
            "epoch: 1422 train_loss: 0.2843\n",
            "epoch: 1423 train_loss: 0.2843\n",
            "epoch: 1424 train_loss: 0.2843\n",
            "epoch: 1425 train_loss: 0.2843\n",
            "epoch: 1426 train_loss: 0.2843\n",
            "epoch: 1427 train_loss: 0.2843\n",
            "epoch: 1428 train_loss: 0.2843\n",
            "epoch: 1429 train_loss: 0.2843\n",
            "epoch: 1430 train_loss: 0.2843\n",
            "epoch: 1431 train_loss: 0.2843\n",
            "epoch: 1432 train_loss: 0.2843\n",
            "epoch: 1433 train_loss: 0.2843\n",
            "epoch: 1434 train_loss: 0.2843\n",
            "epoch: 1435 train_loss: 0.2843\n",
            "epoch: 1436 train_loss: 0.2843\n",
            "epoch: 1437 train_loss: 0.2843\n",
            "epoch: 1438 train_loss: 0.2843\n",
            "epoch: 1439 train_loss: 0.2843\n",
            "epoch: 1440 train_loss: 0.2843\n",
            "epoch: 1441 train_loss: 0.2843\n",
            "epoch: 1442 train_loss: 0.2843\n",
            "epoch: 1443 train_loss: 0.2843\n",
            "epoch: 1444 train_loss: 0.2843\n",
            "epoch: 1445 train_loss: 0.2843\n",
            "epoch: 1446 train_loss: 0.2843\n",
            "epoch: 1447 train_loss: 0.2843\n",
            "epoch: 1448 train_loss: 0.2843\n",
            "epoch: 1449 train_loss: 0.2843\n",
            "epoch: 1450 train_loss: 0.2843\n",
            "epoch: 1451 train_loss: 0.2843\n",
            "epoch: 1452 train_loss: 0.2843\n",
            "epoch: 1453 train_loss: 0.2843\n",
            "epoch: 1454 train_loss: 0.2843\n",
            "epoch: 1455 train_loss: 0.2843\n",
            "epoch: 1456 train_loss: 0.2843\n",
            "epoch: 1457 train_loss: 0.2843\n",
            "epoch: 1458 train_loss: 0.2843\n",
            "epoch: 1459 train_loss: 0.2843\n",
            "epoch: 1460 train_loss: 0.2843\n",
            "epoch: 1461 train_loss: 0.2843\n",
            "epoch: 1462 train_loss: 0.2843\n",
            "epoch: 1463 train_loss: 0.2843\n",
            "epoch: 1464 train_loss: 0.2843\n",
            "epoch: 1465 train_loss: 0.2843\n",
            "epoch: 1466 train_loss: 0.2843\n",
            "epoch: 1467 train_loss: 0.2843\n",
            "epoch: 1468 train_loss: 0.2843\n",
            "epoch: 1469 train_loss: 0.2843\n",
            "epoch: 1470 train_loss: 0.2843\n",
            "epoch: 1471 train_loss: 0.2843\n",
            "epoch: 1472 train_loss: 0.2843\n",
            "epoch: 1473 train_loss: 0.2843\n",
            "epoch: 1474 train_loss: 0.2843\n",
            "epoch: 1475 train_loss: 0.2843\n",
            "epoch: 1476 train_loss: 0.2843\n",
            "epoch: 1477 train_loss: 0.2843\n",
            "epoch: 1478 train_loss: 0.2843\n",
            "epoch: 1479 train_loss: 0.2843\n",
            "epoch: 1480 train_loss: 0.2843\n",
            "epoch: 1481 train_loss: 0.2843\n",
            "epoch: 1482 train_loss: 0.2843\n",
            "epoch: 1483 train_loss: 0.2843\n",
            "epoch: 1484 train_loss: 0.2843\n",
            "epoch: 1485 train_loss: 0.2843\n",
            "epoch: 1486 train_loss: 0.2843\n",
            "epoch: 1487 train_loss: 0.2843\n",
            "epoch: 1488 train_loss: 0.2843\n",
            "epoch: 1489 train_loss: 0.2843\n",
            "epoch: 1490 train_loss: 0.2843\n",
            "epoch: 1491 train_loss: 0.2843\n",
            "epoch: 1492 train_loss: 0.2843\n",
            "epoch: 1493 train_loss: 0.2843\n",
            "epoch: 1494 train_loss: 0.2843\n",
            "epoch: 1495 train_loss: 0.2843\n",
            "epoch: 1496 train_loss: 0.2843\n",
            "epoch: 1497 train_loss: 0.2843\n",
            "epoch: 1498 train_loss: 0.2843\n",
            "epoch: 1499 train_loss: 0.2843\n",
            "epoch: 1500 train_loss: 0.2843\n",
            "epoch: 1501 train_loss: 0.2843\n",
            "epoch: 1502 train_loss: 0.2843\n",
            "epoch: 1503 train_loss: 0.2843\n",
            "epoch: 1504 train_loss: 0.2843\n",
            "epoch: 1505 train_loss: 0.2843\n",
            "epoch: 1506 train_loss: 0.2843\n",
            "epoch: 1507 train_loss: 0.2843\n",
            "epoch: 1508 train_loss: 0.2843\n",
            "epoch: 1509 train_loss: 0.2843\n",
            "epoch: 1510 train_loss: 0.2843\n",
            "epoch: 1511 train_loss: 0.2843\n",
            "epoch: 1512 train_loss: 0.2843\n",
            "epoch: 1513 train_loss: 0.2843\n",
            "epoch: 1514 train_loss: 0.2843\n",
            "epoch: 1515 train_loss: 0.2843\n",
            "epoch: 1516 train_loss: 0.2843\n",
            "epoch: 1517 train_loss: 0.2843\n",
            "epoch: 1518 train_loss: 0.2843\n",
            "epoch: 1519 train_loss: 0.2843\n",
            "epoch: 1520 train_loss: 0.2843\n",
            "epoch: 1521 train_loss: 0.2843\n",
            "epoch: 1522 train_loss: 0.2843\n",
            "epoch: 1523 train_loss: 0.2843\n",
            "epoch: 1524 train_loss: 0.2843\n",
            "epoch: 1525 train_loss: 0.2843\n",
            "epoch: 1526 train_loss: 0.2843\n",
            "epoch: 1527 train_loss: 0.2843\n",
            "epoch: 1528 train_loss: 0.2843\n",
            "epoch: 1529 train_loss: 0.2843\n",
            "epoch: 1530 train_loss: 0.2843\n",
            "epoch: 1531 train_loss: 0.2843\n",
            "epoch: 1532 train_loss: 0.2843\n",
            "epoch: 1533 train_loss: 0.2843\n",
            "epoch: 1534 train_loss: 0.2843\n",
            "epoch: 1535 train_loss: 0.2843\n",
            "epoch: 1536 train_loss: 0.2843\n",
            "epoch: 1537 train_loss: 0.2843\n",
            "epoch: 1538 train_loss: 0.2843\n",
            "epoch: 1539 train_loss: 0.2843\n",
            "epoch: 1540 train_loss: 0.2843\n",
            "epoch: 1541 train_loss: 0.2843\n",
            "epoch: 1542 train_loss: 0.2843\n",
            "epoch: 1543 train_loss: 0.2843\n",
            "epoch: 1544 train_loss: 0.2843\n",
            "epoch: 1545 train_loss: 0.2843\n",
            "epoch: 1546 train_loss: 0.2843\n",
            "epoch: 1547 train_loss: 0.2843\n",
            "epoch: 1548 train_loss: 0.2843\n",
            "epoch: 1549 train_loss: 0.2843\n",
            "epoch: 1550 train_loss: 0.2843\n",
            "epoch: 1551 train_loss: 0.2843\n",
            "epoch: 1552 train_loss: 0.2843\n",
            "epoch: 1553 train_loss: 0.2843\n",
            "epoch: 1554 train_loss: 0.2843\n",
            "epoch: 1555 train_loss: 0.2843\n",
            "epoch: 1556 train_loss: 0.2843\n",
            "epoch: 1557 train_loss: 0.2843\n",
            "epoch: 1558 train_loss: 0.2843\n",
            "epoch: 1559 train_loss: 0.2843\n",
            "epoch: 1560 train_loss: 0.2843\n",
            "epoch: 1561 train_loss: 0.2843\n",
            "epoch: 1562 train_loss: 0.2843\n",
            "epoch: 1563 train_loss: 0.2843\n",
            "epoch: 1564 train_loss: 0.2843\n",
            "epoch: 1565 train_loss: 0.2843\n",
            "epoch: 1566 train_loss: 0.2843\n",
            "epoch: 1567 train_loss: 0.2843\n",
            "epoch: 1568 train_loss: 0.2843\n",
            "epoch: 1569 train_loss: 0.2843\n",
            "epoch: 1570 train_loss: 0.2843\n",
            "epoch: 1571 train_loss: 0.2843\n",
            "epoch: 1572 train_loss: 0.2843\n",
            "epoch: 1573 train_loss: 0.2843\n",
            "epoch: 1574 train_loss: 0.2843\n",
            "epoch: 1575 train_loss: 0.2843\n",
            "epoch: 1576 train_loss: 0.2843\n",
            "epoch: 1577 train_loss: 0.2843\n",
            "epoch: 1578 train_loss: 0.2843\n",
            "epoch: 1579 train_loss: 0.2843\n",
            "epoch: 1580 train_loss: 0.2843\n",
            "epoch: 1581 train_loss: 0.2843\n",
            "epoch: 1582 train_loss: 0.2843\n",
            "epoch: 1583 train_loss: 0.2843\n",
            "epoch: 1584 train_loss: 0.2843\n",
            "epoch: 1585 train_loss: 0.2843\n",
            "epoch: 1586 train_loss: 0.2843\n",
            "epoch: 1587 train_loss: 0.2843\n",
            "epoch: 1588 train_loss: 0.2843\n",
            "epoch: 1589 train_loss: 0.2843\n",
            "epoch: 1590 train_loss: 0.2843\n",
            "epoch: 1591 train_loss: 0.2843\n",
            "epoch: 1592 train_loss: 0.2843\n",
            "epoch: 1593 train_loss: 0.2843\n",
            "epoch: 1594 train_loss: 0.2843\n",
            "epoch: 1595 train_loss: 0.2843\n",
            "epoch: 1596 train_loss: 0.2843\n",
            "epoch: 1597 train_loss: 0.2843\n",
            "epoch: 1598 train_loss: 0.2843\n",
            "epoch: 1599 train_loss: 0.2843\n",
            "epoch: 1600 train_loss: 0.2843\n",
            "epoch: 1601 train_loss: 0.2843\n",
            "epoch: 1602 train_loss: 0.2843\n",
            "epoch: 1603 train_loss: 0.2843\n",
            "epoch: 1604 train_loss: 0.2843\n",
            "epoch: 1605 train_loss: 0.2843\n",
            "epoch: 1606 train_loss: 0.2843\n",
            "epoch: 1607 train_loss: 0.2843\n",
            "epoch: 1608 train_loss: 0.2843\n",
            "epoch: 1609 train_loss: 0.2843\n",
            "epoch: 1610 train_loss: 0.2843\n",
            "epoch: 1611 train_loss: 0.2843\n",
            "epoch: 1612 train_loss: 0.2843\n",
            "epoch: 1613 train_loss: 0.2843\n",
            "epoch: 1614 train_loss: 0.2843\n",
            "epoch: 1615 train_loss: 0.2843\n",
            "epoch: 1616 train_loss: 0.2843\n",
            "epoch: 1617 train_loss: 0.2843\n",
            "epoch: 1618 train_loss: 0.2843\n",
            "epoch: 1619 train_loss: 0.2843\n",
            "epoch: 1620 train_loss: 0.2843\n",
            "epoch: 1621 train_loss: 0.2843\n",
            "epoch: 1622 train_loss: 0.2843\n",
            "epoch: 1623 train_loss: 0.2843\n",
            "epoch: 1624 train_loss: 0.2843\n",
            "epoch: 1625 train_loss: 0.2843\n",
            "epoch: 1626 train_loss: 0.2843\n",
            "epoch: 1627 train_loss: 0.2843\n",
            "epoch: 1628 train_loss: 0.2843\n",
            "epoch: 1629 train_loss: 0.2843\n",
            "epoch: 1630 train_loss: 0.2843\n",
            "epoch: 1631 train_loss: 0.2843\n",
            "epoch: 1632 train_loss: 0.2843\n",
            "epoch: 1633 train_loss: 0.2843\n",
            "epoch: 1634 train_loss: 0.2843\n",
            "epoch: 1635 train_loss: 0.2843\n",
            "epoch: 1636 train_loss: 0.2843\n",
            "epoch: 1637 train_loss: 0.2843\n",
            "epoch: 1638 train_loss: 0.2843\n",
            "epoch: 1639 train_loss: 0.2843\n",
            "epoch: 1640 train_loss: 0.2843\n",
            "epoch: 1641 train_loss: 0.2843\n",
            "epoch: 1642 train_loss: 0.2843\n",
            "epoch: 1643 train_loss: 0.2843\n",
            "epoch: 1644 train_loss: 0.2843\n",
            "epoch: 1645 train_loss: 0.2843\n",
            "epoch: 1646 train_loss: 0.2843\n",
            "epoch: 1647 train_loss: 0.2843\n",
            "epoch: 1648 train_loss: 0.2843\n",
            "epoch: 1649 train_loss: 0.2843\n",
            "epoch: 1650 train_loss: 0.2843\n",
            "epoch: 1651 train_loss: 0.2843\n",
            "epoch: 1652 train_loss: 0.2843\n",
            "epoch: 1653 train_loss: 0.2843\n",
            "epoch: 1654 train_loss: 0.2843\n",
            "epoch: 1655 train_loss: 0.2843\n",
            "epoch: 1656 train_loss: 0.2843\n",
            "epoch: 1657 train_loss: 0.2843\n",
            "epoch: 1658 train_loss: 0.2843\n",
            "epoch: 1659 train_loss: 0.2843\n",
            "epoch: 1660 train_loss: 0.2843\n",
            "epoch: 1661 train_loss: 0.2843\n",
            "epoch: 1662 train_loss: 0.2843\n",
            "epoch: 1663 train_loss: 0.2843\n",
            "epoch: 1664 train_loss: 0.2843\n",
            "epoch: 1665 train_loss: 0.2843\n",
            "epoch: 1666 train_loss: 0.2843\n",
            "epoch: 1667 train_loss: 0.2843\n",
            "epoch: 1668 train_loss: 0.2843\n",
            "epoch: 1669 train_loss: 0.2843\n",
            "epoch: 1670 train_loss: 0.2843\n",
            "epoch: 1671 train_loss: 0.2843\n",
            "epoch: 1672 train_loss: 0.2843\n",
            "epoch: 1673 train_loss: 0.2843\n",
            "epoch: 1674 train_loss: 0.2843\n",
            "epoch: 1675 train_loss: 0.2843\n",
            "epoch: 1676 train_loss: 0.2843\n",
            "epoch: 1677 train_loss: 0.2843\n",
            "epoch: 1678 train_loss: 0.2843\n",
            "epoch: 1679 train_loss: 0.2843\n",
            "epoch: 1680 train_loss: 0.2843\n",
            "epoch: 1681 train_loss: 0.2843\n",
            "epoch: 1682 train_loss: 0.2843\n",
            "epoch: 1683 train_loss: 0.2843\n",
            "epoch: 1684 train_loss: 0.2843\n",
            "epoch: 1685 train_loss: 0.2843\n",
            "epoch: 1686 train_loss: 0.2843\n",
            "epoch: 1687 train_loss: 0.2843\n",
            "epoch: 1688 train_loss: 0.2843\n",
            "epoch: 1689 train_loss: 0.2843\n",
            "epoch: 1690 train_loss: 0.2843\n",
            "epoch: 1691 train_loss: 0.2843\n",
            "epoch: 1692 train_loss: 0.2843\n",
            "epoch: 1693 train_loss: 0.2843\n",
            "epoch: 1694 train_loss: 0.2843\n",
            "epoch: 1695 train_loss: 0.2843\n",
            "epoch: 1696 train_loss: 0.2843\n",
            "epoch: 1697 train_loss: 0.2843\n",
            "epoch: 1698 train_loss: 0.2843\n",
            "epoch: 1699 train_loss: 0.2843\n",
            "epoch: 1700 train_loss: 0.2843\n",
            "epoch: 1701 train_loss: 0.2843\n",
            "epoch: 1702 train_loss: 0.2843\n",
            "epoch: 1703 train_loss: 0.2843\n",
            "epoch: 1704 train_loss: 0.2843\n",
            "epoch: 1705 train_loss: 0.2843\n",
            "epoch: 1706 train_loss: 0.2843\n",
            "epoch: 1707 train_loss: 0.2843\n",
            "epoch: 1708 train_loss: 0.2843\n",
            "epoch: 1709 train_loss: 0.2843\n",
            "epoch: 1710 train_loss: 0.2843\n",
            "epoch: 1711 train_loss: 0.2843\n",
            "epoch: 1712 train_loss: 0.2843\n",
            "epoch: 1713 train_loss: 0.2843\n",
            "epoch: 1714 train_loss: 0.2843\n",
            "epoch: 1715 train_loss: 0.2843\n",
            "epoch: 1716 train_loss: 0.2843\n",
            "epoch: 1717 train_loss: 0.2843\n",
            "epoch: 1718 train_loss: 0.2843\n",
            "epoch: 1719 train_loss: 0.2843\n",
            "epoch: 1720 train_loss: 0.2843\n",
            "epoch: 1721 train_loss: 0.2843\n",
            "epoch: 1722 train_loss: 0.2843\n",
            "epoch: 1723 train_loss: 0.2843\n",
            "epoch: 1724 train_loss: 0.2843\n",
            "epoch: 1725 train_loss: 0.2843\n",
            "epoch: 1726 train_loss: 0.2843\n",
            "epoch: 1727 train_loss: 0.2843\n",
            "epoch: 1728 train_loss: 0.2843\n",
            "epoch: 1729 train_loss: 0.2843\n",
            "epoch: 1730 train_loss: 0.2843\n",
            "epoch: 1731 train_loss: 0.2843\n",
            "epoch: 1732 train_loss: 0.2843\n",
            "epoch: 1733 train_loss: 0.2843\n",
            "epoch: 1734 train_loss: 0.2843\n",
            "epoch: 1735 train_loss: 0.2843\n",
            "epoch: 1736 train_loss: 0.2843\n",
            "epoch: 1737 train_loss: 0.2843\n",
            "epoch: 1738 train_loss: 0.2843\n",
            "epoch: 1739 train_loss: 0.2843\n",
            "epoch: 1740 train_loss: 0.2843\n",
            "epoch: 1741 train_loss: 0.2843\n",
            "epoch: 1742 train_loss: 0.2843\n",
            "epoch: 1743 train_loss: 0.2843\n",
            "epoch: 1744 train_loss: 0.2843\n",
            "epoch: 1745 train_loss: 0.2843\n",
            "epoch: 1746 train_loss: 0.2843\n",
            "epoch: 1747 train_loss: 0.2843\n",
            "epoch: 1748 train_loss: 0.2843\n",
            "epoch: 1749 train_loss: 0.2843\n",
            "epoch: 1750 train_loss: 0.2843\n",
            "epoch: 1751 train_loss: 0.2843\n",
            "epoch: 1752 train_loss: 0.2843\n",
            "epoch: 1753 train_loss: 0.2843\n",
            "epoch: 1754 train_loss: 0.2843\n",
            "epoch: 1755 train_loss: 0.2843\n",
            "epoch: 1756 train_loss: 0.2843\n",
            "epoch: 1757 train_loss: 0.2843\n",
            "epoch: 1758 train_loss: 0.2843\n",
            "epoch: 1759 train_loss: 0.2843\n",
            "epoch: 1760 train_loss: 0.2843\n",
            "epoch: 1761 train_loss: 0.2843\n",
            "epoch: 1762 train_loss: 0.2843\n",
            "epoch: 1763 train_loss: 0.2843\n",
            "epoch: 1764 train_loss: 0.2843\n",
            "epoch: 1765 train_loss: 0.2843\n",
            "epoch: 1766 train_loss: 0.2843\n",
            "epoch: 1767 train_loss: 0.2843\n",
            "epoch: 1768 train_loss: 0.2843\n",
            "epoch: 1769 train_loss: 0.2843\n",
            "epoch: 1770 train_loss: 0.2843\n",
            "epoch: 1771 train_loss: 0.2843\n",
            "epoch: 1772 train_loss: 0.2843\n",
            "epoch: 1773 train_loss: 0.2843\n",
            "epoch: 1774 train_loss: 0.2843\n",
            "epoch: 1775 train_loss: 0.2843\n",
            "epoch: 1776 train_loss: 0.2843\n",
            "epoch: 1777 train_loss: 0.2843\n",
            "epoch: 1778 train_loss: 0.2843\n",
            "epoch: 1779 train_loss: 0.2843\n",
            "epoch: 1780 train_loss: 0.2843\n",
            "epoch: 1781 train_loss: 0.2843\n",
            "epoch: 1782 train_loss: 0.2843\n",
            "epoch: 1783 train_loss: 0.2843\n",
            "epoch: 1784 train_loss: 0.2843\n",
            "epoch: 1785 train_loss: 0.2843\n",
            "epoch: 1786 train_loss: 0.2843\n",
            "epoch: 1787 train_loss: 0.2843\n",
            "epoch: 1788 train_loss: 0.2843\n",
            "epoch: 1789 train_loss: 0.2843\n",
            "epoch: 1790 train_loss: 0.2843\n",
            "epoch: 1791 train_loss: 0.2843\n",
            "epoch: 1792 train_loss: 0.2843\n",
            "epoch: 1793 train_loss: 0.2843\n",
            "epoch: 1794 train_loss: 0.2843\n",
            "epoch: 1795 train_loss: 0.2843\n",
            "epoch: 1796 train_loss: 0.2843\n",
            "epoch: 1797 train_loss: 0.2843\n",
            "epoch: 1798 train_loss: 0.2843\n",
            "epoch: 1799 train_loss: 0.2843\n",
            "epoch: 1800 train_loss: 0.2843\n",
            "epoch: 1801 train_loss: 0.2843\n",
            "epoch: 1802 train_loss: 0.2843\n",
            "epoch: 1803 train_loss: 0.2843\n",
            "epoch: 1804 train_loss: 0.2843\n",
            "epoch: 1805 train_loss: 0.2843\n",
            "epoch: 1806 train_loss: 0.2843\n",
            "epoch: 1807 train_loss: 0.2843\n",
            "epoch: 1808 train_loss: 0.2843\n",
            "epoch: 1809 train_loss: 0.2843\n",
            "epoch: 1810 train_loss: 0.2843\n",
            "epoch: 1811 train_loss: 0.2843\n",
            "epoch: 1812 train_loss: 0.2843\n",
            "epoch: 1813 train_loss: 0.2843\n",
            "epoch: 1814 train_loss: 0.2843\n",
            "epoch: 1815 train_loss: 0.2843\n",
            "epoch: 1816 train_loss: 0.2843\n",
            "epoch: 1817 train_loss: 0.2843\n",
            "epoch: 1818 train_loss: 0.2843\n",
            "epoch: 1819 train_loss: 0.2843\n",
            "epoch: 1820 train_loss: 0.2843\n",
            "epoch: 1821 train_loss: 0.2843\n",
            "epoch: 1822 train_loss: 0.2843\n",
            "epoch: 1823 train_loss: 0.2843\n",
            "epoch: 1824 train_loss: 0.2843\n",
            "epoch: 1825 train_loss: 0.2843\n",
            "epoch: 1826 train_loss: 0.2843\n",
            "epoch: 1827 train_loss: 0.2843\n",
            "epoch: 1828 train_loss: 0.2843\n",
            "epoch: 1829 train_loss: 0.2843\n",
            "epoch: 1830 train_loss: 0.2843\n",
            "epoch: 1831 train_loss: 0.2843\n",
            "epoch: 1832 train_loss: 0.2843\n",
            "epoch: 1833 train_loss: 0.2843\n",
            "epoch: 1834 train_loss: 0.2843\n",
            "epoch: 1835 train_loss: 0.2843\n",
            "epoch: 1836 train_loss: 0.2843\n",
            "epoch: 1837 train_loss: 0.2843\n",
            "epoch: 1838 train_loss: 0.2843\n",
            "epoch: 1839 train_loss: 0.2843\n",
            "epoch: 1840 train_loss: 0.2843\n",
            "epoch: 1841 train_loss: 0.2843\n",
            "epoch: 1842 train_loss: 0.2843\n",
            "epoch: 1843 train_loss: 0.2843\n",
            "epoch: 1844 train_loss: 0.2843\n",
            "epoch: 1845 train_loss: 0.2843\n",
            "epoch: 1846 train_loss: 0.2843\n",
            "epoch: 1847 train_loss: 0.2843\n",
            "epoch: 1848 train_loss: 0.2843\n",
            "epoch: 1849 train_loss: 0.2843\n",
            "epoch: 1850 train_loss: 0.2843\n",
            "epoch: 1851 train_loss: 0.2843\n",
            "epoch: 1852 train_loss: 0.2843\n",
            "epoch: 1853 train_loss: 0.2843\n",
            "epoch: 1854 train_loss: 0.2843\n",
            "epoch: 1855 train_loss: 0.2843\n",
            "epoch: 1856 train_loss: 0.2843\n",
            "epoch: 1857 train_loss: 0.2843\n",
            "epoch: 1858 train_loss: 0.2843\n",
            "epoch: 1859 train_loss: 0.2843\n",
            "epoch: 1860 train_loss: 0.2843\n",
            "epoch: 1861 train_loss: 0.2843\n",
            "epoch: 1862 train_loss: 0.2843\n",
            "epoch: 1863 train_loss: 0.2843\n",
            "epoch: 1864 train_loss: 0.2843\n",
            "epoch: 1865 train_loss: 0.2843\n",
            "epoch: 1866 train_loss: 0.2843\n",
            "epoch: 1867 train_loss: 0.2843\n",
            "epoch: 1868 train_loss: 0.2843\n",
            "epoch: 1869 train_loss: 0.2843\n",
            "epoch: 1870 train_loss: 0.2843\n",
            "epoch: 1871 train_loss: 0.2843\n",
            "epoch: 1872 train_loss: 0.2843\n",
            "epoch: 1873 train_loss: 0.2843\n",
            "epoch: 1874 train_loss: 0.2843\n",
            "epoch: 1875 train_loss: 0.2843\n",
            "epoch: 1876 train_loss: 0.2843\n",
            "epoch: 1877 train_loss: 0.2843\n",
            "epoch: 1878 train_loss: 0.2843\n",
            "epoch: 1879 train_loss: 0.2843\n",
            "epoch: 1880 train_loss: 0.2843\n",
            "epoch: 1881 train_loss: 0.2843\n",
            "epoch: 1882 train_loss: 0.2843\n",
            "epoch: 1883 train_loss: 0.2843\n",
            "epoch: 1884 train_loss: 0.2843\n",
            "epoch: 1885 train_loss: 0.2843\n",
            "epoch: 1886 train_loss: 0.2843\n",
            "epoch: 1887 train_loss: 0.2843\n",
            "epoch: 1888 train_loss: 0.2843\n",
            "epoch: 1889 train_loss: 0.2843\n",
            "epoch: 1890 train_loss: 0.2843\n",
            "epoch: 1891 train_loss: 0.2843\n",
            "epoch: 1892 train_loss: 0.2843\n",
            "epoch: 1893 train_loss: 0.2843\n",
            "epoch: 1894 train_loss: 0.2843\n",
            "epoch: 1895 train_loss: 0.2843\n",
            "epoch: 1896 train_loss: 0.2843\n",
            "epoch: 1897 train_loss: 0.2843\n",
            "epoch: 1898 train_loss: 0.2843\n",
            "epoch: 1899 train_loss: 0.2843\n",
            "epoch: 1900 train_loss: 0.2843\n",
            "epoch: 1901 train_loss: 0.2843\n",
            "epoch: 1902 train_loss: 0.2843\n",
            "epoch: 1903 train_loss: 0.2843\n",
            "epoch: 1904 train_loss: 0.2843\n",
            "epoch: 1905 train_loss: 0.2843\n",
            "epoch: 1906 train_loss: 0.2843\n",
            "epoch: 1907 train_loss: 0.2843\n",
            "epoch: 1908 train_loss: 0.2843\n",
            "epoch: 1909 train_loss: 0.2843\n",
            "epoch: 1910 train_loss: 0.2843\n",
            "epoch: 1911 train_loss: 0.2843\n",
            "epoch: 1912 train_loss: 0.2843\n",
            "epoch: 1913 train_loss: 0.2843\n",
            "epoch: 1914 train_loss: 0.2843\n",
            "epoch: 1915 train_loss: 0.2843\n",
            "epoch: 1916 train_loss: 0.2843\n",
            "epoch: 1917 train_loss: 0.2843\n",
            "epoch: 1918 train_loss: 0.2843\n",
            "epoch: 1919 train_loss: 0.2843\n",
            "epoch: 1920 train_loss: 0.2843\n",
            "epoch: 1921 train_loss: 0.2843\n",
            "epoch: 1922 train_loss: 0.2843\n",
            "epoch: 1923 train_loss: 0.2843\n",
            "epoch: 1924 train_loss: 0.2843\n",
            "epoch: 1925 train_loss: 0.2843\n",
            "epoch: 1926 train_loss: 0.2843\n",
            "epoch: 1927 train_loss: 0.2843\n",
            "epoch: 1928 train_loss: 0.2843\n",
            "epoch: 1929 train_loss: 0.2843\n",
            "epoch: 1930 train_loss: 0.2843\n",
            "epoch: 1931 train_loss: 0.2843\n",
            "epoch: 1932 train_loss: 0.2843\n",
            "epoch: 1933 train_loss: 0.2843\n",
            "epoch: 1934 train_loss: 0.2843\n",
            "epoch: 1935 train_loss: 0.2843\n",
            "epoch: 1936 train_loss: 0.2843\n",
            "epoch: 1937 train_loss: 0.2843\n",
            "epoch: 1938 train_loss: 0.2843\n",
            "epoch: 1939 train_loss: 0.2843\n",
            "epoch: 1940 train_loss: 0.2843\n",
            "epoch: 1941 train_loss: 0.2843\n",
            "epoch: 1942 train_loss: 0.2843\n",
            "epoch: 1943 train_loss: 0.2843\n",
            "epoch: 1944 train_loss: 0.2843\n",
            "epoch: 1945 train_loss: 0.2843\n",
            "epoch: 1946 train_loss: 0.2843\n",
            "epoch: 1947 train_loss: 0.2843\n",
            "epoch: 1948 train_loss: 0.2843\n",
            "epoch: 1949 train_loss: 0.2843\n",
            "epoch: 1950 train_loss: 0.2843\n",
            "epoch: 1951 train_loss: 0.2843\n",
            "epoch: 1952 train_loss: 0.2843\n",
            "epoch: 1953 train_loss: 0.2843\n",
            "epoch: 1954 train_loss: 0.2843\n",
            "epoch: 1955 train_loss: 0.2843\n",
            "epoch: 1956 train_loss: 0.2843\n",
            "epoch: 1957 train_loss: 0.2843\n",
            "epoch: 1958 train_loss: 0.2843\n",
            "epoch: 1959 train_loss: 0.2843\n",
            "epoch: 1960 train_loss: 0.2843\n",
            "epoch: 1961 train_loss: 0.2843\n",
            "epoch: 1962 train_loss: 0.2843\n",
            "epoch: 1963 train_loss: 0.2843\n",
            "epoch: 1964 train_loss: 0.2843\n",
            "epoch: 1965 train_loss: 0.2843\n",
            "epoch: 1966 train_loss: 0.2843\n",
            "epoch: 1967 train_loss: 0.2843\n",
            "epoch: 1968 train_loss: 0.2843\n",
            "epoch: 1969 train_loss: 0.2843\n",
            "epoch: 1970 train_loss: 0.2843\n",
            "epoch: 1971 train_loss: 0.2843\n",
            "epoch: 1972 train_loss: 0.2843\n",
            "epoch: 1973 train_loss: 0.2843\n",
            "epoch: 1974 train_loss: 0.2843\n",
            "epoch: 1975 train_loss: 0.2843\n",
            "epoch: 1976 train_loss: 0.2843\n",
            "epoch: 1977 train_loss: 0.2843\n",
            "epoch: 1978 train_loss: 0.2843\n",
            "epoch: 1979 train_loss: 0.2843\n",
            "epoch: 1980 train_loss: 0.2843\n",
            "epoch: 1981 train_loss: 0.2843\n",
            "epoch: 1982 train_loss: 0.2843\n",
            "epoch: 1983 train_loss: 0.2843\n",
            "epoch: 1984 train_loss: 0.2843\n",
            "epoch: 1985 train_loss: 0.2843\n",
            "epoch: 1986 train_loss: 0.2843\n",
            "epoch: 1987 train_loss: 0.2843\n",
            "epoch: 1988 train_loss: 0.2843\n",
            "epoch: 1989 train_loss: 0.2843\n",
            "epoch: 1990 train_loss: 0.2843\n",
            "epoch: 1991 train_loss: 0.2843\n",
            "epoch: 1992 train_loss: 0.2843\n",
            "epoch: 1993 train_loss: 0.2843\n",
            "epoch: 1994 train_loss: 0.2843\n",
            "epoch: 1995 train_loss: 0.2843\n",
            "epoch: 1996 train_loss: 0.2843\n",
            "epoch: 1997 train_loss: 0.2843\n",
            "epoch: 1998 train_loss: 0.2843\n",
            "epoch: 1999 train_loss: 0.2843\n",
            "Finish training after 2000 epochs\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# plot_learning_curve(model_loss_record, title='MF model')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "V[1:4]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6415,  0.5864, -0.4928,  0.1856, -0.6383, -0.3647,  0.3854, -0.5441,\n",
              "         -0.1961,  0.2173,  0.4751,  0.2366, -0.7045, -0.6043, -0.4009, -0.3298,\n",
              "         -0.7767, -0.6387, -0.6175, -0.8895],\n",
              "        [ 0.4283,  0.2832, -0.0845, -0.1470, -0.0795, -0.3145, -0.0724,  0.0375,\n",
              "         -0.4534,  0.6837,  0.3904,  0.2906, -0.4650, -0.6292, -0.5667, -0.1778,\n",
              "         -0.6784, -0.3649, -0.6034, -0.5467],\n",
              "        [ 0.2628,  0.4229, -0.4310, -0.4454, -0.3224, -0.5331,  0.2927, -0.3426,\n",
              "         -0.3448,  0.8053,  0.0897, -0.4763,  0.0247, -0.4313, -0.4606, -0.3986,\n",
              "         -0.4444, -0.6274, -0.4168, -0.4746]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "with torch.no_grad():\n",
        "    print(model.net(V_tilde[1:4].to(device)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4187,  0.3437, -0.1813, -0.1442, -0.3098, -0.3514,  0.3227, -0.3371,\n",
            "         -0.4475,  0.3324,  0.3210,  0.2577, -0.2884, -0.2103, -0.4819, -0.1831,\n",
            "         -0.1221, -0.3170, -0.2455, -0.3142],\n",
            "        [ 0.5004,  0.4015, -0.2357, -0.3144, -0.3439, -0.3841,  0.5069, -0.4297,\n",
            "         -0.3418,  0.2850,  0.2506,  0.3777, -0.4287, -0.2568, -0.3792, -0.2197,\n",
            "         -0.2918, -0.3717, -0.1971, -0.3096],\n",
            "        [ 0.8974,  0.4406, -0.3025, -0.4658, -0.4909, -0.5831,  0.4129, -0.2776,\n",
            "         -0.2709,  0.3251,  0.3821,  0.2295, -0.4354, -0.3237, -0.5151, -0.4819,\n",
            "         -0.2715, -0.5388, -0.5477, -0.4067]], device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}