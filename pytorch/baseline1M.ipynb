{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('py38': conda)"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b1d710d4a2dd0e836743a9708dcf2dd87750cb6db75a03dbc0a1931aaec4e6cb"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from utils.mylib import *\n",
        "from d2l import torch as d2l"
      ],
      "outputs": [],
      "metadata": {
        "id": "k-onQd4JNA5H",
        "outputId": "0317f255-2dd4-4eeb-822f-ed20d8ea782b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "init_Seed()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users=6050, m_items=4050, n_factors=50):\n",
        "        super(MF, self).__init__()\n",
        "\n",
        "        self.U = torch.nn.Parameter(torch.rand(n_users, n_factors, requires_grad=True))\n",
        "\n",
        "        self.P = torch.rand(m_items, n_factors, requires_grad=False, device=device)\n",
        "        self.Q = torch.nn.Parameter(torch.rand(n_factors, n_factors, requires_grad=True))\n",
        "        \n",
        "        # self.net = nn.Sequential(nn.Linear(n_factors, 500 * n_factors), nn.ReLU(), nn.Linear(500 * n_factors, n_factors))\n",
        "\n",
        "\n",
        "        # self.init_net()\n",
        "\n",
        "        self.U.data.uniform_(-0.05, 0.05)\n",
        "        self.P.data.uniform_(-0.05, 0.05)\n",
        "        self.Q.data.normal_(0, 0.1)\n",
        "        \n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        return torch.einsum('ij, ij -> i', [self.U[user], self.P[item] @ self.Q])\n",
        "        # return torch.einsum('ij, ij -> i', [self.U[user], self.net(self.P[item])])\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)\n",
        "    \n",
        "    # def init_net(self):\n",
        "    #     def init_weights(m):\n",
        "    #         if type(m) == nn.Linear:\n",
        "    #             nn.init.normal_(m.weight, std=0.01)\n",
        "    #     self.net.apply(init_weights)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def train(tr_set, dv_set, model, config):\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.95)\n",
        "\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []} \n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < n_epochs:\n",
        "        model.train()\n",
        "        for X, y in tr_set:\n",
        "            optimizer.zero_grad()    \n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = model(X[:, 0], X[:, 1])\n",
        "\n",
        "            mse_loss = model.cal_loss(y_hat, y)\n",
        "            mse_loss.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "\n",
        "            # loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "            # print(\"train_loss: {:.4f}\".format(mse_loss.detach().cpu().item()))\n",
        "            print(\"train_loss: {:.4f}\".format(np.sqrt(mse_loss.detach().cpu().item())))\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch += 1\n",
        "\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            min_mse = dev_mse\n",
        "            early_stop_cnt = 0\n",
        "            print(\"Saving model (epoch = {:4d}  loss = {:.4f} )\".format(epoch, np.sqrt(dev_mse)))\n",
        "            torch.save(model.state_dict(), config['save_path'])\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "        \n",
        "        loss_record['dev'].append(dev_mse)\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            break\n",
        "\n",
        "    print(\"Finish training after {} epochs\".format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/           \n",
        "\n",
        "config = {\n",
        "    'n_epochs': 500,              # maximum number of epochs\n",
        "    'batch_size': 50000,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.005,                # learning rate\n",
        "        # 'weight_decay': 0.001\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 5,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth',  # your model will be saved here\n",
        "    'D': 50\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "NPXpdumwPjE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and model**"
      ],
      "metadata": {
        "id": "6j1eOV3TOH-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "ML1M_train = '../data/ML1M/ML1M_copy1_train.txt'\n",
        "ML1M_test = '../data/ML1M/ML1M_copy1_test.txt'\n",
        "\n",
        "tr_set = prep_dataloader(ML1M_train, 'train', config['batch_size'])\n",
        "dv_set = prep_dataloader(ML1M_test, 'dev', config['batch_size'])\n",
        "# tt_set = prep_dataloader(\"data/ML100K/ML100K_copy1_test.txt\", 'test', config['batch_size'], target_only=target_only)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the train set of MoviesLen Dataset (600126 samples found, each dim = 2)\n",
            "Max user: 6040\n",
            "Max item: 3952\n",
            "Finished reading the dev set of MoviesLen Dataset (200041 samples found, each dim = 2)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "model = MF(n_factors=config['D']).to(device) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Training!**"
      ],
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# %%time\n",
        "# for _ in range(4):\n",
        "#     model_loss, model_loss_record = train(tr_set, dv_set, model, config)\n",
        "#     model.P = model.net(model.P).detach()\n",
        "#     # model.net = nn.Sequential(nn.Linear(config['D'], config['D']), nn.ReLU(), nn.Linear(config['D'], config['D']))\n",
        "#     # model = model.to(device)\n",
        "#     model.init_net()\n",
        "#     print(\"ending a era !!!\")\n",
        "\n",
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss: 3.7512\n",
            "train_loss: 3.7467\n",
            "train_loss: 3.7565\n",
            "train_loss: 3.7641\n",
            "train_loss: 3.7613\n",
            "train_loss: 3.7506\n",
            "train_loss: 3.7519\n",
            "train_loss: 3.7490\n",
            "train_loss: 3.7508\n",
            "train_loss: 3.7450\n",
            "train_loss: 3.7472\n",
            "train_loss: 3.7535\n",
            "train_loss: 3.6597\n",
            "Saving model (epoch =    1  loss = 3.7498 )\n",
            "train_loss: 3.7496\n",
            "train_loss: 3.7484\n",
            "train_loss: 3.7521\n",
            "train_loss: 3.7541\n",
            "train_loss: 3.7524\n",
            "train_loss: 3.7493\n",
            "train_loss: 3.7453\n",
            "train_loss: 3.7484\n",
            "train_loss: 3.7517\n",
            "train_loss: 3.7497\n",
            "train_loss: 3.7461\n",
            "train_loss: 3.7476\n",
            "train_loss: 3.8413\n",
            "Saving model (epoch =    2  loss = 3.7484 )\n",
            "train_loss: 3.7401\n",
            "train_loss: 3.7444\n",
            "train_loss: 3.7446\n",
            "train_loss: 3.7440\n",
            "train_loss: 3.7466\n",
            "train_loss: 3.7459\n",
            "train_loss: 3.7494\n",
            "train_loss: 3.7476\n",
            "train_loss: 3.7398\n",
            "train_loss: 3.7442\n",
            "train_loss: 3.7460\n",
            "train_loss: 3.7453\n",
            "train_loss: 3.7211\n",
            "Saving model (epoch =    3  loss = 3.7446 )\n",
            "train_loss: 3.7357\n",
            "train_loss: 3.7353\n",
            "train_loss: 3.7419\n",
            "train_loss: 3.7403\n",
            "train_loss: 3.7403\n",
            "train_loss: 3.7311\n",
            "train_loss: 3.7425\n",
            "train_loss: 3.7357\n",
            "train_loss: 3.7368\n",
            "train_loss: 3.7346\n",
            "train_loss: 3.7329\n",
            "train_loss: 3.7259\n",
            "train_loss: 3.7400\n",
            "Saving model (epoch =    4  loss = 3.7375 )\n",
            "train_loss: 3.7304\n",
            "train_loss: 3.7257\n",
            "train_loss: 3.7216\n",
            "train_loss: 3.7226\n",
            "train_loss: 3.7237\n",
            "train_loss: 3.7198\n",
            "train_loss: 3.7244\n",
            "train_loss: 3.7260\n",
            "train_loss: 3.7139\n",
            "train_loss: 3.7212\n",
            "train_loss: 3.7188\n",
            "train_loss: 3.7183\n",
            "train_loss: 3.8963\n",
            "Saving model (epoch =    5  loss = 3.7268 )\n",
            "train_loss: 3.7107\n",
            "train_loss: 3.7069\n",
            "train_loss: 3.7091\n",
            "train_loss: 3.7078\n",
            "train_loss: 3.7102\n",
            "train_loss: 3.7092\n",
            "train_loss: 3.7068\n",
            "train_loss: 3.7012\n",
            "train_loss: 3.6954\n",
            "train_loss: 3.6999\n",
            "train_loss: 3.7010\n",
            "train_loss: 3.6884\n",
            "train_loss: 3.6840\n",
            "Saving model (epoch =    6  loss = 3.7143 )\n",
            "train_loss: 3.6924\n",
            "train_loss: 3.6858\n",
            "train_loss: 3.6856\n",
            "train_loss: 3.6781\n",
            "train_loss: 3.6885\n",
            "train_loss: 3.6813\n",
            "train_loss: 3.6899\n",
            "train_loss: 3.6822\n",
            "train_loss: 3.6792\n",
            "train_loss: 3.6790\n",
            "train_loss: 3.6839\n",
            "train_loss: 3.6726\n",
            "train_loss: 3.7144\n",
            "Saving model (epoch =    7  loss = 3.7026 )\n",
            "train_loss: 3.6644\n",
            "train_loss: 3.6794\n",
            "train_loss: 3.6672\n",
            "train_loss: 3.6574\n",
            "train_loss: 3.6641\n",
            "train_loss: 3.6677\n",
            "train_loss: 3.6609\n",
            "train_loss: 3.6620\n",
            "train_loss: 3.6613\n",
            "train_loss: 3.6576\n",
            "train_loss: 3.6590\n",
            "train_loss: 3.6592\n",
            "train_loss: 3.7749\n",
            "Saving model (epoch =    8  loss = 3.6943 )\n",
            "train_loss: 3.6455\n",
            "train_loss: 3.6484\n",
            "train_loss: 3.6487\n",
            "train_loss: 3.6525\n",
            "train_loss: 3.6420\n",
            "train_loss: 3.6523\n",
            "train_loss: 3.6452\n",
            "train_loss: 3.6422\n",
            "train_loss: 3.6434\n",
            "train_loss: 3.6520\n",
            "train_loss: 3.6436\n",
            "train_loss: 3.6393\n",
            "train_loss: 3.7053\n",
            "Saving model (epoch =    9  loss = 3.6890 )\n",
            "train_loss: 3.6342\n",
            "train_loss: 3.6365\n",
            "train_loss: 3.6417\n",
            "train_loss: 3.6379\n",
            "train_loss: 3.6312\n",
            "train_loss: 3.6297\n",
            "train_loss: 3.6284\n",
            "train_loss: 3.6355\n",
            "train_loss: 3.6262\n",
            "train_loss: 3.6285\n",
            "train_loss: 3.6303\n",
            "train_loss: 3.6211\n",
            "train_loss: 3.6592\n",
            "Saving model (epoch =   10  loss = 3.6861 )\n",
            "train_loss: 3.6273\n",
            "train_loss: 3.6121\n",
            "train_loss: 3.6240\n",
            "train_loss: 3.6209\n",
            "train_loss: 3.6183\n",
            "train_loss: 3.6199\n",
            "train_loss: 3.6319\n",
            "train_loss: 3.6121\n",
            "train_loss: 3.6219\n",
            "train_loss: 3.6145\n",
            "train_loss: 3.6146\n",
            "train_loss: 3.6127\n",
            "train_loss: 3.5811\n",
            "Saving model (epoch =   11  loss = 3.6844 )\n",
            "train_loss: 3.6110\n",
            "train_loss: 3.6082\n",
            "train_loss: 3.6103\n",
            "train_loss: 3.6059\n",
            "train_loss: 3.6106\n",
            "train_loss: 3.6143\n",
            "train_loss: 3.5966\n",
            "train_loss: 3.6017\n",
            "train_loss: 3.6135\n",
            "train_loss: 3.6009\n",
            "train_loss: 3.6147\n",
            "train_loss: 3.6020\n",
            "train_loss: 3.6112\n",
            "Saving model (epoch =   12  loss = 3.6837 )\n",
            "train_loss: 3.6035\n",
            "train_loss: 3.6010\n",
            "train_loss: 3.6023\n",
            "train_loss: 3.6001\n",
            "train_loss: 3.5914\n",
            "train_loss: 3.5950\n",
            "train_loss: 3.6049\n",
            "train_loss: 3.5894\n",
            "train_loss: 3.6024\n",
            "train_loss: 3.5803\n",
            "train_loss: 3.5907\n",
            "train_loss: 3.5953\n",
            "train_loss: 3.6959\n",
            "train_loss: 3.5803\n",
            "train_loss: 3.5947\n",
            "train_loss: 3.5849\n",
            "train_loss: 3.5944\n",
            "train_loss: 3.5882\n",
            "train_loss: 3.5894\n",
            "train_loss: 3.5829\n",
            "train_loss: 3.5807\n",
            "train_loss: 3.5904\n",
            "train_loss: 3.5711\n",
            "train_loss: 3.5916\n",
            "train_loss: 3.5801\n",
            "train_loss: 3.6466\n",
            "train_loss: 3.5817\n",
            "train_loss: 3.5741\n",
            "train_loss: 3.5794\n",
            "train_loss: 3.5708\n",
            "train_loss: 3.5590\n",
            "train_loss: 3.5767\n",
            "train_loss: 3.5710\n",
            "train_loss: 3.5800\n",
            "train_loss: 3.5741\n",
            "train_loss: 3.5797\n",
            "train_loss: 3.5697\n",
            "train_loss: 3.5864\n",
            "train_loss: 3.6215\n",
            "train_loss: 3.5650\n",
            "train_loss: 3.5616\n",
            "train_loss: 3.5626\n",
            "train_loss: 3.5696\n",
            "train_loss: 3.5699\n",
            "train_loss: 3.5627\n",
            "train_loss: 3.5693\n",
            "train_loss: 3.5670\n",
            "train_loss: 3.5695\n",
            "train_loss: 3.5604\n",
            "train_loss: 3.5605\n",
            "train_loss: 3.5589\n",
            "train_loss: 3.6413\n",
            "train_loss: 3.5565\n",
            "train_loss: 3.5568\n",
            "train_loss: 3.5530\n",
            "train_loss: 3.5543\n",
            "train_loss: 3.5521\n",
            "train_loss: 3.5513\n",
            "train_loss: 3.5498\n",
            "train_loss: 3.5494\n",
            "train_loss: 3.5647\n",
            "train_loss: 3.5532\n",
            "train_loss: 3.5514\n",
            "train_loss: 3.5606\n",
            "train_loss: 3.7053\n",
            "train_loss: 3.5383\n",
            "train_loss: 3.5470\n",
            "train_loss: 3.5558\n",
            "train_loss: 3.5440\n",
            "train_loss: 3.5453\n",
            "train_loss: 3.5459\n",
            "train_loss: 3.5385\n",
            "train_loss: 3.5424\n",
            "train_loss: 3.5443\n",
            "train_loss: 3.5431\n",
            "train_loss: 3.5443\n",
            "train_loss: 3.5430\n",
            "train_loss: 3.4963\n",
            "Finish training after 18 epochs\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "plot_learning_curve(model_loss_record, title='MF model')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZb0lEQVR4nO3df7RVdZ3/8ecrvHkBUZDIVAxQSUm+I/HDbFC+wpQCmr9airVssmz4zmIq+jYzZc7MWvbt649pbJqYMqIvZP4CxzF/kVo6gr9Qi6tIKCCiGNc0FAOBwAF5f//Y++Lhes65F87d93A/9/VY6yz2r7P3e+9zeN3P+ex99lFEYGZm6XlPvQswM7NiOODNzBLlgDczS5QD3swsUQ54M7NEOeDNzBLlgLdCSTpZ0sp617GvkDRW0ipJmyWdXe96aiVpoaQvtnPZkHR00TXZOxzwCZO0RtLH61lDRDwcEcfUs4Z9zP8BfhARB0TE7a1n5q/Zf0t6X6vpT+UBOTgfvzZfbnPJY0qn7IF1GQ54q4mkHvWuoVadvA+DgGfaWOZF4NMtI5L+B9CrzHLfyf9QtDxu7sA6LQEO+G5I0nskXSJptaT1kv5D0sEl82+R9KqkjZIeknRcybxrJf1I0t2StgDj81bn30lamj/nZkmN+fKnSGoueX7FZfP5X5f0iqTfS/pitY/1kg6W9NN82T9Kuj2ffpGkR1otu2s9Zfbh7/L97VGy/DmSlrbneJWp668kPS/pDUl3Sjosn74aOBK4K29x719hFdcDf1ky/jngukrba0u+79PyrqFNkr4t6ShJiyS9me/Pe9uqP5/3CUkr8tfuB4BabesLkpbnr8cvJQ3a27qtdg747unLwNnA/wQOA/4I/LBk/j3AUOD9wJPAja2e/xngcqAP0BKk5wMTgSHAnwEXVdl+2WUlTQS+BnwcOBo4pY39uJ6sZXtcXuv32li+0j58H9gCTGg1/6Z8uK3jtYukCcCVZPt4KPASMA8gIo4Cfgd8Mm9xv1WhtseBAyUNy//oXADcsAf7Vs5pwCjgRODrwCzgQuAIYDj5J4Zq9efdRj8H/hF4H7AaGFuy72cBlwLnAgOAh4G5NdZttYgIPxJ9AGuAj5eZvhz4i5LxQ4HtwH5llu0LBHBQPn4tcF2Z7VxYMv4dYGY+fArQ3M5l5wBXlsw7Ot/20WXqOhTYCfQrM+8i4JFW03atp8I+/F9gTj7chyzwB+3F8ZpN1nXSMn5Avuzgaq9J69eMLESvJPtDeB+wX74Pg0v2YRuwIX+8XmWdAYwtGW8CvlEy/l3g39qqn+xTxeMl8wQ0A1/Mx+8BLi6Z/x7gTyXHsexr6UdxD7fgu6dBwG2SNkjaQBZgbwOHSOoh6aq8O+JNssCBrMXWYm2Zdb5aMvwnsmCopNKyh7Vad7nttDgCeCMi/lhlmWpar/sm4Ny82+Rc4MmIeCmfV/F4lVnvYWStXgAiYjOwHjh8D+u7nuxTxEVU7p65OiL65o/3VVimxR9KhreWGS99DSrVv9vrE1lqlx7HQcD3S47TG2R/BPZ0362DOOC7p7XApJJw6BsRjRHxMlmonEXWijyIrOUGu/e1FnUL0leAgSXjR1RZdi1wsKS+ZeZtoeSkpKQPlFlmt32IiGfJgm0Su3fPtGyr0vFq7fdkQdey7d5Af6DcshXlf1xeBCaTdYt0lmr1v0LJayJJ7P4arQX+V6vj1DMiFnVO6daaAz59DZIaSx77ATOBy1tOgEkakPefQtY98RZZq60XcEUn1vofwOfzvudewD9VWjAiXiHrErhGUj9JDZLG5bOfBo6TNCI/gXtZO7d/EzAdGAfcUjK92vFqbW6+DyPyTwNXAE9ExJp21lDqYmBCRGzZi+furWr1/4LsuJ6bv4++ApT+8ZwJfFP5SXlJB0k6rxNrt1Yc8Om7m+wjeMvjMrKTincCv5K0ieyk3kfz5a8ja8m+DDybz+sUEXEPMANYADxfsu1KJyM/S9Y/vAJYB3w1X89zZNeb3w+s4p0TwW2ZS3Yi9YGIeL1kerXj1Xof7if7w3QrWYv3KLKTpHssIlZHxOK9ee7eqlZ/fkzOA64iawAMBR4tee5twD8D8/LuvWVkn4isTpR1o5nteyQNIwuJ/SNiR73rMetq3IK3fUp+/fn+kvqRtQbvcrib7Z1CW/CS1gCbyK442BERowvbmCVB0r3Ax8jeMw8C0/L+djPbQ50R8KNb9WeamVkncBeNmVmiim7Bv0j2te4AfhwRs8osMxWYCtC7d+9Rxx57bGH1mJmlpqmp6fWIGFBuXtEBf3hEvCzp/WRft/5yRDxUafnRo0fH4sWdelWYmVmXJqmp0vnNQrtoWr7pFxHrgNuAE4rcnpmZvaOwgJfUW1KflmHgVLJrms3MrBPsV+C6DyG7QVPLdm6KiHsL3J6ZmZUoLOAj4gXg+KLWb2YGsH37dpqbm9m2bVu9SylUY2MjAwcOpKGhod3PKbIFb2ZWuObmZvr06cPgwYPJewySExGsX7+e5uZmhgwZ0u7n+Tp4M+vStm3bRv/+/ZMNdwBJ9O/ff48/pTjgzazLSzncW+zNPjrgzcwS5YA3M6vBhg0buOaaa/b4eZMnT2bDhg0dX1AJB7yZWQ0qBfyOHdXvcn333XfTt2/fgqrK+CoaM7MaXHLJJaxevZoRI0bQ0NBAY2Mj/fr1Y8WKFTz33HOcffbZrF27lm3btjF9+nSmTp0KwODBg1m8eDGbN29m0qRJnHTSSSxatIjDDz+cO+64g549e9ZcmwPezJLx6hVX8NbyFR26zv2HHcsHLr204vyrrrqKZcuWsWTJEhYuXMjpp5/OsmXLdl3OOGfOHA4++GC2bt3KmDFj+NSnPkX//v13W8eqVauYO3cuP/nJTzj//PO59dZbufDCC2uu3QFvZtaBTjjhhN2uVZ8xYwa33XYbAGvXrmXVqlXvCvghQ4YwYsQIAEaNGsWaNWs6pBYHvJklo1pLu7P07t171/DChQu5//77eeyxx+jVqxennHJK2WvZ999//13DPXr0YOvWrR1Si0+ympnVoE+fPmzatKnsvI0bN9KvXz969erFihUrePzxxzu1Nrfgzcxq0L9/f8aOHcvw4cPp2bMnhxxyyK55EydOZObMmQwbNoxjjjmGE088sVNrK/QHP/aUf/DDzPbU8uXLGTZsWL3L6BTl9rVuP/hhZmb144A3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MrINddtllXH311fUuwwFvZpYqB7yZWQe4/PLL+dCHPsRJJ53EypUrAVi9ejUTJ05k1KhRnHzyyaxYsYKNGzcyaNAgdu7cCcCWLVs44ogj2L59e4fX5FsVmFky/mlVM8s2d8yNuloMP6An3x46sOoyTU1NzJs3jyVLlrBjxw5GjhzJqFGjmDp1KjNnzmTo0KE88cQTTJs2jQceeIARI0bw4IMPMn78eObPn89pp51GQ0NDh9YNDngzs5o9/PDDnHPOOfTq1QuAM888k23btrFo0SLOO++8Xcu99dZbAEyZMoWbb76Z8ePHM2/ePKZNm1ZIXQ54M0tGWy3tzrRz50769u3LkiVL3jXvzDPP5NJLL+WNN96gqamJCRMmFFKD++DNzGo0btw4br/9drZu3cqmTZu466676NWrF0OGDOGWW24BICJ4+umnATjggAMYM2YM06dP54wzzqBHjx6F1OWANzOr0ciRI5kyZQrHH388kyZNYsyYMQDceOONzJ49m+OPP57jjjuOO+64Y9dzpkyZwg033MCUKVMKq8u3CzazLs23C/btgs3Muh0HvJlZohzwZtbl7UtdzUXZm310wJtZl9bY2Mj69euTDvmIYP369TQ2Nu7R83wdvJl1aQMHDqS5uZnXXnut3qUUqrGxkYED9+w6fwe8mXVpDQ0NDBkypN5l7JPcRWNmlqjCA15SD0lPSZpf9LbMzOwdndGCnw4s74TtmJlZiUIDXtJA4HTg/xW5HTMze7eiW/D/Bnwd2FlpAUlTJS2WtDj1s+BmZp2psICXdAawLiKaqi0XEbMiYnREjB4wYEBR5ZiZdTtFtuDHAmdKWgPMAyZIuqHA7ZmZWYnCAj4ivhkRAyNiMHAB8EBEXFjU9szMbHe+Dt7MLFGd8k3WiFgILOyMbZmZWcYteDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwS5YA3M0uUA97MLFEOeDOzRDngzcwSVVjAS2qU9GtJT0t6RtK3itqWmZm9234FrvstYEJEbJbUADwi6Z6IeLzAbZqZWa6wgI+IADbnow35I4ranpmZ7a7QPnhJPSQtAdYB90XEE2WWmSppsaTFr732WpHlmJl1K4UGfES8HREjgIHACZKGl1lmVkSMjojRAwYMKLIcM7NuZY8CXtJ7JB24pxuJiA3AAmDinj7XzMz2TpsBL+kmSQdK6g0sA56V9PfteN4ASX3z4Z7AJ4AVNdZrZmbt1J4W/Icj4k3gbOAeYAjw2XY871BggaSlwG/I+uDn722hZma2Z9pzFU1Dfpnj2cAPImK7pDavhomIpcBHaqzPzMz2Unta8D8G1gC9gYckDQLeLLIoMzOrXZst+IiYAcwomfSSpPHFlWRmZh2hPSdZp+cnWSVptqQngQmdUJuZmdWgPV00X8hPsp4K9CM7wXpVoVWZmVnN2hPwyv+dDFwfEc+UTDMzs31UewK+SdKvyAL+l5L6ADuLLcvMzGrVnsskLwZGAC9ExJ8k9Qc+X2hVZmZWs/ZcRbNT0kDgM5IAHoyIuwqvzMzMatKeq2iuAqYDz+aPr0i6oujCzMysNu3popkMjIiInQCSfgY8BVxaZGFmZlab9t5Nsm/J8EEF1GFmZh2sPS34K4GnJC0guzxyHHBJoVWZmVnN2nOSda6khcCYfNI3IuLVQqsyM7OaVQx4SSNbTWrO/z1M0mER8WRxZZmZWa2qteC/W2Ve4PvRmJnt0yoGfET4jpFmZl1YoT+6bWZm9eOANzNLlAPezCxRFQNe0oUlw2NbzftSkUWZmVntqrXgv1Yy/O+t5n2hgFrMzKwDVQt4VRguN25mZvuYagEfFYbLjZuZ2T6m2hedjpW0lKy1flQ+TD5+ZOGVmZlZTaoF/LBOq8LMzDpctW+yvlQ6nv9U3zjgdxHRVHRhZmZWm2qXSc6XNDwfPhRYRnb1zPWSvto55ZmZ2d6qdpJ1SEQsy4c/D9wXEZ8EPoovkzQz2+dVC/jtJcN/AdwNEBGbgJ1FFmVmZrWrdpJ1raQvk90HfiRwL4CknkBDJ9RmZmY1qNaCvxg4DrgImBIRG/LpJwI/LbYsMzOrVbWraNYBf11m+gJgQZFFmZlZ7ar9ZN+d1Z4YEWd2fDlmZtZRqvXBfwxYC8wFnsD3nzEz61KqBfwHgE8AnwY+A/wCmBsRz3RGYWZmVpuKJ1kj4u2IuDciPkd2YvV5YGF77wUv6QhJCyQ9K+kZSdM7qGYzM2uHai14JO0PnE7Wih8MzABua+e6dwB/GxFPSuoDNEm6LyKeraFeMzNrp2onWa8DhpN9welbJd9qbZeIeAV4JR/eJGk5cDjggDcz6wSKKH9rd0k7gS35aOlCAiIiDmz3RqTBwEPA8Ih4s9W8qcBUgA9+8IOjXnrppXevwMzMypLUFBGjy82rdh18h/wgt6QDgFuBr7YO93w7s4BZAKNHj/YPiZiZdZAOCfFKJDWQhfuNEfHzIrdlZma7KyzgJQmYDSyPiH8tajtmZlZekS34scBngQmSluSPyQVuz8zMSlS9TLIWEfEI/varmVndFNoHb2Zm9eOANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLlAPezCxRDngzs0Q54M3MEuWANzNLVGEBL2mOpHWSlhW1DTMzq6zIFvy1wMQC129mZlUUFvAR8RDwRlHrNzOz6ureBy9pqqTFkha/9tpr9S7HzCwZdQ/4iJgVEaMjYvSAAQPqXY6ZWTLqHvBmZlYMB7yZWaKKvExyLvAYcIykZkkXF7UtMzN7t/2KWnFEfLqodZuZWdvcRWNmligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJcsCbmSXKAW9mligHvJlZohzwZmaJKjTgJU2UtFLS85IuKXJbZma2u8ICXlIP4IfAJODDwKclfbio7ZmZ2e6KbMGfADwfES9ExH8D84CzCtyemZmV2K/AdR8OrC0ZbwY+2nohSVOBqfnoZkkr93J77wNe38vnpsLHwMeghY9D9zkGgyrNKDLg2yUiZgGzal2PpMURMboDSuqyfAx8DFr4OPgYQLFdNC8DR5SMD8ynmZlZJygy4H8DDJU0RNJ7gQuAOwvcnpmZlSisiyYidkj6EvBLoAcwJyKeKWp7dEA3TwJ8DHwMWvg4+BigiKh3DWZmVgB/k9XMLFEOeDOzRHX5gO/Ot0OQtEbSbyUtkbQ4n3awpPskrcr/7VfvOjuSpDmS1klaVjKt7D4rMyN/byyVNLJ+lXecCsfgMkkv5++FJZIml8z7Zn4MVko6rT5VdyxJR0haIOlZSc9Imp5P71bvhbZ06YD37RAAGB8RI0qu970E+K+IGAr8Vz6ekmuBia2mVdrnScDQ/DEV+FEn1Vi0a3n3MQD4Xv5eGBERdwPk/x8uAI7Ln3NN/v+mq9sB/G1EfBg4EfibfF+723uhqi4d8Ph2COWcBfwsH/4ZcHb9Sul4EfEQ8EaryZX2+Szgusg8DvSVdGinFFqgCsegkrOAeRHxVkS8CDxP9v+mS4uIVyLiyXx4E7Cc7Nvz3eq90JauHvDlbodweJ1qqYcAfiWpKb/lA8AhEfFKPvwqcEh9SutUlfa5u70/vpR3P8wp6ZpL/hhIGgx8BHgCvxd209UDvrs7KSJGkn38/BtJ40pnRnYNbLe6DrY77nPuR8BRwAjgFeC7da2mk0g6ALgV+GpEvFk6rxu/F3bp6gHfrW+HEBEv5/+uA24j++j9h5aPnvm/6+pXYaeptM/d5v0REX+IiLcjYifwE97phkn2GEhqIAv3GyPi5/nkbv9eKNXVA77b3g5BUm9JfVqGgVOBZWT7/7l8sc8Bd9Snwk5VaZ/vBP4yv4LiRGBjycf3pLTqTz6H7L0A2TG4QNL+koaQnWT8dWfX19EkCZgNLI+Ify2Z1e3fC7uJiC79ACYDzwGrgX+odz2duN9HAk/nj2da9h3oT3b1wCrgfuDgetfawfs9l6wLYjtZP+rFlfYZENlVVquB3wKj611/gcfg+nwfl5KF2aEly/9DfgxWApPqXX8HHYOTyLpflgJL8sfk7vZeaOvhWxWYmSWqq3fRmJlZBQ54M7NEOeDNzBLlgDczS5QD3swsUQ5422dJ6l9yd8RXW90t8b1tPHe0pBnt2Maijqv4XevuK2laUes3a4svk7QuQdJlwOaIuLpk2n4RsaN+VVWX3yNlfkQMr3ct1j25BW9diqRrJc2U9ATwHUknSHpM0lOSFkk6Jl/uFEnz8+HL8htwLZT0gqSvlKxvc8nyCyX9p6QVkm7Mvy2JpMn5tKb8nuLzy9R1nKRf558ulkoaClwFHJVP+5d8ub+X9Jt8mW/l0waXbHN5XkOvfN5V+T3Pl0q6uvV2zaop7Ee3zQo0EPjziHhb0oHAyZH9yPvHgSuAT5V5zrHAeKAPsFLSjyJie6tlPkJ23/TfA48CY5X9kMqPgXER8aKkuRVq+mvg+xFxY9591IPsXuTDI2IEgKRTyW4VcALZNyvvzG8Q9zvgGODiiHhU0hxgmqSfkt124NiICEl99/RAWffmFrx1RbdExNv58EHALcp+3eh7ZAFdzi8iuyf662Q3oCp3G+VfR0RzZDfsWgIMJvvD8EJk91KH7DYB5TwGXCrpG8CgiNhaZplT88dTwJP5uofm89ZGxKP58A1kX8XfCGwDZks6F/hThW2bleWAt65oS8nwt4EFeT/3J4HGCs95q2T4bcp/em3PMmVFxE3AmcBW4G5JE8osJuDKeOdXl46OiNktq3j3KmMHWWv/P4EzgHvbW48ZOOCt6zuId277elEB618JHJmfMAWYUm4hSUeStfRnkN3B8M+ATWRdQi1+CXwhv4c5kg6X9P583gclfSwf/gzwSL7cQZH9/N7/Bo7vuN2y7sABb13dd4ArJT1FAeeU8q6WacC9kprIQntjmUXPB5ZJWgIMJ/t5uPXAo5KWSfqXiPgVcBPwmKTfkrXMW/4ArCT70ZblQD+yH/DoA8yXtBR4BPhaR++fpc2XSZq1QdIBEbE5v6rmh8CqiPheB65/ML6c0grgFrxZ2/4qb5k/Q9Yl9OP6lmPWPm7Bm5klyi14M7NEOeDNzBLlgDczS5QD3swsUQ54M7NE/X8yR5GXGgPuxQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "01787b1a-86eb-4560-81ad-62766a56b2b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "np.sqrt(model_loss)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6836760959308625"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "np.sqrt(model_loss_record['dev'][-1])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6916185376571597"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# del model\n",
        "# model = MF().to(device)\n",
        "# ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "# model.load_state_dict(ckpt)\n",
        "# # plot_pred(dv_set, model, device)  # Show prediction on the validation set\n",
        "# dev(dv_set, model, device)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "c2e65627-13cd-46f9-e51b-5973eb26d865"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ],
      "metadata": {
        "id": "aQikz3IPiyPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# def save_pred(preds, file):\n",
        "#     ''' Save predictions to specified file '''\n",
        "#     print('Saving results to {}'.format(file))\n",
        "#     with open(file, 'w') as fp:\n",
        "#         writer = csv.writer(fp)\n",
        "#         writer.writerow(['id', 'tested_positive'])\n",
        "#         for i, p in enumerate(preds):\n",
        "#             writer.writerow([i, p])\n",
        "\n",
        "# preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "# save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "30dfbdf5-3b47-4bec-a993-e306f49f47a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "for X, y in dv_set:                         # iterate through the dataloader\n",
        "    X, y = X.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "    with torch.no_grad():                   # disable gradient calculation\n",
        "        pred = model(X[:, 0], X[:, 1])                     # forward pass (compute output)\n",
        "        print(((pred -y) ** 2).mean())\n",
        "\n",
        "# total_loss = total_loss / len(dv_set.dataset)   \n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(13.4770, device='cuda:0')\n",
            "tensor(13.6992, device='cuda:0')\n",
            "tensor(13.5283, device='cuda:0')\n",
            "tensor(13.8071, device='cuda:0')\n",
            "tensor(14.3920, device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# U = torch.nn.Parameter(torch.rand(2, 3, requires_grad=True))\n",
        "# P = torch.nn.Parameter(torch.rand(2, 6, requires_grad=False))\n",
        "# net = nn.Sequential(nn.Linear(6, 3), nn.ReLU())\n",
        "# torch.einsum('ij, ij -> i', [U[[0, 1]], net(P[[0, 1]])])\n",
        "# gg = torch.Tensor([0.3, 0.7])\n",
        "# nn.Sigmoid()(gg)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}